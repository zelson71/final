{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>subsubregion</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14.1</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This white has an expert level of intensity an...</td>\n",
       "      <td>Dutton Ranch Walker Hill Vineyard</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Green Valley</td>\n",
       "      <td>Dutton-Goldfield 2016 Dutton Ranch Walker Hill...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/dutton-go...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Dutton-Goldfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>12.6</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>Stunning aromatics combine spicy citrus, tangy...</td>\n",
       "      <td>Maresh Vineyard Old Vine</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Dundee Hills</td>\n",
       "      <td>Harper Voit 2016 Maresh Vineyard Old Vine Ries...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/harper-vo...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "      <td>Harper Voit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a structured and remarkable wine, burs...</td>\n",
       "      <td>Year of the Monkey Single Vineyard</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Carneros</td>\n",
       "      <td>Donum 2016 Year of the Monkey Single Vineyard ...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/donum-201...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Donum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>The wine's aromas are arresting in notes of le...</td>\n",
       "      <td>Chaleur Blanc</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>DeLille 2017 Chaleur Blanc White (Columbia Val...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/delille-2...</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "      <td>DeLille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>14.7</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a grainy, structured and textured whit...</td>\n",
       "      <td>Lewis MacGregor Estate Vineyard</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Russian River Valley</td>\n",
       "      <td>Williams Selyem 2016 Lewis MacGregor Estate Vi...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/williams-...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Williams Selyem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  alcohol category country  \\\n",
       "0   4     14.1    White      US   \n",
       "1  39     12.6    White      US   \n",
       "2  53     13.8    White      US   \n",
       "3  56     13.8    White      US   \n",
       "4  74     14.7    White      US   \n",
       "\n",
       "                                         description  \\\n",
       "0  This white has an expert level of intensity an...   \n",
       "1  Stunning aromatics combine spicy citrus, tangy...   \n",
       "2  This is a structured and remarkable wine, burs...   \n",
       "3  The wine's aromas are arresting in notes of le...   \n",
       "4  This is a grainy, structured and textured whit...   \n",
       "\n",
       "                          designation  price  rating      region  \\\n",
       "0   Dutton Ranch Walker Hill Vineyard   50.0      94  California   \n",
       "1            Maresh Vineyard Old Vine   30.0      94      Oregon   \n",
       "2  Year of the Monkey Single Vineyard   60.0      94  California   \n",
       "3                       Chaleur Blanc   35.0      94  Washington   \n",
       "4     Lewis MacGregor Estate Vineyard   65.0      94  California   \n",
       "\n",
       "           subregion          subsubregion  \\\n",
       "0             Sonoma          Green Valley   \n",
       "1  Willamette Valley          Dundee Hills   \n",
       "2        Napa-Sonoma              Carneros   \n",
       "3    Columbia Valley  Columbia Valley (WA)   \n",
       "4             Sonoma  Russian River Valley   \n",
       "\n",
       "                                               title  \\\n",
       "0  Dutton-Goldfield 2016 Dutton Ranch Walker Hill...   \n",
       "1  Harper Voit 2016 Maresh Vineyard Old Vine Ries...   \n",
       "2  Donum 2016 Year of the Monkey Single Vineyard ...   \n",
       "3  DeLille 2017 Chaleur Blanc White (Columbia Val...   \n",
       "4  Williams Selyem 2016 Lewis MacGregor Estate Vi...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.winemag.com/buying-guide/dutton-go...   \n",
       "1  https://www.winemag.com/buying-guide/harper-vo...   \n",
       "2  https://www.winemag.com/buying-guide/donum-201...   \n",
       "3  https://www.winemag.com/buying-guide/delille-2...   \n",
       "4  https://www.winemag.com/buying-guide/williams-...   \n",
       "\n",
       "                     varietal  vintage            winery  \n",
       "0                  Chardonnay     2016  Dutton-Goldfield  \n",
       "1                    Riesling     2016       Harper Voit  \n",
       "2                  Chardonnay     2016             Donum  \n",
       "3  Bordeaux-style White Blend     2017           DeLille  \n",
       "4                  Chardonnay     2016   Williams Selyem  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Resources/uswhites.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  price  rating      region          subregion  \\\n",
       "0     14.1   50.0      94  California             Sonoma   \n",
       "1     12.6   30.0      94      Oregon  Willamette Valley   \n",
       "2     13.8   60.0      94  California        Napa-Sonoma   \n",
       "3     13.8   35.0      94  Washington    Columbia Valley   \n",
       "4     14.7   65.0      94  California             Sonoma   \n",
       "\n",
       "                     varietal  vintage  \n",
       "0                  Chardonnay     2016  \n",
       "1                    Riesling     2016  \n",
       "2                  Chardonnay     2016  \n",
       "3  Bordeaux-style White Blend     2017  \n",
       "4                  Chardonnay     2016  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['ID', 'category', 'country', 'description', 'designation', 'subsubregion', 'title', 'url', 'winery'], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(df)\n",
    "X_df['subregion'] = X_df['subregion'].astype(str)\n",
    "X_df['vintage'] = X_df['vintage'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23653</td>\n",
       "      <td>147</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23654</td>\n",
       "      <td>147</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23655</td>\n",
       "      <td>159</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23656</td>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>138</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23657</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23658 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       alcohol  rating  region  subregion  varietal  vintage\n",
       "0          130      14       2         42        17       22\n",
       "1           68      14      17         56        87       22\n",
       "2          118      14       2         28        17       22\n",
       "3          118      14      24          7        10       23\n",
       "4          152      14       2         42        17       22\n",
       "...        ...     ...     ...        ...       ...      ...\n",
       "23653      147      11       2          3        17       11\n",
       "23654      147      12       2          3        17       11\n",
       "23655      159      12       2          3        17       11\n",
       "23656      176      12       2         42       138       10\n",
       "23657      139      12       2          2        17       11\n",
       "\n",
       "[23658 rows x 6 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= X.apply(LabelEncoder().fit_transform)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130.  14.   2.  42.  17.  22.]\n",
      " [ 68.  14.  17.  56.  87.  22.]\n",
      " [118.  14.   2.  28.  17.  22.]\n",
      " ...\n",
      " [159.  12.   2.   3.  17.  11.]\n",
      " [176.  12.   2.  42. 138.  10.]\n",
      " [139.  12.   2.   2.  17.  11.]]\n",
      "[50. 30. 60. ... 29. 24. 55.]\n"
     ]
    }
   ],
   "source": [
    "X = X.values.astype(\"float32\")\n",
    "print(X)\n",
    "y = y.values.astype(\"float32\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915,)\n",
      "(5915, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = scaler_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 197\n",
      "Trainable params: 197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', name='dense_1', kernel_initializer='random_uniform', input_dim=(input_dims)))\n",
    "model.add(Dense(8, activation='relu', name='dense_2', kernel_initializer='random_uniform'))\n",
    "model.add(Dense(1, activation='linear', name='predictions'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14194 samples, validate on 3549 samples\n",
      "Epoch 1/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 89.6385 - mse: 89.6385 - mae: 6.6026 - val_loss: 84.6232 - val_mse: 84.6232 - val_mae: 6.5394\n",
      "Epoch 2/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 89.5536 - mse: 89.5536 - mae: 6.5980 - val_loss: 84.4986 - val_mse: 84.4986 - val_mae: 6.5536\n",
      "Epoch 3/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 89.4734 - mse: 89.4734 - mae: 6.5989 - val_loss: 85.8070 - val_mse: 85.8070 - val_mae: 6.7018\n",
      "Epoch 4/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.5732 - mse: 89.5732 - mae: 6.6052 - val_loss: 84.3683 - val_mse: 84.3682 - val_mae: 6.6043\n",
      "Epoch 5/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 89.5058 - mse: 89.5058 - mae: 6.5996 - val_loss: 84.3797 - val_mse: 84.3798 - val_mae: 6.5839\n",
      "Epoch 6/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 89.5980 - mse: 89.5980 - mae: 6.6036 - val_loss: 84.3596 - val_mse: 84.3596 - val_mae: 6.5985\n",
      "Epoch 7/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.5143 - mse: 89.5143 - mae: 6.6029 - val_loss: 85.4015 - val_mse: 85.4014 - val_mae: 6.5239\n",
      "Epoch 8/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 89.4604 - mse: 89.4604 - mae: 6.5946 - val_loss: 84.3297 - val_mse: 84.3298 - val_mae: 6.5815\n",
      "Epoch 9/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 89.4027 - mse: 89.4026 - mae: 6.5907 - val_loss: 84.4684 - val_mse: 84.4684 - val_mae: 6.5502\n",
      "Epoch 10/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 89.3305 - mse: 89.3306 - mae: 6.5953 - val_loss: 84.6638 - val_mse: 84.6638 - val_mae: 6.6279\n",
      "Epoch 11/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 89.4157 - mse: 89.4157 - mae: 6.5997 - val_loss: 84.3000 - val_mse: 84.3000 - val_mae: 6.6087\n",
      "Epoch 12/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 89.4178 - mse: 89.4178 - mae: 6.6024 - val_loss: 84.4092 - val_mse: 84.4092 - val_mae: 6.5786\n",
      "Epoch 13/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 89.2792 - mse: 89.2792 - mae: 6.5940 - val_loss: 84.8048 - val_mse: 84.8048 - val_mae: 6.5779\n",
      "Epoch 14/1000\n",
      "14194/14194 [==============================] - 1s 82us/step - loss: 89.2460 - mse: 89.2460 - mae: 6.6006 - val_loss: 84.1928 - val_mse: 84.1928 - val_mae: 6.5655\n",
      "Epoch 15/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.3477 - mse: 89.3477 - mae: 6.5937 - val_loss: 84.5549 - val_mse: 84.5549 - val_mae: 6.5918\n",
      "Epoch 16/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 89.3164 - mse: 89.3164 - mae: 6.5971 - val_loss: 84.6886 - val_mse: 84.6886 - val_mae: 6.5004\n",
      "Epoch 17/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 89.2307 - mse: 89.2306 - mae: 6.5895 - val_loss: 84.6327 - val_mse: 84.6327 - val_mae: 6.6368\n",
      "Epoch 18/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 89.3693 - mse: 89.3693 - mae: 6.5916 - val_loss: 84.2657 - val_mse: 84.2657 - val_mae: 6.5199\n",
      "Epoch 19/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 89.2393 - mse: 89.2393 - mae: 6.5849 - val_loss: 84.4430 - val_mse: 84.4430 - val_mae: 6.5286\n",
      "Epoch 20/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 89.1884 - mse: 89.1885 - mae: 6.5906 - val_loss: 84.4668 - val_mse: 84.4668 - val_mae: 6.5366\n",
      "Epoch 21/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 89.1480 - mse: 89.1480 - mae: 6.5822 - val_loss: 85.0434 - val_mse: 85.0434 - val_mae: 6.5286\n",
      "Epoch 22/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 89.2992 - mse: 89.2992 - mae: 6.5843 - val_loss: 84.0958 - val_mse: 84.0958 - val_mae: 6.5749\n",
      "Epoch 23/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 89.2140 - mse: 89.2139 - mae: 6.5919 - val_loss: 85.4987 - val_mse: 85.4987 - val_mae: 6.4891\n",
      "Epoch 24/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 89.1772 - mse: 89.1772 - mae: 6.5821 - val_loss: 84.5681 - val_mse: 84.5681 - val_mae: 6.5935\n",
      "Epoch 25/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 89.1195 - mse: 89.1195 - mae: 6.5844 - val_loss: 84.1172 - val_mse: 84.1172 - val_mae: 6.5914\n",
      "Epoch 26/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 89.1317 - mse: 89.1317 - mae: 6.5869 - val_loss: 84.3570 - val_mse: 84.3570 - val_mae: 6.6437\n",
      "Epoch 27/1000\n",
      "14194/14194 [==============================] - 1s 81us/step - loss: 89.1225 - mse: 89.1225 - mae: 6.5871 - val_loss: 84.3926 - val_mse: 84.3926 - val_mae: 6.5309\n",
      "Epoch 28/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 89.1450 - mse: 89.1449 - mae: 6.5811 - val_loss: 84.2186 - val_mse: 84.2186 - val_mae: 6.5460\n",
      "Epoch 29/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 89.0965 - mse: 89.0964 - mae: 6.5774 - val_loss: 84.6976 - val_mse: 84.6975 - val_mae: 6.5330\n",
      "Epoch 30/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 89.1232 - mse: 89.1231 - mae: 6.5820 - val_loss: 84.2540 - val_mse: 84.2540 - val_mae: 6.6014\n",
      "Epoch 31/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 89.0217 - mse: 89.0217 - mae: 6.5797 - val_loss: 84.4435 - val_mse: 84.4435 - val_mae: 6.5165\n",
      "Epoch 32/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 89.0115 - mse: 89.0114 - mae: 6.5834 - val_loss: 83.9440 - val_mse: 83.9439 - val_mae: 6.5442\n",
      "Epoch 33/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 89.1021 - mse: 89.1022 - mae: 6.5763 - val_loss: 84.2518 - val_mse: 84.2518 - val_mae: 6.5565\n",
      "Epoch 34/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 89.0612 - mse: 89.0612 - mae: 6.5785 - val_loss: 84.1676 - val_mse: 84.1676 - val_mae: 6.5319\n",
      "Epoch 35/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 89.0816 - mse: 89.0816 - mae: 6.5801 - val_loss: 84.1212 - val_mse: 84.1212 - val_mae: 6.5759\n",
      "Epoch 36/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 89.0501 - mse: 89.0501 - mae: 6.5781 - val_loss: 84.4945 - val_mse: 84.4945 - val_mae: 6.5286\n",
      "Epoch 37/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 88.8390 - mse: 88.8391 - mae: 6.5698 - val_loss: 84.1082 - val_mse: 84.1082 - val_mae: 6.5205\n",
      "Epoch 38/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.0997 - mse: 89.0998 - mae: 6.5749 - val_loss: 84.8809 - val_mse: 84.8809 - val_mae: 6.6588\n",
      "Epoch 39/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 89.0419 - mse: 89.0418 - mae: 6.5820 - val_loss: 84.3849 - val_mse: 84.3849 - val_mae: 6.5272\n",
      "Epoch 40/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 88.9570 - mse: 88.9570 - mae: 6.5799 - val_loss: 84.1136 - val_mse: 84.1136 - val_mae: 6.5900\n",
      "Epoch 41/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.8933 - mse: 88.8933 - mae: 6.5814 - val_loss: 84.1166 - val_mse: 84.1166 - val_mae: 6.5229\n",
      "Epoch 42/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 88.9375 - mse: 88.9374 - mae: 6.5795 - val_loss: 84.2364 - val_mse: 84.2364 - val_mae: 6.5769\n",
      "Epoch 43/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 88.9454 - mse: 88.9454 - mae: 6.5771 - val_loss: 84.0846 - val_mse: 84.0846 - val_mae: 6.4959\n",
      "Epoch 44/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 89.0194 - mse: 89.0194 - mae: 6.5805 - val_loss: 84.6284 - val_mse: 84.6284 - val_mae: 6.5046\n",
      "Epoch 45/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 88.9386 - mse: 88.9386 - mae: 6.5804 - val_loss: 83.9632 - val_mse: 83.9632 - val_mae: 6.5526\n",
      "Epoch 46/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.8652 - mse: 88.8652 - mae: 6.5735 - val_loss: 84.0432 - val_mse: 84.0432 - val_mae: 6.5278\n",
      "Epoch 47/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 89.0176 - mse: 89.0176 - mae: 6.5778 - val_loss: 84.0673 - val_mse: 84.0673 - val_mae: 6.5758\n",
      "Epoch 48/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 89.0788 - mse: 89.0788 - mae: 6.5773 - val_loss: 83.9016 - val_mse: 83.9016 - val_mae: 6.5472\n",
      "Epoch 49/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 88.9279 - mse: 88.9279 - mae: 6.5835 - val_loss: 84.1340 - val_mse: 84.1339 - val_mae: 6.4954\n",
      "Epoch 50/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 88.9335 - mse: 88.9334 - mae: 6.5745 - val_loss: 83.9988 - val_mse: 83.9988 - val_mae: 6.5189\n",
      "Epoch 51/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.0209 - mse: 89.0209 - mae: 6.5757 - val_loss: 83.9675 - val_mse: 83.9675 - val_mae: 6.5599\n",
      "Epoch 52/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 88.8367 - mse: 88.8366 - mae: 6.5803 - val_loss: 84.4127 - val_mse: 84.4127 - val_mae: 6.5129\n",
      "Epoch 53/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 88.8645 - mse: 88.8645 - mae: 6.5681 - val_loss: 84.1967 - val_mse: 84.1967 - val_mae: 6.5369\n",
      "Epoch 54/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.8553 - mse: 88.8554 - mae: 6.5697 - val_loss: 83.9157 - val_mse: 83.9157 - val_mae: 6.5261\n",
      "Epoch 55/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.0403 - mse: 89.0404 - mae: 6.5749 - val_loss: 84.1791 - val_mse: 84.1792 - val_mae: 6.5735\n",
      "Epoch 56/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 88.8510 - mse: 88.8510 - mae: 6.5693 - val_loss: 84.5538 - val_mse: 84.5539 - val_mae: 6.6899\n",
      "Epoch 57/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 88.9628 - mse: 88.9628 - mae: 6.5791 - val_loss: 84.0475 - val_mse: 84.0475 - val_mae: 6.5089\n",
      "Epoch 58/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 89.0384 - mse: 89.0384 - mae: 6.5762 - val_loss: 83.9911 - val_mse: 83.9911 - val_mae: 6.6256\n",
      "Epoch 59/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 88.8256 - mse: 88.8256 - mae: 6.5730 - val_loss: 84.4504 - val_mse: 84.4504 - val_mae: 6.6338\n",
      "Epoch 60/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.8444 - mse: 88.8444 - mae: 6.5699 - val_loss: 84.6400 - val_mse: 84.6400 - val_mae: 6.6118\n",
      "Epoch 61/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 88.9170 - mse: 88.9170 - mae: 6.5769 - val_loss: 83.9181 - val_mse: 83.9181 - val_mae: 6.5372\n",
      "Epoch 62/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.8332 - mse: 88.8333 - mae: 6.5680 - val_loss: 84.4545 - val_mse: 84.4545 - val_mae: 6.5214\n",
      "Epoch 63/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.8613 - mse: 88.8614 - mae: 6.5701 - val_loss: 83.7837 - val_mse: 83.7837 - val_mae: 6.5405\n",
      "Epoch 64/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.9376 - mse: 88.9376 - mae: 6.5678 - val_loss: 84.1597 - val_mse: 84.1597 - val_mae: 6.5494\n",
      "Epoch 65/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.8766 - mse: 88.8765 - mae: 6.5739 - val_loss: 84.2968 - val_mse: 84.2968 - val_mae: 6.4817\n",
      "Epoch 66/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 88.9194 - mse: 88.9194 - mae: 6.5691 - val_loss: 83.7931 - val_mse: 83.7931 - val_mae: 6.5282\n",
      "Epoch 67/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 88.9116 - mse: 88.9116 - mae: 6.5736 - val_loss: 84.3181 - val_mse: 84.3181 - val_mae: 6.6041\n",
      "Epoch 68/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 88.9059 - mse: 88.9059 - mae: 6.5707 - val_loss: 84.2732 - val_mse: 84.2732 - val_mae: 6.6092\n",
      "Epoch 69/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 88.9474 - mse: 88.9473 - mae: 6.5756 - val_loss: 84.5635 - val_mse: 84.5635 - val_mae: 6.5805\n",
      "Epoch 70/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 88.8672 - mse: 88.8672 - mae: 6.5746 - val_loss: 84.6329 - val_mse: 84.6329 - val_mae: 6.5239\n",
      "Epoch 71/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 88.8069 - mse: 88.8070 - mae: 6.5688 - val_loss: 83.9547 - val_mse: 83.9546 - val_mae: 6.5538\n",
      "Epoch 72/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.8528 - mse: 88.8527 - mae: 6.5708 - val_loss: 84.0135 - val_mse: 84.0135 - val_mae: 6.5259\n",
      "Epoch 73/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.7445 - mse: 88.7445 - mae: 6.5600 - val_loss: 84.1853 - val_mse: 84.1853 - val_mae: 6.6009\n",
      "Epoch 74/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.8278 - mse: 88.8277 - mae: 6.5697 - val_loss: 84.1969 - val_mse: 84.1969 - val_mae: 6.5942\n",
      "Epoch 75/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 88.7293 - mse: 88.7294 - mae: 6.5697 - val_loss: 84.1529 - val_mse: 84.1529 - val_mae: 6.5808\n",
      "Epoch 76/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.7987 - mse: 88.7987 - mae: 6.5661 - val_loss: 84.3863 - val_mse: 84.3863 - val_mae: 6.6391\n",
      "Epoch 77/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 88.8000 - mse: 88.8000 - mae: 6.5733 - val_loss: 83.8447 - val_mse: 83.8447 - val_mae: 6.5429\n",
      "Epoch 78/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.9077 - mse: 88.9077 - mae: 6.5698 - val_loss: 83.8029 - val_mse: 83.8029 - val_mae: 6.5596\n",
      "Epoch 79/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.8574 - mse: 88.8575 - mae: 6.5702 - val_loss: 84.0162 - val_mse: 84.0162 - val_mae: 6.5778\n",
      "Epoch 80/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 88.8020 - mse: 88.8019 - mae: 6.5731 - val_loss: 84.3446 - val_mse: 84.3446 - val_mae: 6.5511\n",
      "Epoch 81/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 88.8373 - mse: 88.8373 - mae: 6.5721 - val_loss: 83.9872 - val_mse: 83.9872 - val_mae: 6.6053\n",
      "Epoch 82/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.7866 - mse: 88.7866 - mae: 6.5706 - val_loss: 83.8799 - val_mse: 83.8799 - val_mae: 6.5368\n",
      "Epoch 83/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.8019 - mse: 88.8019 - mae: 6.5623 - val_loss: 84.1695 - val_mse: 84.1695 - val_mae: 6.6479\n",
      "Epoch 84/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.6875 - mse: 88.6876 - mae: 6.5642 - val_loss: 84.5363 - val_mse: 84.5363 - val_mae: 6.6615\n",
      "Epoch 85/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 88.8775 - mse: 88.8774 - mae: 6.5699 - val_loss: 84.2061 - val_mse: 84.2061 - val_mae: 6.5297\n",
      "Epoch 86/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.7954 - mse: 88.7954 - mae: 6.5639 - val_loss: 84.6097 - val_mse: 84.6097 - val_mae: 6.6784\n",
      "Epoch 87/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.7396 - mse: 88.7396 - mae: 6.5691 - val_loss: 85.2680 - val_mse: 85.2681 - val_mae: 6.7716\n",
      "Epoch 88/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.7319 - mse: 88.7319 - mae: 6.5633 - val_loss: 84.0163 - val_mse: 84.0163 - val_mae: 6.6230\n",
      "Epoch 89/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.6875 - mse: 88.6875 - mae: 6.5608 - val_loss: 83.7525 - val_mse: 83.7525 - val_mae: 6.5246\n",
      "Epoch 90/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.6975 - mse: 88.6975 - mae: 6.5635 - val_loss: 84.4528 - val_mse: 84.4529 - val_mae: 6.4898\n",
      "Epoch 91/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.7246 - mse: 88.7245 - mae: 6.5579 - val_loss: 83.5434 - val_mse: 83.5434 - val_mae: 6.5320\n",
      "Epoch 92/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.6205 - mse: 88.6205 - mae: 6.5517 - val_loss: 84.6668 - val_mse: 84.6668 - val_mae: 6.7127\n",
      "Epoch 93/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.5707 - mse: 88.5707 - mae: 6.5490 - val_loss: 83.6361 - val_mse: 83.6361 - val_mae: 6.5112\n",
      "Epoch 94/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.5670 - mse: 88.5670 - mae: 6.5502 - val_loss: 83.8571 - val_mse: 83.8570 - val_mae: 6.5034\n",
      "Epoch 95/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.5468 - mse: 88.5469 - mae: 6.5478 - val_loss: 83.8303 - val_mse: 83.8303 - val_mae: 6.5023\n",
      "Epoch 96/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.4611 - mse: 88.4611 - mae: 6.5442 - val_loss: 83.5731 - val_mse: 83.5731 - val_mae: 6.5499\n",
      "Epoch 97/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4300 - mse: 88.4300 - mae: 6.5477 - val_loss: 83.8389 - val_mse: 83.8389 - val_mae: 6.4813\n",
      "Epoch 98/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.3267 - mse: 88.3267 - mae: 6.5310 - val_loss: 83.5378 - val_mse: 83.5379 - val_mae: 6.5625\n",
      "Epoch 99/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.3206 - mse: 88.3205 - mae: 6.5372 - val_loss: 83.6018 - val_mse: 83.6018 - val_mae: 6.5624\n",
      "Epoch 100/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.2743 - mse: 88.2742 - mae: 6.5318 - val_loss: 83.3496 - val_mse: 83.3496 - val_mae: 6.5161\n",
      "Epoch 101/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 88.2630 - mse: 88.2630 - mae: 6.5328 - val_loss: 83.6967 - val_mse: 83.6967 - val_mae: 6.4779\n",
      "Epoch 102/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.3080 - mse: 88.3080 - mae: 6.5355 - val_loss: 83.4279 - val_mse: 83.4280 - val_mae: 6.4611\n",
      "Epoch 103/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.0852 - mse: 88.0852 - mae: 6.5187 - val_loss: 84.1854 - val_mse: 84.1853 - val_mae: 6.6027\n",
      "Epoch 104/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.1882 - mse: 88.1882 - mae: 6.5233 - val_loss: 84.0299 - val_mse: 84.0300 - val_mae: 6.4928\n",
      "Epoch 105/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.2287 - mse: 88.2287 - mae: 6.5249 - val_loss: 83.3523 - val_mse: 83.3523 - val_mae: 6.5137\n",
      "Epoch 106/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.1158 - mse: 88.1158 - mae: 6.5280 - val_loss: 84.4129 - val_mse: 84.4129 - val_mae: 6.4285\n",
      "Epoch 107/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.0423 - mse: 88.0424 - mae: 6.5091 - val_loss: 83.4323 - val_mse: 83.4324 - val_mae: 6.5743\n",
      "Epoch 108/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.0855 - mse: 88.0854 - mae: 6.5177 - val_loss: 84.4338 - val_mse: 84.4338 - val_mae: 6.4813\n",
      "Epoch 109/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.9729 - mse: 87.9728 - mae: 6.5112 - val_loss: 83.4995 - val_mse: 83.4995 - val_mae: 6.5637\n",
      "Epoch 110/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.9544 - mse: 87.9543 - mae: 6.5153 - val_loss: 83.5718 - val_mse: 83.5718 - val_mae: 6.5848\n",
      "Epoch 111/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.0196 - mse: 88.0196 - mae: 6.5162 - val_loss: 83.6104 - val_mse: 83.6103 - val_mae: 6.5082\n",
      "Epoch 112/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.8136 - mse: 87.8136 - mae: 6.5008 - val_loss: 83.5036 - val_mse: 83.5036 - val_mae: 6.5580\n",
      "Epoch 113/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.9016 - mse: 87.9017 - mae: 6.5127 - val_loss: 83.3357 - val_mse: 83.3357 - val_mae: 6.4883\n",
      "Epoch 114/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.9769 - mse: 87.9770 - mae: 6.5091 - val_loss: 83.1965 - val_mse: 83.1965 - val_mae: 6.5210\n",
      "Epoch 115/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.7871 - mse: 87.7871 - mae: 6.5069 - val_loss: 83.1986 - val_mse: 83.1985 - val_mae: 6.5382\n",
      "Epoch 116/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 87.8054 - mse: 87.8054 - mae: 6.5102 - val_loss: 83.6345 - val_mse: 83.6346 - val_mae: 6.4989\n",
      "Epoch 117/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.8007 - mse: 87.8007 - mae: 6.5062 - val_loss: 83.0060 - val_mse: 83.0060 - val_mae: 6.5064\n",
      "Epoch 118/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.7990 - mse: 87.7990 - mae: 6.4980 - val_loss: 83.2461 - val_mse: 83.2461 - val_mae: 6.5021\n",
      "Epoch 119/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.6615 - mse: 87.6615 - mae: 6.4998 - val_loss: 83.0584 - val_mse: 83.0583 - val_mae: 6.5046\n",
      "Epoch 120/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.7501 - mse: 87.7501 - mae: 6.5015 - val_loss: 83.3731 - val_mse: 83.3731 - val_mae: 6.4882\n",
      "Epoch 121/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.6946 - mse: 87.6946 - mae: 6.4978 - val_loss: 82.9791 - val_mse: 82.9791 - val_mae: 6.5143\n",
      "Epoch 122/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.7288 - mse: 87.7289 - mae: 6.4952 - val_loss: 83.1491 - val_mse: 83.1491 - val_mae: 6.5748\n",
      "Epoch 123/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.6502 - mse: 87.6502 - mae: 6.4996 - val_loss: 83.1551 - val_mse: 83.1551 - val_mae: 6.4954\n",
      "Epoch 124/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.5841 - mse: 87.5841 - mae: 6.4932 - val_loss: 83.1096 - val_mse: 83.1096 - val_mae: 6.5396\n",
      "Epoch 125/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.6082 - mse: 87.6082 - mae: 6.4974 - val_loss: 83.4948 - val_mse: 83.4947 - val_mae: 6.5205\n",
      "Epoch 126/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.5353 - mse: 87.5353 - mae: 6.4887 - val_loss: 83.2808 - val_mse: 83.2808 - val_mae: 6.5170\n",
      "Epoch 127/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.6220 - mse: 87.6219 - mae: 6.4945 - val_loss: 83.4149 - val_mse: 83.4149 - val_mae: 6.5715\n",
      "Epoch 128/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.5436 - mse: 87.5437 - mae: 6.4876 - val_loss: 82.9436 - val_mse: 82.9436 - val_mae: 6.5264\n",
      "Epoch 129/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.4692 - mse: 87.4692 - mae: 6.4897 - val_loss: 82.8732 - val_mse: 82.8732 - val_mae: 6.5304\n",
      "Epoch 130/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.4387 - mse: 87.4388 - mae: 6.4869 - val_loss: 83.6206 - val_mse: 83.6207 - val_mae: 6.5736\n",
      "Epoch 131/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.5287 - mse: 87.5287 - mae: 6.4883 - val_loss: 82.8422 - val_mse: 82.8422 - val_mae: 6.4941\n",
      "Epoch 132/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.5107 - mse: 87.5107 - mae: 6.4952 - val_loss: 84.5558 - val_mse: 84.5558 - val_mae: 6.4100\n",
      "Epoch 133/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.5389 - mse: 87.5388 - mae: 6.4821 - val_loss: 83.8880 - val_mse: 83.8879 - val_mae: 6.4787\n",
      "Epoch 134/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 87.5133 - mse: 87.5134 - mae: 6.4875 - val_loss: 83.1026 - val_mse: 83.1025 - val_mae: 6.5409\n",
      "Epoch 135/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.3890 - mse: 87.3890 - mae: 6.4892 - val_loss: 82.8460 - val_mse: 82.8460 - val_mae: 6.5014\n",
      "Epoch 136/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4427 - mse: 87.4426 - mae: 6.4846 - val_loss: 82.7456 - val_mse: 82.7456 - val_mae: 6.4808\n",
      "Epoch 137/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.3053 - mse: 87.3053 - mae: 6.4865 - val_loss: 82.8793 - val_mse: 82.8793 - val_mae: 6.5180\n",
      "Epoch 138/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.3910 - mse: 87.3911 - mae: 6.4896 - val_loss: 83.2885 - val_mse: 83.2885 - val_mae: 6.4698\n",
      "Epoch 139/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.3969 - mse: 87.3968 - mae: 6.4843 - val_loss: 82.8876 - val_mse: 82.8877 - val_mae: 6.4797\n",
      "Epoch 140/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.2657 - mse: 87.2657 - mae: 6.4782 - val_loss: 82.9120 - val_mse: 82.9120 - val_mae: 6.4895\n",
      "Epoch 141/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.1645 - mse: 87.1645 - mae: 6.4763 - val_loss: 83.6478 - val_mse: 83.6478 - val_mae: 6.5809\n",
      "Epoch 142/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.4413 - mse: 87.4414 - mae: 6.4878 - val_loss: 83.2624 - val_mse: 83.2624 - val_mae: 6.5562\n",
      "Epoch 143/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.3210 - mse: 87.3210 - mae: 6.4787 - val_loss: 82.8398 - val_mse: 82.8398 - val_mae: 6.4928\n",
      "Epoch 144/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 87.2285 - mse: 87.2285 - mae: 6.4799 - val_loss: 82.7945 - val_mse: 82.7945 - val_mae: 6.5141\n",
      "Epoch 145/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 87.1606 - mse: 87.1606 - mae: 6.4802 - val_loss: 83.2009 - val_mse: 83.2008 - val_mae: 6.4727\n",
      "Epoch 146/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.1957 - mse: 87.1957 - mae: 6.4766 - val_loss: 82.7892 - val_mse: 82.7892 - val_mae: 6.4809\n",
      "Epoch 147/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.2182 - mse: 87.2182 - mae: 6.4824 - val_loss: 82.9736 - val_mse: 82.9736 - val_mae: 6.4778\n",
      "Epoch 148/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.0601 - mse: 87.0601 - mae: 6.4689 - val_loss: 82.9349 - val_mse: 82.9349 - val_mae: 6.5008\n",
      "Epoch 149/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.2495 - mse: 87.2495 - mae: 6.4696 - val_loss: 82.8179 - val_mse: 82.8179 - val_mae: 6.5366\n",
      "Epoch 150/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.0796 - mse: 87.0796 - mae: 6.4785 - val_loss: 82.7897 - val_mse: 82.7897 - val_mae: 6.4727\n",
      "Epoch 151/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.0904 - mse: 87.0905 - mae: 6.4716 - val_loss: 83.0157 - val_mse: 83.0157 - val_mae: 6.4766\n",
      "Epoch 152/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.0492 - mse: 87.0493 - mae: 6.4696 - val_loss: 82.9350 - val_mse: 82.9351 - val_mae: 6.5251\n",
      "Epoch 153/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.1277 - mse: 87.1277 - mae: 6.4708 - val_loss: 82.5782 - val_mse: 82.5782 - val_mae: 6.5093\n",
      "Epoch 154/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.0281 - mse: 87.0282 - mae: 6.4762 - val_loss: 82.6869 - val_mse: 82.6869 - val_mae: 6.4774\n",
      "Epoch 155/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.9110 - mse: 86.9110 - mae: 6.4648 - val_loss: 82.7240 - val_mse: 82.7241 - val_mae: 6.4713\n",
      "Epoch 156/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 87.0455 - mse: 87.0455 - mae: 6.4709 - val_loss: 82.6565 - val_mse: 82.6565 - val_mae: 6.5105\n",
      "Epoch 157/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 86.9207 - mse: 86.9207 - mae: 6.4660 - val_loss: 82.5314 - val_mse: 82.5315 - val_mae: 6.5213\n",
      "Epoch 158/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 86.9882 - mse: 86.9882 - mae: 6.4669 - val_loss: 82.9120 - val_mse: 82.9120 - val_mae: 6.5267\n",
      "Epoch 159/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 86.8511 - mse: 86.8510 - mae: 6.4603 - val_loss: 82.7903 - val_mse: 82.7903 - val_mae: 6.5736\n",
      "Epoch 160/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.7804 - mse: 86.7804 - mae: 6.4578 - val_loss: 82.6095 - val_mse: 82.6095 - val_mae: 6.5125\n",
      "Epoch 161/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.8791 - mse: 86.8792 - mae: 6.4612 - val_loss: 82.8962 - val_mse: 82.8961 - val_mae: 6.5848\n",
      "Epoch 162/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 86.7978 - mse: 86.7978 - mae: 6.4592 - val_loss: 82.6967 - val_mse: 82.6966 - val_mae: 6.5271\n",
      "Epoch 163/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.7212 - mse: 86.7212 - mae: 6.4631 - val_loss: 82.8073 - val_mse: 82.8073 - val_mae: 6.5443\n",
      "Epoch 164/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.8831 - mse: 86.8831 - mae: 6.4670 - val_loss: 82.5929 - val_mse: 82.5929 - val_mae: 6.4542\n",
      "Epoch 165/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.7760 - mse: 86.7759 - mae: 6.4572 - val_loss: 83.5026 - val_mse: 83.5026 - val_mae: 6.5988\n",
      "Epoch 166/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.7123 - mse: 86.7122 - mae: 6.4593 - val_loss: 82.8164 - val_mse: 82.8165 - val_mae: 6.4673\n",
      "Epoch 167/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.7070 - mse: 86.7070 - mae: 6.4607 - val_loss: 82.9936 - val_mse: 82.9937 - val_mae: 6.4421\n",
      "Epoch 168/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.8107 - mse: 86.8107 - mae: 6.4643 - val_loss: 82.5439 - val_mse: 82.5440 - val_mae: 6.4339\n",
      "Epoch 169/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.7604 - mse: 86.7603 - mae: 6.4551 - val_loss: 82.7186 - val_mse: 82.7186 - val_mae: 6.5382\n",
      "Epoch 170/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 86.6395 - mse: 86.6395 - mae: 6.4515 - val_loss: 82.6272 - val_mse: 82.6272 - val_mae: 6.4522\n",
      "Epoch 171/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.6918 - mse: 86.6917 - mae: 6.4535 - val_loss: 82.5605 - val_mse: 82.5605 - val_mae: 6.5379\n",
      "Epoch 172/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 86.6721 - mse: 86.6720 - mae: 6.4538 - val_loss: 83.5434 - val_mse: 83.5434 - val_mae: 6.5882\n",
      "Epoch 173/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.8355 - mse: 86.8355 - mae: 6.4595 - val_loss: 82.6520 - val_mse: 82.6520 - val_mae: 6.4464\n",
      "Epoch 174/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 86.7081 - mse: 86.7082 - mae: 6.4573 - val_loss: 82.5528 - val_mse: 82.5528 - val_mae: 6.5039\n",
      "Epoch 175/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 86.6576 - mse: 86.6576 - mae: 6.4530 - val_loss: 82.3977 - val_mse: 82.3977 - val_mae: 6.4739\n",
      "Epoch 176/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.5812 - mse: 86.5812 - mae: 6.4531 - val_loss: 82.4591 - val_mse: 82.4591 - val_mae: 6.4583\n",
      "Epoch 177/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.7726 - mse: 86.7726 - mae: 6.4608 - val_loss: 82.4735 - val_mse: 82.4735 - val_mae: 6.4729\n",
      "Epoch 178/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.7034 - mse: 86.7034 - mae: 6.4556 - val_loss: 82.4150 - val_mse: 82.4150 - val_mae: 6.4475\n",
      "Epoch 179/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.6493 - mse: 86.6493 - mae: 6.4484 - val_loss: 83.3732 - val_mse: 83.3732 - val_mae: 6.4428\n",
      "Epoch 180/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.7205 - mse: 86.7205 - mae: 6.4530 - val_loss: 82.7460 - val_mse: 82.7460 - val_mae: 6.5441\n",
      "Epoch 181/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.6690 - mse: 86.6690 - mae: 6.4592 - val_loss: 84.2918 - val_mse: 84.2917 - val_mae: 6.3785\n",
      "Epoch 182/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 86.6103 - mse: 86.6103 - mae: 6.4481 - val_loss: 82.2891 - val_mse: 82.2891 - val_mae: 6.4898\n",
      "Epoch 183/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.5448 - mse: 86.5448 - mae: 6.4432 - val_loss: 82.3784 - val_mse: 82.3784 - val_mae: 6.4996\n",
      "Epoch 184/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 86.5697 - mse: 86.5696 - mae: 6.4479 - val_loss: 82.6896 - val_mse: 82.6897 - val_mae: 6.5559\n",
      "Epoch 185/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.5028 - mse: 86.5029 - mae: 6.4525 - val_loss: 82.9470 - val_mse: 82.9470 - val_mae: 6.4589\n",
      "Epoch 186/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 86.6068 - mse: 86.6068 - mae: 6.4458 - val_loss: 82.2876 - val_mse: 82.2876 - val_mae: 6.5116\n",
      "Epoch 187/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 86.6630 - mse: 86.6630 - mae: 6.4438 - val_loss: 82.2519 - val_mse: 82.2519 - val_mae: 6.4684\n",
      "Epoch 188/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 86.5238 - mse: 86.5237 - mae: 6.4444 - val_loss: 83.2714 - val_mse: 83.2714 - val_mae: 6.4234\n",
      "Epoch 189/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.5531 - mse: 86.5532 - mae: 6.4425 - val_loss: 83.0303 - val_mse: 83.0303 - val_mae: 6.4330\n",
      "Epoch 190/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 86.6516 - mse: 86.6516 - mae: 6.4499 - val_loss: 82.7003 - val_mse: 82.7003 - val_mae: 6.4147\n",
      "Epoch 191/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 86.5533 - mse: 86.5532 - mae: 6.4458 - val_loss: 82.3226 - val_mse: 82.3226 - val_mae: 6.4807\n",
      "Epoch 192/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.5430 - mse: 86.5430 - mae: 6.4410 - val_loss: 82.6978 - val_mse: 82.6978 - val_mae: 6.5902\n",
      "Epoch 193/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.4915 - mse: 86.4916 - mae: 6.4425 - val_loss: 82.2556 - val_mse: 82.2556 - val_mae: 6.5392\n",
      "Epoch 194/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 86.4746 - mse: 86.4746 - mae: 6.4523 - val_loss: 82.4651 - val_mse: 82.4650 - val_mae: 6.5012\n",
      "Epoch 195/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.5458 - mse: 86.5458 - mae: 6.4479 - val_loss: 82.6832 - val_mse: 82.6832 - val_mae: 6.4103\n",
      "Epoch 196/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.5057 - mse: 86.5056 - mae: 6.4468 - val_loss: 82.6148 - val_mse: 82.6148 - val_mae: 6.4442\n",
      "Epoch 197/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.5822 - mse: 86.5822 - mae: 6.4469 - val_loss: 82.3717 - val_mse: 82.3718 - val_mae: 6.5233\n",
      "Epoch 198/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 86.5036 - mse: 86.5036 - mae: 6.4398 - val_loss: 82.7274 - val_mse: 82.7274 - val_mae: 6.4871\n",
      "Epoch 199/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.6416 - mse: 86.6416 - mae: 6.4493 - val_loss: 83.0356 - val_mse: 83.0356 - val_mae: 6.6224\n",
      "Epoch 200/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.5002 - mse: 86.5003 - mae: 6.4517 - val_loss: 82.6568 - val_mse: 82.6568 - val_mae: 6.4097\n",
      "Epoch 201/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.3603 - mse: 86.3603 - mae: 6.4381 - val_loss: 82.9338 - val_mse: 82.9338 - val_mae: 6.5978\n",
      "Epoch 202/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.4691 - mse: 86.4692 - mae: 6.4449 - val_loss: 82.1654 - val_mse: 82.1654 - val_mae: 6.4738\n",
      "Epoch 203/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 86.4178 - mse: 86.4177 - mae: 6.4436 - val_loss: 83.0465 - val_mse: 83.0465 - val_mae: 6.4205\n",
      "Epoch 204/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.6027 - mse: 86.6026 - mae: 6.4423 - val_loss: 83.0051 - val_mse: 83.0051 - val_mae: 6.5882\n",
      "Epoch 205/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.5037 - mse: 86.5037 - mae: 6.4483 - val_loss: 82.4587 - val_mse: 82.4587 - val_mae: 6.4960\n",
      "Epoch 206/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.4066 - mse: 86.4067 - mae: 6.4333 - val_loss: 82.3868 - val_mse: 82.3868 - val_mae: 6.4876\n",
      "Epoch 207/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.4689 - mse: 86.4690 - mae: 6.4406 - val_loss: 82.5471 - val_mse: 82.5471 - val_mae: 6.4157\n",
      "Epoch 208/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 86.4559 - mse: 86.4559 - mae: 6.4403 - val_loss: 82.2631 - val_mse: 82.2631 - val_mae: 6.5311\n",
      "Epoch 209/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 86.3962 - mse: 86.3962 - mae: 6.4486 - val_loss: 82.5886 - val_mse: 82.5885 - val_mae: 6.4205\n",
      "Epoch 210/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.4431 - mse: 86.4432 - mae: 6.4403 - val_loss: 82.3997 - val_mse: 82.3997 - val_mae: 6.4413\n",
      "Epoch 211/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.3973 - mse: 86.3972 - mae: 6.4345 - val_loss: 82.2272 - val_mse: 82.2272 - val_mae: 6.5014\n",
      "Epoch 212/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.3025 - mse: 86.3025 - mae: 6.4372 - val_loss: 83.0148 - val_mse: 83.0148 - val_mae: 6.5900\n",
      "Epoch 213/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.3510 - mse: 86.3510 - mae: 6.4313 - val_loss: 83.7022 - val_mse: 83.7022 - val_mae: 6.6471\n",
      "Epoch 214/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.2257 - mse: 86.2257 - mae: 6.4368 - val_loss: 82.5390 - val_mse: 82.5391 - val_mae: 6.4492\n",
      "Epoch 215/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.4925 - mse: 86.4926 - mae: 6.4440 - val_loss: 82.2734 - val_mse: 82.2734 - val_mae: 6.4685\n",
      "Epoch 216/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.3339 - mse: 86.3339 - mae: 6.4387 - val_loss: 82.6636 - val_mse: 82.6637 - val_mae: 6.5831\n",
      "Epoch 217/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.4512 - mse: 86.4512 - mae: 6.4337 - val_loss: 82.4405 - val_mse: 82.4405 - val_mae: 6.6030\n",
      "Epoch 218/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 86.3784 - mse: 86.3784 - mae: 6.4410 - val_loss: 82.5758 - val_mse: 82.5757 - val_mae: 6.4904\n",
      "Epoch 219/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.3436 - mse: 86.3436 - mae: 6.4359 - val_loss: 82.1355 - val_mse: 82.1355 - val_mae: 6.4564\n",
      "Epoch 220/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 86.3953 - mse: 86.3953 - mae: 6.4404 - val_loss: 82.0904 - val_mse: 82.0904 - val_mae: 6.4869\n",
      "Epoch 221/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.3318 - mse: 86.3318 - mae: 6.4334 - val_loss: 82.3829 - val_mse: 82.3829 - val_mae: 6.5043\n",
      "Epoch 222/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.3484 - mse: 86.3483 - mae: 6.4295 - val_loss: 82.4200 - val_mse: 82.4199 - val_mae: 6.4219\n",
      "Epoch 223/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.2588 - mse: 86.2588 - mae: 6.4327 - val_loss: 83.0066 - val_mse: 83.0066 - val_mae: 6.4546\n",
      "Epoch 224/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.3703 - mse: 86.3703 - mae: 6.4368 - val_loss: 82.0733 - val_mse: 82.0733 - val_mae: 6.4373\n",
      "Epoch 225/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.4451 - mse: 86.4451 - mae: 6.4404 - val_loss: 82.2774 - val_mse: 82.2774 - val_mae: 6.4255\n",
      "Epoch 226/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.3814 - mse: 86.3813 - mae: 6.4377 - val_loss: 82.2624 - val_mse: 82.2624 - val_mae: 6.4732\n",
      "Epoch 227/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.2671 - mse: 86.2672 - mae: 6.4400 - val_loss: 82.2402 - val_mse: 82.2403 - val_mae: 6.5189\n",
      "Epoch 228/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.3210 - mse: 86.3211 - mae: 6.4371 - val_loss: 82.7115 - val_mse: 82.7115 - val_mae: 6.4641\n",
      "Epoch 229/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.3436 - mse: 86.3435 - mae: 6.4339 - val_loss: 82.5659 - val_mse: 82.5659 - val_mae: 6.4685\n",
      "Epoch 230/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.2887 - mse: 86.2887 - mae: 6.4265 - val_loss: 82.1338 - val_mse: 82.1338 - val_mae: 6.4147\n",
      "Epoch 231/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.1827 - mse: 86.1828 - mae: 6.4268 - val_loss: 82.1637 - val_mse: 82.1637 - val_mae: 6.5259\n",
      "Epoch 232/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.2815 - mse: 86.2815 - mae: 6.4308 - val_loss: 82.7737 - val_mse: 82.7738 - val_mae: 6.5206\n",
      "Epoch 233/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 86.3487 - mse: 86.3487 - mae: 6.4312 - val_loss: 82.4531 - val_mse: 82.4531 - val_mae: 6.4074\n",
      "Epoch 234/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 86.3341 - mse: 86.3341 - mae: 6.4374 - val_loss: 82.3551 - val_mse: 82.3551 - val_mae: 6.5155\n",
      "Epoch 235/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 86.2228 - mse: 86.2228 - mae: 6.4286 - val_loss: 82.4243 - val_mse: 82.4243 - val_mae: 6.4846\n",
      "Epoch 236/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.2123 - mse: 86.2123 - mae: 6.4263 - val_loss: 82.5074 - val_mse: 82.5074 - val_mae: 6.3850\n",
      "Epoch 237/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 86.0965 - mse: 86.0965 - mae: 6.4265 - val_loss: 82.3830 - val_mse: 82.3830 - val_mae: 6.4285\n",
      "Epoch 238/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 86.3629 - mse: 86.3628 - mae: 6.4309 - val_loss: 82.6599 - val_mse: 82.6599 - val_mae: 6.5340\n",
      "Epoch 239/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.3702 - mse: 86.3703 - mae: 6.4433 - val_loss: 82.0906 - val_mse: 82.0906 - val_mae: 6.4627\n",
      "Epoch 240/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.4589 - mse: 86.4589 - mae: 6.4351 - val_loss: 82.2267 - val_mse: 82.2266 - val_mae: 6.5326\n",
      "Epoch 241/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.2852 - mse: 86.2852 - mae: 6.4366 - val_loss: 83.0637 - val_mse: 83.0637 - val_mae: 6.4158\n",
      "Epoch 242/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.3812 - mse: 86.3812 - mae: 6.4321 - val_loss: 82.0842 - val_mse: 82.0842 - val_mae: 6.4592\n",
      "Epoch 243/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.2861 - mse: 86.2860 - mae: 6.4333 - val_loss: 82.3475 - val_mse: 82.3475 - val_mae: 6.4419\n",
      "Epoch 244/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.3169 - mse: 86.3168 - mae: 6.4319 - val_loss: 82.2717 - val_mse: 82.2717 - val_mae: 6.5211\n",
      "Epoch 245/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.3667 - mse: 86.3667 - mae: 6.4398 - val_loss: 82.1143 - val_mse: 82.1143 - val_mae: 6.5197\n",
      "Epoch 246/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 86.3334 - mse: 86.3334 - mae: 6.4293 - val_loss: 82.5838 - val_mse: 82.5838 - val_mae: 6.3977\n",
      "Epoch 247/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.1125 - mse: 86.1124 - mae: 6.4277 - val_loss: 82.8920 - val_mse: 82.8920 - val_mae: 6.4184\n",
      "Epoch 248/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.1294 - mse: 86.1295 - mae: 6.4282 - val_loss: 82.6078 - val_mse: 82.6078 - val_mae: 6.4971\n",
      "Epoch 249/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.2689 - mse: 86.2689 - mae: 6.4243 - val_loss: 82.4877 - val_mse: 82.4877 - val_mae: 6.4740\n",
      "Epoch 250/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.3460 - mse: 86.3460 - mae: 6.4357 - val_loss: 82.7917 - val_mse: 82.7917 - val_mae: 6.4432\n",
      "Epoch 251/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 86.2977 - mse: 86.2977 - mae: 6.4271 - val_loss: 82.2366 - val_mse: 82.2367 - val_mae: 6.5257\n",
      "Epoch 252/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 86.4020 - mse: 86.4020 - mae: 6.4324 - val_loss: 82.3223 - val_mse: 82.3223 - val_mae: 6.5228\n",
      "Epoch 253/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.2465 - mse: 86.2465 - mae: 6.4315 - val_loss: 82.3586 - val_mse: 82.3586 - val_mae: 6.4214\n",
      "Epoch 254/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.3404 - mse: 86.3404 - mae: 6.4349 - val_loss: 82.9299 - val_mse: 82.9299 - val_mae: 6.4341\n",
      "Epoch 255/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 86.2427 - mse: 86.2427 - mae: 6.4295 - val_loss: 82.3081 - val_mse: 82.3081 - val_mae: 6.4529\n",
      "Epoch 256/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.3071 - mse: 86.3070 - mae: 6.4286 - val_loss: 82.6121 - val_mse: 82.6121 - val_mae: 6.5197\n",
      "Epoch 257/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 86.2034 - mse: 86.2035 - mae: 6.4274 - val_loss: 82.2813 - val_mse: 82.2813 - val_mae: 6.5033\n",
      "Epoch 258/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.2781 - mse: 86.2781 - mae: 6.4340 - val_loss: 82.6199 - val_mse: 82.6199 - val_mae: 6.5518\n",
      "Epoch 259/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.2877 - mse: 86.2877 - mae: 6.4311 - val_loss: 82.6806 - val_mse: 82.6806 - val_mae: 6.4327\n",
      "Epoch 260/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.4311 - mse: 86.4311 - mae: 6.4412 - val_loss: 82.3023 - val_mse: 82.3023 - val_mae: 6.4207\n",
      "Epoch 261/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.3273 - mse: 86.3272 - mae: 6.4365 - val_loss: 82.9663 - val_mse: 82.9663 - val_mae: 6.6108\n",
      "Epoch 262/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.2100 - mse: 86.2100 - mae: 6.4378 - val_loss: 82.4148 - val_mse: 82.4148 - val_mae: 6.4921\n",
      "Epoch 263/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 86.2217 - mse: 86.2218 - mae: 6.4356 - val_loss: 82.8244 - val_mse: 82.8244 - val_mae: 6.4748\n",
      "Epoch 264/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 86.1283 - mse: 86.1283 - mae: 6.4275 - val_loss: 83.2152 - val_mse: 83.2152 - val_mae: 6.5921\n",
      "Epoch 265/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.1919 - mse: 86.1919 - mae: 6.4342 - val_loss: 83.2614 - val_mse: 83.2614 - val_mae: 6.3625\n",
      "Epoch 266/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.2414 - mse: 86.2413 - mae: 6.4273 - val_loss: 82.3139 - val_mse: 82.3139 - val_mae: 6.4861\n",
      "Epoch 267/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 86.3344 - mse: 86.3344 - mae: 6.4354 - val_loss: 82.5082 - val_mse: 82.5081 - val_mae: 6.4519\n",
      "Epoch 268/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.0742 - mse: 86.0742 - mae: 6.4205 - val_loss: 82.9452 - val_mse: 82.9452 - val_mae: 6.4333\n",
      "Epoch 269/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.1766 - mse: 86.1766 - mae: 6.4202 - val_loss: 82.5255 - val_mse: 82.5255 - val_mae: 6.5270\n",
      "Epoch 270/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.2657 - mse: 86.2659 - mae: 6.4342 - val_loss: 82.3419 - val_mse: 82.3419 - val_mae: 6.4394\n",
      "Epoch 271/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0944 - mse: 86.0944 - mae: 6.4279 - val_loss: 82.5521 - val_mse: 82.5521 - val_mae: 6.4130\n",
      "Epoch 272/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.3258 - mse: 86.3258 - mae: 6.4374 - val_loss: 82.7630 - val_mse: 82.7630 - val_mae: 6.5460\n",
      "Epoch 273/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 86.0419 - mse: 86.0418 - mae: 6.4279 - val_loss: 83.3513 - val_mse: 83.3513 - val_mae: 6.4131\n",
      "Epoch 274/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.3509 - mse: 86.3508 - mae: 6.4333 - val_loss: 82.2790 - val_mse: 82.2790 - val_mae: 6.4222\n",
      "Epoch 275/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.2674 - mse: 86.2674 - mae: 6.4346 - val_loss: 82.2772 - val_mse: 82.2772 - val_mae: 6.4170\n",
      "Epoch 276/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 86.2479 - mse: 86.2479 - mae: 6.4325 - val_loss: 82.8191 - val_mse: 82.8191 - val_mae: 6.4048\n",
      "Epoch 277/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.1299 - mse: 86.1300 - mae: 6.4223 - val_loss: 82.1678 - val_mse: 82.1678 - val_mae: 6.5006\n",
      "Epoch 278/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.1043 - mse: 86.1043 - mae: 6.4267 - val_loss: 82.6380 - val_mse: 82.6380 - val_mae: 6.5786\n",
      "Epoch 279/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.0801 - mse: 86.0800 - mae: 6.4255 - val_loss: 82.7347 - val_mse: 82.7347 - val_mae: 6.4787\n",
      "Epoch 280/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.1286 - mse: 86.1285 - mae: 6.4274 - val_loss: 83.0526 - val_mse: 83.0526 - val_mae: 6.5284\n",
      "Epoch 281/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.1664 - mse: 86.1664 - mae: 6.4308 - val_loss: 83.5910 - val_mse: 83.5910 - val_mae: 6.3847\n",
      "Epoch 282/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.1901 - mse: 86.1902 - mae: 6.4222 - val_loss: 82.1366 - val_mse: 82.1366 - val_mae: 6.5081\n",
      "Epoch 283/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.9616 - mse: 85.9616 - mae: 6.4283 - val_loss: 82.5856 - val_mse: 82.5856 - val_mae: 6.4231\n",
      "Epoch 284/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 86.1910 - mse: 86.1910 - mae: 6.4283 - val_loss: 82.1161 - val_mse: 82.1161 - val_mae: 6.4218\n",
      "Epoch 285/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 86.2320 - mse: 86.2321 - mae: 6.4331 - val_loss: 82.4948 - val_mse: 82.4948 - val_mae: 6.4098\n",
      "Epoch 286/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 86.1228 - mse: 86.1227 - mae: 6.4258 - val_loss: 82.5942 - val_mse: 82.5942 - val_mae: 6.4478\n",
      "Epoch 287/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 86.2529 - mse: 86.2529 - mae: 6.4234 - val_loss: 82.8411 - val_mse: 82.8411 - val_mae: 6.5945\n",
      "Epoch 288/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 86.2732 - mse: 86.2732 - mae: 6.4320 - val_loss: 82.2907 - val_mse: 82.2907 - val_mae: 6.5121\n",
      "Epoch 289/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.1501 - mse: 86.1501 - mae: 6.4321 - val_loss: 83.5295 - val_mse: 83.5294 - val_mae: 6.4024\n",
      "Epoch 290/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 86.2230 - mse: 86.2230 - mae: 6.4285 - val_loss: 82.4957 - val_mse: 82.4958 - val_mae: 6.4191\n",
      "Epoch 291/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.1574 - mse: 86.1573 - mae: 6.4198 - val_loss: 82.5315 - val_mse: 82.5315 - val_mae: 6.4496\n",
      "Epoch 292/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.0885 - mse: 86.0885 - mae: 6.4226 - val_loss: 82.1067 - val_mse: 82.1067 - val_mae: 6.5117\n",
      "Epoch 293/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.9365 - mse: 85.9365 - mae: 6.4221 - val_loss: 82.0248 - val_mse: 82.0249 - val_mae: 6.4783\n",
      "Epoch 294/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.1066 - mse: 86.1066 - mae: 6.4255 - val_loss: 82.4602 - val_mse: 82.4602 - val_mae: 6.5523\n",
      "Epoch 295/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.9729 - mse: 85.9729 - mae: 6.4167 - val_loss: 82.4174 - val_mse: 82.4174 - val_mae: 6.5117\n",
      "Epoch 296/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.1279 - mse: 86.1278 - mae: 6.4282 - val_loss: 82.0466 - val_mse: 82.0465 - val_mae: 6.5001\n",
      "Epoch 297/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.1145 - mse: 86.1145 - mae: 6.4313 - val_loss: 82.5234 - val_mse: 82.5235 - val_mae: 6.5771\n",
      "Epoch 298/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.2071 - mse: 86.2070 - mae: 6.4364 - val_loss: 82.4742 - val_mse: 82.4742 - val_mae: 6.4990\n",
      "Epoch 299/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 86.1579 - mse: 86.1579 - mae: 6.4249 - val_loss: 82.9269 - val_mse: 82.9269 - val_mae: 6.5551\n",
      "Epoch 300/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.0408 - mse: 86.0408 - mae: 6.4251 - val_loss: 82.9924 - val_mse: 82.9924 - val_mae: 6.3865\n",
      "Epoch 301/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.2052 - mse: 86.2052 - mae: 6.4216 - val_loss: 82.3323 - val_mse: 82.3323 - val_mae: 6.4736\n",
      "Epoch 302/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.1035 - mse: 86.1034 - mae: 6.4200 - val_loss: 82.3429 - val_mse: 82.3429 - val_mae: 6.4894\n",
      "Epoch 303/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0526 - mse: 86.0526 - mae: 6.4220 - val_loss: 82.4121 - val_mse: 82.4121 - val_mae: 6.4178\n",
      "Epoch 304/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.1427 - mse: 86.1427 - mae: 6.4261 - val_loss: 82.3878 - val_mse: 82.3877 - val_mae: 6.4683\n",
      "Epoch 305/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 86.0722 - mse: 86.0723 - mae: 6.4223 - val_loss: 82.6317 - val_mse: 82.6316 - val_mae: 6.4488\n",
      "Epoch 306/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 86.1035 - mse: 86.1035 - mae: 6.4293 - val_loss: 82.5497 - val_mse: 82.5497 - val_mae: 6.5624\n",
      "Epoch 307/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0627 - mse: 86.0627 - mae: 6.4205 - val_loss: 82.2455 - val_mse: 82.2455 - val_mae: 6.5122\n",
      "Epoch 308/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.0217 - mse: 86.0216 - mae: 6.4294 - val_loss: 82.6650 - val_mse: 82.6650 - val_mae: 6.4244\n",
      "Epoch 309/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 86.0874 - mse: 86.0874 - mae: 6.4241 - val_loss: 82.4905 - val_mse: 82.4904 - val_mae: 6.4176\n",
      "Epoch 310/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 86.0705 - mse: 86.0705 - mae: 6.4270 - val_loss: 82.5245 - val_mse: 82.5246 - val_mae: 6.5603\n",
      "Epoch 311/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.9601 - mse: 85.9601 - mae: 6.4240 - val_loss: 82.2864 - val_mse: 82.2864 - val_mae: 6.4613\n",
      "Epoch 312/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.1884 - mse: 86.1884 - mae: 6.4351 - val_loss: 82.4101 - val_mse: 82.4101 - val_mae: 6.5644\n",
      "Epoch 313/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.1543 - mse: 86.1543 - mae: 6.4250 - val_loss: 82.1435 - val_mse: 82.1435 - val_mae: 6.4808\n",
      "Epoch 314/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0972 - mse: 86.0972 - mae: 6.4209 - val_loss: 83.4477 - val_mse: 83.4477 - val_mae: 6.7242\n",
      "Epoch 315/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.0496 - mse: 86.0497 - mae: 6.4354 - val_loss: 81.9736 - val_mse: 81.9737 - val_mae: 6.4667\n",
      "Epoch 316/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0402 - mse: 86.0401 - mae: 6.4274 - val_loss: 82.6351 - val_mse: 82.6351 - val_mae: 6.5954\n",
      "Epoch 317/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0156 - mse: 86.0155 - mae: 6.4197 - val_loss: 82.4716 - val_mse: 82.4715 - val_mae: 6.4956\n",
      "Epoch 318/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.0859 - mse: 86.0859 - mae: 6.4219 - val_loss: 82.5060 - val_mse: 82.5061 - val_mae: 6.4577\n",
      "Epoch 319/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 86.0741 - mse: 86.0740 - mae: 6.4275 - val_loss: 82.5339 - val_mse: 82.5339 - val_mae: 6.4653\n",
      "Epoch 320/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.1100 - mse: 86.1100 - mae: 6.4241 - val_loss: 82.6962 - val_mse: 82.6962 - val_mae: 6.4655\n",
      "Epoch 321/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 86.1065 - mse: 86.1065 - mae: 6.4273 - val_loss: 82.3379 - val_mse: 82.3379 - val_mae: 6.4811\n",
      "Epoch 322/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.0750 - mse: 86.0751 - mae: 6.4227 - val_loss: 82.8435 - val_mse: 82.8435 - val_mae: 6.6083\n",
      "Epoch 323/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.9744 - mse: 85.9744 - mae: 6.4300 - val_loss: 82.6656 - val_mse: 82.6655 - val_mae: 6.5497\n",
      "Epoch 324/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.1084 - mse: 86.1084 - mae: 6.4276 - val_loss: 82.0007 - val_mse: 82.0007 - val_mae: 6.4430\n",
      "Epoch 325/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0125 - mse: 86.0124 - mae: 6.4191 - val_loss: 82.2405 - val_mse: 82.2405 - val_mae: 6.5081\n",
      "Epoch 326/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0179 - mse: 86.0178 - mae: 6.4212 - val_loss: 82.8631 - val_mse: 82.8631 - val_mae: 6.4834\n",
      "Epoch 327/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.0744 - mse: 86.0743 - mae: 6.4264 - val_loss: 82.3806 - val_mse: 82.3807 - val_mae: 6.4172\n",
      "Epoch 328/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 86.0606 - mse: 86.0607 - mae: 6.4228 - val_loss: 82.0696 - val_mse: 82.0696 - val_mae: 6.4732\n",
      "Epoch 329/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.1957 - mse: 86.1956 - mae: 6.4252 - val_loss: 82.3960 - val_mse: 82.3960 - val_mae: 6.4362\n",
      "Epoch 330/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 86.0050 - mse: 86.0049 - mae: 6.4256 - val_loss: 83.1880 - val_mse: 83.1880 - val_mae: 6.6281\n",
      "Epoch 331/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.1118 - mse: 86.1118 - mae: 6.4283 - val_loss: 82.1613 - val_mse: 82.1613 - val_mae: 6.4439\n",
      "Epoch 332/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0917 - mse: 86.0917 - mae: 6.4195 - val_loss: 82.5840 - val_mse: 82.5840 - val_mae: 6.5621\n",
      "Epoch 333/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.9518 - mse: 85.9518 - mae: 6.4336 - val_loss: 82.7164 - val_mse: 82.7164 - val_mae: 6.3790\n",
      "Epoch 334/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 86.1047 - mse: 86.1048 - mae: 6.4252 - val_loss: 82.3920 - val_mse: 82.3921 - val_mae: 6.4101\n",
      "Epoch 335/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0883 - mse: 86.0883 - mae: 6.4220 - val_loss: 82.4489 - val_mse: 82.4489 - val_mae: 6.4133\n",
      "Epoch 336/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.0744 - mse: 86.0744 - mae: 6.4230 - val_loss: 82.0315 - val_mse: 82.0314 - val_mae: 6.5178\n",
      "Epoch 337/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.0139 - mse: 86.0138 - mae: 6.4263 - val_loss: 82.7885 - val_mse: 82.7885 - val_mae: 6.4443\n",
      "Epoch 338/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9410 - mse: 85.9410 - mae: 6.4135 - val_loss: 82.1953 - val_mse: 82.1953 - val_mae: 6.5099\n",
      "Epoch 339/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.9818 - mse: 85.9817 - mae: 6.4294 - val_loss: 82.0929 - val_mse: 82.0929 - val_mae: 6.5293\n",
      "Epoch 340/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.9756 - mse: 85.9755 - mae: 6.4259 - val_loss: 82.3629 - val_mse: 82.3629 - val_mae: 6.4979\n",
      "Epoch 341/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8657 - mse: 85.8656 - mae: 6.4153 - val_loss: 82.8002 - val_mse: 82.8003 - val_mae: 6.5617\n",
      "Epoch 342/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 86.0072 - mse: 86.0071 - mae: 6.4212 - val_loss: 82.5203 - val_mse: 82.5203 - val_mae: 6.6057\n",
      "Epoch 343/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8452 - mse: 85.8452 - mae: 6.4167 - val_loss: 82.2790 - val_mse: 82.2790 - val_mae: 6.4174\n",
      "Epoch 344/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.9472 - mse: 85.9472 - mae: 6.4208 - val_loss: 82.2129 - val_mse: 82.2129 - val_mae: 6.4727\n",
      "Epoch 345/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 86.0529 - mse: 86.0529 - mae: 6.4245 - val_loss: 82.0819 - val_mse: 82.0819 - val_mae: 6.4369\n",
      "Epoch 346/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8367 - mse: 85.8367 - mae: 6.4188 - val_loss: 82.3026 - val_mse: 82.3026 - val_mae: 6.4231\n",
      "Epoch 347/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.9817 - mse: 85.9817 - mae: 6.4299 - val_loss: 83.3335 - val_mse: 83.3335 - val_mae: 6.3940\n",
      "Epoch 348/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.9765 - mse: 85.9765 - mae: 6.4187 - val_loss: 82.1403 - val_mse: 82.1403 - val_mae: 6.5026\n",
      "Epoch 349/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9059 - mse: 85.9059 - mae: 6.4262 - val_loss: 82.2627 - val_mse: 82.2626 - val_mae: 6.4815\n",
      "Epoch 350/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.9851 - mse: 85.9850 - mae: 6.4239 - val_loss: 82.2453 - val_mse: 82.2454 - val_mae: 6.4944\n",
      "Epoch 351/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.9080 - mse: 85.9080 - mae: 6.4248 - val_loss: 83.4130 - val_mse: 83.4130 - val_mae: 6.4101\n",
      "Epoch 352/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9986 - mse: 85.9987 - mae: 6.4220 - val_loss: 82.2046 - val_mse: 82.2046 - val_mae: 6.5368\n",
      "Epoch 353/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9991 - mse: 85.9990 - mae: 6.4214 - val_loss: 82.7537 - val_mse: 82.7537 - val_mae: 6.4759\n",
      "Epoch 354/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9576 - mse: 85.9575 - mae: 6.4240 - val_loss: 83.5416 - val_mse: 83.5416 - val_mae: 6.5655\n",
      "Epoch 355/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.9501 - mse: 85.9500 - mae: 6.4257 - val_loss: 82.1683 - val_mse: 82.1682 - val_mae: 6.4893\n",
      "Epoch 356/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.0231 - mse: 86.0231 - mae: 6.4264 - val_loss: 82.1225 - val_mse: 82.1225 - val_mae: 6.5188\n",
      "Epoch 357/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9951 - mse: 85.9951 - mae: 6.4193 - val_loss: 82.8566 - val_mse: 82.8567 - val_mae: 6.4177\n",
      "Epoch 358/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 86.0491 - mse: 86.0491 - mae: 6.4234 - val_loss: 82.3668 - val_mse: 82.3668 - val_mae: 6.4424\n",
      "Epoch 359/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8743 - mse: 85.8743 - mae: 6.4168 - val_loss: 83.7068 - val_mse: 83.7068 - val_mae: 6.6304\n",
      "Epoch 360/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 86.1191 - mse: 86.1191 - mae: 6.4273 - val_loss: 82.8614 - val_mse: 82.8614 - val_mae: 6.4157\n",
      "Epoch 361/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.9355 - mse: 85.9356 - mae: 6.4258 - val_loss: 82.3533 - val_mse: 82.3534 - val_mae: 6.5346\n",
      "Epoch 362/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 86.0406 - mse: 86.0406 - mae: 6.4273 - val_loss: 82.0011 - val_mse: 82.0011 - val_mae: 6.4500\n",
      "Epoch 363/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9949 - mse: 85.9949 - mae: 6.4260 - val_loss: 82.2598 - val_mse: 82.2598 - val_mae: 6.4676\n",
      "Epoch 364/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 86.0426 - mse: 86.0426 - mae: 6.4235 - val_loss: 82.5251 - val_mse: 82.5251 - val_mae: 6.4327\n",
      "Epoch 365/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.9930 - mse: 85.9930 - mae: 6.4253 - val_loss: 82.1953 - val_mse: 82.1953 - val_mae: 6.4485\n",
      "Epoch 366/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.9117 - mse: 85.9116 - mae: 6.4165 - val_loss: 82.3220 - val_mse: 82.3220 - val_mae: 6.5079\n",
      "Epoch 367/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7928 - mse: 85.7927 - mae: 6.4163 - val_loss: 82.6315 - val_mse: 82.6315 - val_mae: 6.5646\n",
      "Epoch 368/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0730 - mse: 86.0731 - mae: 6.4240 - val_loss: 82.4376 - val_mse: 82.4376 - val_mae: 6.4710\n",
      "Epoch 369/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0545 - mse: 86.0545 - mae: 6.4286 - val_loss: 82.9126 - val_mse: 82.9126 - val_mae: 6.4443\n",
      "Epoch 370/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 86.0451 - mse: 86.0451 - mae: 6.4284 - val_loss: 81.9931 - val_mse: 81.9931 - val_mae: 6.4577\n",
      "Epoch 371/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.0785 - mse: 86.0784 - mae: 6.4191 - val_loss: 81.9646 - val_mse: 81.9646 - val_mae: 6.4834\n",
      "Epoch 372/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8354 - mse: 85.8354 - mae: 6.4218 - val_loss: 82.2422 - val_mse: 82.2421 - val_mae: 6.4590\n",
      "Epoch 373/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.8827 - mse: 85.8827 - mae: 6.4196 - val_loss: 82.8971 - val_mse: 82.8971 - val_mae: 6.4486\n",
      "Epoch 374/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.9949 - mse: 85.9950 - mae: 6.4230 - val_loss: 82.1311 - val_mse: 82.1311 - val_mae: 6.5082\n",
      "Epoch 375/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 86.0461 - mse: 86.0461 - mae: 6.4305 - val_loss: 83.1042 - val_mse: 83.1042 - val_mae: 6.5958\n",
      "Epoch 376/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 86.0328 - mse: 86.0328 - mae: 6.4230 - val_loss: 82.1790 - val_mse: 82.1790 - val_mae: 6.4495\n",
      "Epoch 377/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8207 - mse: 85.8207 - mae: 6.4179 - val_loss: 82.5807 - val_mse: 82.5808 - val_mae: 6.5683\n",
      "Epoch 378/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8661 - mse: 85.8662 - mae: 6.4251 - val_loss: 82.5644 - val_mse: 82.5644 - val_mae: 6.4630\n",
      "Epoch 379/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7594 - mse: 85.7594 - mae: 6.4173 - val_loss: 82.3173 - val_mse: 82.3173 - val_mae: 6.5003\n",
      "Epoch 380/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0553 - mse: 86.0553 - mae: 6.4225 - val_loss: 82.0401 - val_mse: 82.0400 - val_mae: 6.4793\n",
      "Epoch 381/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8454 - mse: 85.8455 - mae: 6.4130 - val_loss: 82.3202 - val_mse: 82.3202 - val_mae: 6.5352\n",
      "Epoch 382/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 86.0594 - mse: 86.0594 - mae: 6.4251 - val_loss: 84.3767 - val_mse: 84.3767 - val_mae: 6.4598\n",
      "Epoch 383/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 86.0213 - mse: 86.0212 - mae: 6.4236 - val_loss: 82.3150 - val_mse: 82.3150 - val_mae: 6.4655\n",
      "Epoch 384/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 86.0638 - mse: 86.0637 - mae: 6.4213 - val_loss: 82.2158 - val_mse: 82.2158 - val_mae: 6.4410\n",
      "Epoch 385/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9571 - mse: 85.9572 - mae: 6.4292 - val_loss: 82.4582 - val_mse: 82.4582 - val_mae: 6.4543\n",
      "Epoch 386/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.9983 - mse: 85.9982 - mae: 6.4212 - val_loss: 82.4136 - val_mse: 82.4136 - val_mae: 6.5030\n",
      "Epoch 387/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 86.0069 - mse: 86.0068 - mae: 6.4238 - val_loss: 82.2300 - val_mse: 82.2301 - val_mae: 6.5098\n",
      "Epoch 388/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.9213 - mse: 85.9213 - mae: 6.4218 - val_loss: 82.6035 - val_mse: 82.6035 - val_mae: 6.5540\n",
      "Epoch 389/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8542 - mse: 85.8543 - mae: 6.4204 - val_loss: 82.1097 - val_mse: 82.1097 - val_mae: 6.4599\n",
      "Epoch 390/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.9093 - mse: 85.9094 - mae: 6.4222 - val_loss: 82.0802 - val_mse: 82.0803 - val_mae: 6.4435\n",
      "Epoch 391/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9433 - mse: 85.9432 - mae: 6.4266 - val_loss: 82.5554 - val_mse: 82.5554 - val_mae: 6.5414\n",
      "Epoch 392/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.9754 - mse: 85.9755 - mae: 6.4229 - val_loss: 82.7393 - val_mse: 82.7393 - val_mae: 6.5260\n",
      "Epoch 393/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.9351 - mse: 85.9350 - mae: 6.4204 - val_loss: 82.1543 - val_mse: 82.1543 - val_mae: 6.4361\n",
      "Epoch 394/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.9983 - mse: 85.9983 - mae: 6.4120 - val_loss: 82.5249 - val_mse: 82.5249 - val_mae: 6.5477\n",
      "Epoch 395/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8778 - mse: 85.8778 - mae: 6.4188 - val_loss: 82.4167 - val_mse: 82.4167 - val_mae: 6.5634\n",
      "Epoch 396/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.9085 - mse: 85.9086 - mae: 6.4191 - val_loss: 82.1850 - val_mse: 82.1850 - val_mae: 6.5002\n",
      "Epoch 397/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8355 - mse: 85.8355 - mae: 6.4194 - val_loss: 82.3951 - val_mse: 82.3951 - val_mae: 6.5106\n",
      "Epoch 398/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8323 - mse: 85.8323 - mae: 6.4248 - val_loss: 82.4719 - val_mse: 82.4718 - val_mae: 6.5030\n",
      "Epoch 399/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.8450 - mse: 85.8450 - mae: 6.4145 - val_loss: 82.0813 - val_mse: 82.0813 - val_mae: 6.4535\n",
      "Epoch 400/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7980 - mse: 85.7981 - mae: 6.4176 - val_loss: 82.5015 - val_mse: 82.5015 - val_mae: 6.4942\n",
      "Epoch 401/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9802 - mse: 85.9802 - mae: 6.4172 - val_loss: 82.0512 - val_mse: 82.0512 - val_mae: 6.4767\n",
      "Epoch 402/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8862 - mse: 85.8862 - mae: 6.4231 - val_loss: 82.4333 - val_mse: 82.4333 - val_mae: 6.5008\n",
      "Epoch 403/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.9295 - mse: 85.9296 - mae: 6.4186 - val_loss: 82.0794 - val_mse: 82.0794 - val_mae: 6.4779\n",
      "Epoch 404/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.9423 - mse: 85.9423 - mae: 6.4233 - val_loss: 82.2890 - val_mse: 82.2890 - val_mae: 6.4942\n",
      "Epoch 405/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 86.0290 - mse: 86.0290 - mae: 6.4283 - val_loss: 82.1012 - val_mse: 82.1012 - val_mae: 6.4696\n",
      "Epoch 406/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9337 - mse: 85.9338 - mae: 6.4262 - val_loss: 82.4860 - val_mse: 82.4861 - val_mae: 6.4272\n",
      "Epoch 407/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.9562 - mse: 85.9562 - mae: 6.4204 - val_loss: 82.3990 - val_mse: 82.3990 - val_mae: 6.4570\n",
      "Epoch 408/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8959 - mse: 85.8959 - mae: 6.4225 - val_loss: 82.4789 - val_mse: 82.4789 - val_mae: 6.4050\n",
      "Epoch 409/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9429 - mse: 85.9429 - mae: 6.4166 - val_loss: 82.2022 - val_mse: 82.2022 - val_mae: 6.4268\n",
      "Epoch 410/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8143 - mse: 85.8143 - mae: 6.4287 - val_loss: 82.4998 - val_mse: 82.4998 - val_mae: 6.4902\n",
      "Epoch 411/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9796 - mse: 85.9796 - mae: 6.4222 - val_loss: 82.4526 - val_mse: 82.4527 - val_mae: 6.5415\n",
      "Epoch 412/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.9228 - mse: 85.9228 - mae: 6.4191 - val_loss: 82.7094 - val_mse: 82.7094 - val_mae: 6.4692\n",
      "Epoch 413/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.9394 - mse: 85.9394 - mae: 6.4286 - val_loss: 82.6744 - val_mse: 82.6744 - val_mae: 6.5432\n",
      "Epoch 414/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.8577 - mse: 85.8576 - mae: 6.4227 - val_loss: 82.3273 - val_mse: 82.3273 - val_mae: 6.5100\n",
      "Epoch 415/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9525 - mse: 85.9524 - mae: 6.4202 - val_loss: 82.2102 - val_mse: 82.2103 - val_mae: 6.4474\n",
      "Epoch 416/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9891 - mse: 85.9891 - mae: 6.4181 - val_loss: 82.3853 - val_mse: 82.3854 - val_mae: 6.4191\n",
      "Epoch 417/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9006 - mse: 85.9007 - mae: 6.4187 - val_loss: 82.0852 - val_mse: 82.0852 - val_mae: 6.4715\n",
      "Epoch 418/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7636 - mse: 85.7636 - mae: 6.4191 - val_loss: 82.5765 - val_mse: 82.5766 - val_mae: 6.5592\n",
      "Epoch 419/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7852 - mse: 85.7852 - mae: 6.4229 - val_loss: 82.4940 - val_mse: 82.4940 - val_mae: 6.4233\n",
      "Epoch 420/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8597 - mse: 85.8597 - mae: 6.4214 - val_loss: 82.8064 - val_mse: 82.8064 - val_mae: 6.3933\n",
      "Epoch 421/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7608 - mse: 85.7608 - mae: 6.4181 - val_loss: 82.5116 - val_mse: 82.5116 - val_mae: 6.4740\n",
      "Epoch 422/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9595 - mse: 85.9594 - mae: 6.4273 - val_loss: 81.9974 - val_mse: 81.9974 - val_mae: 6.4614\n",
      "Epoch 423/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9390 - mse: 85.9390 - mae: 6.4158 - val_loss: 82.8199 - val_mse: 82.8199 - val_mae: 6.6235\n",
      "Epoch 424/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9631 - mse: 85.9631 - mae: 6.4210 - val_loss: 82.2621 - val_mse: 82.2622 - val_mae: 6.4474\n",
      "Epoch 425/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8101 - mse: 85.8102 - mae: 6.4173 - val_loss: 82.4530 - val_mse: 82.4530 - val_mae: 6.5427\n",
      "Epoch 426/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8217 - mse: 85.8218 - mae: 6.4096 - val_loss: 82.7692 - val_mse: 82.7692 - val_mae: 6.5950\n",
      "Epoch 427/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.9397 - mse: 85.9398 - mae: 6.4232 - val_loss: 82.2593 - val_mse: 82.2593 - val_mae: 6.3965\n",
      "Epoch 428/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8985 - mse: 85.8984 - mae: 6.4233 - val_loss: 82.5317 - val_mse: 82.5317 - val_mae: 6.5528\n",
      "Epoch 429/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8934 - mse: 85.8933 - mae: 6.4279 - val_loss: 82.0826 - val_mse: 82.0826 - val_mae: 6.4574\n",
      "Epoch 430/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8897 - mse: 85.8897 - mae: 6.4217 - val_loss: 82.2954 - val_mse: 82.2954 - val_mae: 6.4295\n",
      "Epoch 431/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8116 - mse: 85.8116 - mae: 6.4137 - val_loss: 82.4555 - val_mse: 82.4555 - val_mae: 6.6052\n",
      "Epoch 432/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7632 - mse: 85.7632 - mae: 6.4179 - val_loss: 83.1532 - val_mse: 83.1532 - val_mae: 6.5843\n",
      "Epoch 433/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8616 - mse: 85.8616 - mae: 6.4235 - val_loss: 82.3292 - val_mse: 82.3292 - val_mae: 6.4226\n",
      "Epoch 434/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.8687 - mse: 85.8687 - mae: 6.4178 - val_loss: 82.2454 - val_mse: 82.2455 - val_mae: 6.4919\n",
      "Epoch 435/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9716 - mse: 85.9715 - mae: 6.4206 - val_loss: 82.6782 - val_mse: 82.6782 - val_mae: 6.4901\n",
      "Epoch 436/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8668 - mse: 85.8668 - mae: 6.4235 - val_loss: 82.1929 - val_mse: 82.1929 - val_mae: 6.5496\n",
      "Epoch 437/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8881 - mse: 85.8881 - mae: 6.4261 - val_loss: 82.1903 - val_mse: 82.1903 - val_mae: 6.4849\n",
      "Epoch 438/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8272 - mse: 85.8273 - mae: 6.4196 - val_loss: 82.4853 - val_mse: 82.4853 - val_mae: 6.4402\n",
      "Epoch 439/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8058 - mse: 85.8058 - mae: 6.4100 - val_loss: 81.8656 - val_mse: 81.8656 - val_mae: 6.4753\n",
      "Epoch 440/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8518 - mse: 85.8518 - mae: 6.4121 - val_loss: 82.2354 - val_mse: 82.2354 - val_mae: 6.4824\n",
      "Epoch 441/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.5732 - mse: 85.5732 - mae: 6.4052 - val_loss: 82.8430 - val_mse: 82.8430 - val_mae: 6.6023\n",
      "Epoch 442/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9137 - mse: 85.9138 - mae: 6.4189 - val_loss: 81.8444 - val_mse: 81.8444 - val_mae: 6.5005\n",
      "Epoch 443/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8695 - mse: 85.8695 - mae: 6.4205 - val_loss: 82.0112 - val_mse: 82.0113 - val_mae: 6.4463\n",
      "Epoch 444/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.7300 - mse: 85.7301 - mae: 6.4206 - val_loss: 82.3889 - val_mse: 82.3889 - val_mae: 6.5041\n",
      "Epoch 445/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7689 - mse: 85.7688 - mae: 6.4174 - val_loss: 82.5070 - val_mse: 82.5070 - val_mae: 6.5975\n",
      "Epoch 446/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9509 - mse: 85.9509 - mae: 6.4208 - val_loss: 82.3049 - val_mse: 82.3049 - val_mae: 6.5033\n",
      "Epoch 447/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7937 - mse: 85.7937 - mae: 6.4116 - val_loss: 82.0673 - val_mse: 82.0673 - val_mae: 6.5026\n",
      "Epoch 448/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7630 - mse: 85.7629 - mae: 6.4165 - val_loss: 82.0925 - val_mse: 82.0925 - val_mae: 6.4697\n",
      "Epoch 449/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8687 - mse: 85.8688 - mae: 6.4094 - val_loss: 82.1245 - val_mse: 82.1245 - val_mae: 6.5449\n",
      "Epoch 450/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8134 - mse: 85.8135 - mae: 6.4279 - val_loss: 82.4574 - val_mse: 82.4575 - val_mae: 6.4557\n",
      "Epoch 451/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8300 - mse: 85.8300 - mae: 6.4180 - val_loss: 82.0340 - val_mse: 82.0340 - val_mae: 6.5085\n",
      "Epoch 452/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8255 - mse: 85.8255 - mae: 6.4178 - val_loss: 82.2379 - val_mse: 82.2379 - val_mae: 6.4822\n",
      "Epoch 453/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.7925 - mse: 85.7925 - mae: 6.4135 - val_loss: 82.0894 - val_mse: 82.0894 - val_mae: 6.4937\n",
      "Epoch 454/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7652 - mse: 85.7652 - mae: 6.4167 - val_loss: 81.9339 - val_mse: 81.9339 - val_mae: 6.4678\n",
      "Epoch 455/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7175 - mse: 85.7176 - mae: 6.4091 - val_loss: 82.5242 - val_mse: 82.5241 - val_mae: 6.3857\n",
      "Epoch 456/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7633 - mse: 85.7633 - mae: 6.4198 - val_loss: 82.4428 - val_mse: 82.4428 - val_mae: 6.4050\n",
      "Epoch 457/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8792 - mse: 85.8793 - mae: 6.4137 - val_loss: 82.3307 - val_mse: 82.3307 - val_mae: 6.4961\n",
      "Epoch 458/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7746 - mse: 85.7747 - mae: 6.4177 - val_loss: 82.1563 - val_mse: 82.1562 - val_mae: 6.4863\n",
      "Epoch 459/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.8529 - mse: 85.8529 - mae: 6.4150 - val_loss: 81.9929 - val_mse: 81.9928 - val_mae: 6.4918\n",
      "Epoch 460/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9489 - mse: 85.9488 - mae: 6.4164 - val_loss: 82.0069 - val_mse: 82.0069 - val_mae: 6.4587\n",
      "Epoch 461/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.6626 - mse: 85.6626 - mae: 6.4049 - val_loss: 82.5960 - val_mse: 82.5960 - val_mae: 6.6416\n",
      "Epoch 462/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.9746 - mse: 85.9745 - mae: 6.4251 - val_loss: 82.1632 - val_mse: 82.1632 - val_mae: 6.5650\n",
      "Epoch 463/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.7558 - mse: 85.7558 - mae: 6.4165 - val_loss: 81.9424 - val_mse: 81.9424 - val_mae: 6.4821\n",
      "Epoch 464/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7059 - mse: 85.7059 - mae: 6.4151 - val_loss: 82.5244 - val_mse: 82.5244 - val_mae: 6.4606\n",
      "Epoch 465/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7776 - mse: 85.7777 - mae: 6.4125 - val_loss: 83.1405 - val_mse: 83.1405 - val_mae: 6.4643\n",
      "Epoch 466/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8180 - mse: 85.8180 - mae: 6.4154 - val_loss: 82.6958 - val_mse: 82.6958 - val_mae: 6.4641\n",
      "Epoch 467/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8126 - mse: 85.8126 - mae: 6.4153 - val_loss: 82.0785 - val_mse: 82.0785 - val_mae: 6.4817\n",
      "Epoch 468/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8157 - mse: 85.8157 - mae: 6.4186 - val_loss: 82.3837 - val_mse: 82.3837 - val_mae: 6.4441\n",
      "Epoch 469/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8112 - mse: 85.8112 - mae: 6.4121 - val_loss: 81.9281 - val_mse: 81.9281 - val_mae: 6.4798\n",
      "Epoch 470/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8817 - mse: 85.8817 - mae: 6.4119 - val_loss: 81.9724 - val_mse: 81.9724 - val_mae: 6.5118\n",
      "Epoch 471/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7558 - mse: 85.7558 - mae: 6.4173 - val_loss: 81.9670 - val_mse: 81.9669 - val_mae: 6.4501\n",
      "Epoch 472/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7068 - mse: 85.7069 - mae: 6.4115 - val_loss: 82.4763 - val_mse: 82.4763 - val_mae: 6.5572\n",
      "Epoch 473/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8450 - mse: 85.8451 - mae: 6.4165 - val_loss: 82.1795 - val_mse: 82.1795 - val_mae: 6.4794\n",
      "Epoch 474/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.8123 - mse: 85.8123 - mae: 6.4196 - val_loss: 82.1516 - val_mse: 82.1517 - val_mae: 6.4148\n",
      "Epoch 475/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8542 - mse: 85.8543 - mae: 6.4166 - val_loss: 82.6046 - val_mse: 82.6046 - val_mae: 6.4364\n",
      "Epoch 476/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7333 - mse: 85.7332 - mae: 6.4113 - val_loss: 82.7862 - val_mse: 82.7862 - val_mae: 6.5242\n",
      "Epoch 477/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8733 - mse: 85.8734 - mae: 6.4198 - val_loss: 82.1764 - val_mse: 82.1763 - val_mae: 6.5009\n",
      "Epoch 478/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7866 - mse: 85.7866 - mae: 6.4211 - val_loss: 83.0232 - val_mse: 83.0232 - val_mae: 6.4325\n",
      "Epoch 479/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7598 - mse: 85.7599 - mae: 6.4124 - val_loss: 81.9645 - val_mse: 81.9645 - val_mae: 6.4449\n",
      "Epoch 480/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7952 - mse: 85.7952 - mae: 6.4141 - val_loss: 82.2382 - val_mse: 82.2382 - val_mae: 6.5362\n",
      "Epoch 481/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7689 - mse: 85.7689 - mae: 6.4222 - val_loss: 82.4830 - val_mse: 82.4830 - val_mae: 6.4315\n",
      "Epoch 482/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.9106 - mse: 85.9106 - mae: 6.4217 - val_loss: 81.8722 - val_mse: 81.8722 - val_mae: 6.4644\n",
      "Epoch 483/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.6803 - mse: 85.6803 - mae: 6.4165 - val_loss: 82.9872 - val_mse: 82.9872 - val_mae: 6.4051\n",
      "Epoch 484/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.7501 - mse: 85.7501 - mae: 6.4216 - val_loss: 82.3938 - val_mse: 82.3938 - val_mae: 6.4160\n",
      "Epoch 485/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7699 - mse: 85.7699 - mae: 6.4215 - val_loss: 83.1768 - val_mse: 83.1768 - val_mae: 6.5244\n",
      "Epoch 486/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8107 - mse: 85.8108 - mae: 6.4173 - val_loss: 81.9575 - val_mse: 81.9575 - val_mae: 6.4878\n",
      "Epoch 487/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7790 - mse: 85.7791 - mae: 6.4088 - val_loss: 82.1444 - val_mse: 82.1444 - val_mae: 6.4548\n",
      "Epoch 488/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8221 - mse: 85.8221 - mae: 6.4186 - val_loss: 82.1863 - val_mse: 82.1863 - val_mae: 6.4800\n",
      "Epoch 489/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8490 - mse: 85.8489 - mae: 6.4170 - val_loss: 81.9228 - val_mse: 81.9228 - val_mae: 6.4745\n",
      "Epoch 490/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.7726 - mse: 85.7727 - mae: 6.4091 - val_loss: 81.8636 - val_mse: 81.8636 - val_mae: 6.5155\n",
      "Epoch 491/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.9061 - mse: 85.9061 - mae: 6.4182 - val_loss: 82.6089 - val_mse: 82.6089 - val_mae: 6.4446\n",
      "Epoch 492/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6416 - mse: 85.6415 - mae: 6.4102 - val_loss: 82.1317 - val_mse: 82.1317 - val_mae: 6.5028\n",
      "Epoch 493/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8130 - mse: 85.8130 - mae: 6.4197 - val_loss: 82.3388 - val_mse: 82.3388 - val_mae: 6.4246\n",
      "Epoch 494/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7764 - mse: 85.7764 - mae: 6.4138 - val_loss: 82.6345 - val_mse: 82.6345 - val_mae: 6.3938\n",
      "Epoch 495/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.6957 - mse: 85.6957 - mae: 6.4090 - val_loss: 82.1453 - val_mse: 82.1453 - val_mae: 6.4530\n",
      "Epoch 496/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.9757 - mse: 85.9758 - mae: 6.4180 - val_loss: 82.0904 - val_mse: 82.0904 - val_mae: 6.5006\n",
      "Epoch 497/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8276 - mse: 85.8276 - mae: 6.4231 - val_loss: 83.4225 - val_mse: 83.4225 - val_mae: 6.3675\n",
      "Epoch 498/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7249 - mse: 85.7249 - mae: 6.4110 - val_loss: 82.7908 - val_mse: 82.7908 - val_mae: 6.6113\n",
      "Epoch 499/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.9597 - mse: 85.9596 - mae: 6.4265 - val_loss: 81.9982 - val_mse: 81.9982 - val_mae: 6.4772\n",
      "Epoch 500/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8985 - mse: 85.8985 - mae: 6.4196 - val_loss: 81.8779 - val_mse: 81.8779 - val_mae: 6.4590\n",
      "Epoch 501/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7547 - mse: 85.7547 - mae: 6.4135 - val_loss: 82.0979 - val_mse: 82.0978 - val_mae: 6.5158\n",
      "Epoch 502/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8405 - mse: 85.8405 - mae: 6.4244 - val_loss: 82.0942 - val_mse: 82.0943 - val_mae: 6.5192\n",
      "Epoch 503/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7798 - mse: 85.7797 - mae: 6.4121 - val_loss: 82.6145 - val_mse: 82.6144 - val_mae: 6.5650\n",
      "Epoch 504/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8010 - mse: 85.8010 - mae: 6.4210 - val_loss: 82.5441 - val_mse: 82.5442 - val_mae: 6.4138\n",
      "Epoch 505/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8855 - mse: 85.8855 - mae: 6.4176 - val_loss: 82.0352 - val_mse: 82.0352 - val_mae: 6.5434\n",
      "Epoch 506/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8191 - mse: 85.8191 - mae: 6.4167 - val_loss: 82.0369 - val_mse: 82.0370 - val_mae: 6.4285\n",
      "Epoch 507/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8475 - mse: 85.8474 - mae: 6.4194 - val_loss: 82.1631 - val_mse: 82.1631 - val_mae: 6.4682\n",
      "Epoch 508/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8173 - mse: 85.8173 - mae: 6.4134 - val_loss: 82.0243 - val_mse: 82.0243 - val_mae: 6.4330\n",
      "Epoch 509/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8653 - mse: 85.8654 - mae: 6.4186 - val_loss: 82.8309 - val_mse: 82.8309 - val_mae: 6.4603\n",
      "Epoch 510/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6475 - mse: 85.6475 - mae: 6.4079 - val_loss: 82.4709 - val_mse: 82.4709 - val_mae: 6.4297\n",
      "Epoch 511/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.8700 - mse: 85.8700 - mae: 6.4155 - val_loss: 81.9517 - val_mse: 81.9517 - val_mae: 6.5092\n",
      "Epoch 512/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8056 - mse: 85.8056 - mae: 6.4141 - val_loss: 81.9731 - val_mse: 81.9731 - val_mae: 6.4359\n",
      "Epoch 513/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8488 - mse: 85.8488 - mae: 6.4183 - val_loss: 82.0151 - val_mse: 82.0151 - val_mae: 6.4839\n",
      "Epoch 514/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7364 - mse: 85.7365 - mae: 6.4192 - val_loss: 82.6286 - val_mse: 82.6287 - val_mae: 6.4370\n",
      "Epoch 515/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7467 - mse: 85.7468 - mae: 6.4112 - val_loss: 82.5242 - val_mse: 82.5242 - val_mae: 6.6253\n",
      "Epoch 516/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7863 - mse: 85.7863 - mae: 6.4208 - val_loss: 82.4519 - val_mse: 82.4519 - val_mae: 6.3858\n",
      "Epoch 517/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7911 - mse: 85.7910 - mae: 6.4134 - val_loss: 82.0087 - val_mse: 82.0086 - val_mae: 6.4722\n",
      "Epoch 518/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7750 - mse: 85.7750 - mae: 6.4169 - val_loss: 82.0272 - val_mse: 82.0271 - val_mae: 6.4356\n",
      "Epoch 519/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.6731 - mse: 85.6731 - mae: 6.4171 - val_loss: 82.6274 - val_mse: 82.6274 - val_mae: 6.3920\n",
      "Epoch 520/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.6973 - mse: 85.6973 - mae: 6.4039 - val_loss: 82.9903 - val_mse: 82.9903 - val_mae: 6.5403\n",
      "Epoch 521/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8297 - mse: 85.8298 - mae: 6.4214 - val_loss: 82.3355 - val_mse: 82.3355 - val_mae: 6.4997\n",
      "Epoch 522/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.6735 - mse: 85.6734 - mae: 6.4158 - val_loss: 82.7059 - val_mse: 82.7058 - val_mae: 6.4787\n",
      "Epoch 523/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.6503 - mse: 85.6505 - mae: 6.4130 - val_loss: 82.4777 - val_mse: 82.4777 - val_mae: 6.4521\n",
      "Epoch 524/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8396 - mse: 85.8396 - mae: 6.4085 - val_loss: 82.4062 - val_mse: 82.4062 - val_mae: 6.4827\n",
      "Epoch 525/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6290 - mse: 85.6290 - mae: 6.4101 - val_loss: 82.3301 - val_mse: 82.3301 - val_mae: 6.4440\n",
      "Epoch 526/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.9124 - mse: 85.9124 - mae: 6.4161 - val_loss: 82.9161 - val_mse: 82.9161 - val_mae: 6.4159\n",
      "Epoch 527/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.6612 - mse: 85.6611 - mae: 6.4172 - val_loss: 82.4520 - val_mse: 82.4520 - val_mae: 6.4046\n",
      "Epoch 528/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7472 - mse: 85.7472 - mae: 6.4162 - val_loss: 82.5115 - val_mse: 82.5115 - val_mae: 6.4285\n",
      "Epoch 529/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8305 - mse: 85.8305 - mae: 6.4119 - val_loss: 82.0057 - val_mse: 82.0057 - val_mae: 6.4308\n",
      "Epoch 530/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7686 - mse: 85.7686 - mae: 6.4128 - val_loss: 82.0997 - val_mse: 82.0997 - val_mae: 6.4593\n",
      "Epoch 531/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8288 - mse: 85.8287 - mae: 6.4171 - val_loss: 81.9176 - val_mse: 81.9176 - val_mae: 6.4538\n",
      "Epoch 532/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7380 - mse: 85.7380 - mae: 6.4147 - val_loss: 82.3333 - val_mse: 82.3333 - val_mae: 6.5537\n",
      "Epoch 533/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8069 - mse: 85.8068 - mae: 6.4147 - val_loss: 82.1169 - val_mse: 82.1169 - val_mae: 6.4471\n",
      "Epoch 534/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8226 - mse: 85.8227 - mae: 6.4146 - val_loss: 81.9180 - val_mse: 81.9180 - val_mae: 6.4667\n",
      "Epoch 535/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.9448 - mse: 85.9448 - mae: 6.4193 - val_loss: 82.1399 - val_mse: 82.1399 - val_mae: 6.4981\n",
      "Epoch 536/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7167 - mse: 85.7167 - mae: 6.4105 - val_loss: 82.3344 - val_mse: 82.3343 - val_mae: 6.5419\n",
      "Epoch 537/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7547 - mse: 85.7547 - mae: 6.4131 - val_loss: 82.2808 - val_mse: 82.2809 - val_mae: 6.5266\n",
      "Epoch 538/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8483 - mse: 85.8483 - mae: 6.4179 - val_loss: 82.1356 - val_mse: 82.1356 - val_mae: 6.4973\n",
      "Epoch 539/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.6973 - mse: 85.6973 - mae: 6.4112 - val_loss: 82.1716 - val_mse: 82.1716 - val_mae: 6.5118\n",
      "Epoch 540/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7872 - mse: 85.7872 - mae: 6.4190 - val_loss: 82.2711 - val_mse: 82.2711 - val_mae: 6.5080\n",
      "Epoch 541/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7256 - mse: 85.7256 - mae: 6.4149 - val_loss: 82.7140 - val_mse: 82.7140 - val_mae: 6.5246\n",
      "Epoch 542/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7103 - mse: 85.7104 - mae: 6.4109 - val_loss: 82.2210 - val_mse: 82.2210 - val_mae: 6.5802\n",
      "Epoch 543/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7403 - mse: 85.7403 - mae: 6.4213 - val_loss: 82.8168 - val_mse: 82.8168 - val_mae: 6.5131\n",
      "Epoch 544/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7387 - mse: 85.7387 - mae: 6.4103 - val_loss: 82.0807 - val_mse: 82.0806 - val_mae: 6.4432\n",
      "Epoch 545/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7346 - mse: 85.7345 - mae: 6.4177 - val_loss: 83.0099 - val_mse: 83.0099 - val_mae: 6.5998\n",
      "Epoch 546/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.9244 - mse: 85.9245 - mae: 6.4208 - val_loss: 81.8845 - val_mse: 81.8845 - val_mae: 6.4841\n",
      "Epoch 547/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7777 - mse: 85.7778 - mae: 6.4162 - val_loss: 82.5533 - val_mse: 82.5533 - val_mae: 6.5168\n",
      "Epoch 548/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.9303 - mse: 85.9303 - mae: 6.4172 - val_loss: 82.4174 - val_mse: 82.4173 - val_mae: 6.5193\n",
      "Epoch 549/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8628 - mse: 85.8628 - mae: 6.4149 - val_loss: 82.0459 - val_mse: 82.0459 - val_mae: 6.5034\n",
      "Epoch 550/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.6984 - mse: 85.6984 - mae: 6.4182 - val_loss: 81.9440 - val_mse: 81.9440 - val_mae: 6.4570\n",
      "Epoch 551/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6286 - mse: 85.6285 - mae: 6.4114 - val_loss: 82.0265 - val_mse: 82.0266 - val_mae: 6.4566\n",
      "Epoch 552/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.8408 - mse: 85.8408 - mae: 6.4164 - val_loss: 81.9515 - val_mse: 81.9516 - val_mae: 6.4740\n",
      "Epoch 553/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.8203 - mse: 85.8204 - mae: 6.4163 - val_loss: 81.9632 - val_mse: 81.9632 - val_mae: 6.4534\n",
      "Epoch 554/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7844 - mse: 85.7845 - mae: 6.4163 - val_loss: 82.4770 - val_mse: 82.4770 - val_mae: 6.4797\n",
      "Epoch 555/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7938 - mse: 85.7937 - mae: 6.4160 - val_loss: 82.5848 - val_mse: 82.5848 - val_mae: 6.4089\n",
      "Epoch 556/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7933 - mse: 85.7933 - mae: 6.4219 - val_loss: 82.0414 - val_mse: 82.0414 - val_mae: 6.4547\n",
      "Epoch 557/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8109 - mse: 85.8109 - mae: 6.4099 - val_loss: 82.3616 - val_mse: 82.3616 - val_mae: 6.5689\n",
      "Epoch 558/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8674 - mse: 85.8674 - mae: 6.4248 - val_loss: 82.2035 - val_mse: 82.2035 - val_mae: 6.5222\n",
      "Epoch 559/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8467 - mse: 85.8468 - mae: 6.4185 - val_loss: 82.2604 - val_mse: 82.2604 - val_mae: 6.5516\n",
      "Epoch 560/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.9053 - mse: 85.9053 - mae: 6.4297 - val_loss: 82.1232 - val_mse: 82.1232 - val_mae: 6.4672\n",
      "Epoch 561/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8649 - mse: 85.8649 - mae: 6.4146 - val_loss: 82.0330 - val_mse: 82.0330 - val_mae: 6.4931\n",
      "Epoch 562/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6640 - mse: 85.6639 - mae: 6.4127 - val_loss: 82.7467 - val_mse: 82.7466 - val_mae: 6.3701\n",
      "Epoch 563/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8550 - mse: 85.8550 - mae: 6.4129 - val_loss: 81.9095 - val_mse: 81.9096 - val_mae: 6.4612\n",
      "Epoch 564/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7003 - mse: 85.7004 - mae: 6.4101 - val_loss: 83.1104 - val_mse: 83.1104 - val_mae: 6.6586\n",
      "Epoch 565/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8300 - mse: 85.8301 - mae: 6.4228 - val_loss: 82.1747 - val_mse: 82.1747 - val_mae: 6.4993\n",
      "Epoch 566/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.9231 - mse: 85.9231 - mae: 6.4237 - val_loss: 82.1455 - val_mse: 82.1455 - val_mae: 6.4256\n",
      "Epoch 567/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7422 - mse: 85.7422 - mae: 6.4203 - val_loss: 82.3290 - val_mse: 82.3290 - val_mae: 6.5081\n",
      "Epoch 568/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.5932 - mse: 85.5932 - mae: 6.4218 - val_loss: 82.8361 - val_mse: 82.8361 - val_mae: 6.5473\n",
      "Epoch 569/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7956 - mse: 85.7956 - mae: 6.4204 - val_loss: 82.2700 - val_mse: 82.2700 - val_mae: 6.4965\n",
      "Epoch 570/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8127 - mse: 85.8127 - mae: 6.4190 - val_loss: 82.2076 - val_mse: 82.2076 - val_mae: 6.4464\n",
      "Epoch 571/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7237 - mse: 85.7237 - mae: 6.4272 - val_loss: 82.8586 - val_mse: 82.8586 - val_mae: 6.3690\n",
      "Epoch 572/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7534 - mse: 85.7535 - mae: 6.4163 - val_loss: 81.9125 - val_mse: 81.9125 - val_mae: 6.4802\n",
      "Epoch 573/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7236 - mse: 85.7236 - mae: 6.4206 - val_loss: 81.9519 - val_mse: 81.9520 - val_mae: 6.4225\n",
      "Epoch 574/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.6736 - mse: 85.6736 - mae: 6.4137 - val_loss: 82.2934 - val_mse: 82.2934 - val_mae: 6.5140\n",
      "Epoch 575/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.8325 - mse: 85.8325 - mae: 6.4208 - val_loss: 82.1247 - val_mse: 82.1247 - val_mae: 6.4279\n",
      "Epoch 576/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.6756 - mse: 85.6757 - mae: 6.4141 - val_loss: 81.9707 - val_mse: 81.9707 - val_mae: 6.5049\n",
      "Epoch 577/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.7679 - mse: 85.7679 - mae: 6.4164 - val_loss: 82.3073 - val_mse: 82.3073 - val_mae: 6.5795\n",
      "Epoch 578/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7826 - mse: 85.7826 - mae: 6.4161 - val_loss: 82.2421 - val_mse: 82.2421 - val_mae: 6.4409\n",
      "Epoch 579/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7163 - mse: 85.7163 - mae: 6.4154 - val_loss: 82.0543 - val_mse: 82.0543 - val_mae: 6.4579\n",
      "Epoch 580/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8518 - mse: 85.8518 - mae: 6.4182 - val_loss: 82.3155 - val_mse: 82.3155 - val_mae: 6.5319\n",
      "Epoch 581/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7240 - mse: 85.7239 - mae: 6.4206 - val_loss: 82.7752 - val_mse: 82.7753 - val_mae: 6.4109\n",
      "Epoch 582/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.6302 - mse: 85.6302 - mae: 6.4125 - val_loss: 83.1719 - val_mse: 83.1719 - val_mae: 6.3900\n",
      "Epoch 583/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.6986 - mse: 85.6986 - mae: 6.4096 - val_loss: 81.8802 - val_mse: 81.8801 - val_mae: 6.4897\n",
      "Epoch 584/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7236 - mse: 85.7236 - mae: 6.4135 - val_loss: 82.0575 - val_mse: 82.0575 - val_mae: 6.5589\n",
      "Epoch 585/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7961 - mse: 85.7962 - mae: 6.4167 - val_loss: 82.0930 - val_mse: 82.0930 - val_mae: 6.4608\n",
      "Epoch 586/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7750 - mse: 85.7751 - mae: 6.4182 - val_loss: 82.0103 - val_mse: 82.0102 - val_mae: 6.4756\n",
      "Epoch 587/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7494 - mse: 85.7494 - mae: 6.4201 - val_loss: 82.1681 - val_mse: 82.1681 - val_mae: 6.5319\n",
      "Epoch 588/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8147 - mse: 85.8147 - mae: 6.4160 - val_loss: 82.2967 - val_mse: 82.2967 - val_mae: 6.5517\n",
      "Epoch 589/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7305 - mse: 85.7305 - mae: 6.4074 - val_loss: 82.6526 - val_mse: 82.6526 - val_mae: 6.5121\n",
      "Epoch 590/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7204 - mse: 85.7204 - mae: 6.4140 - val_loss: 82.5892 - val_mse: 82.5892 - val_mae: 6.4300\n",
      "Epoch 591/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7456 - mse: 85.7456 - mae: 6.4171 - val_loss: 82.3475 - val_mse: 82.3475 - val_mae: 6.4193\n",
      "Epoch 592/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7508 - mse: 85.7508 - mae: 6.4139 - val_loss: 82.1683 - val_mse: 82.1683 - val_mae: 6.4562\n",
      "Epoch 593/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7952 - mse: 85.7952 - mae: 6.4136 - val_loss: 81.8398 - val_mse: 81.8399 - val_mae: 6.4988\n",
      "Epoch 594/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8240 - mse: 85.8239 - mae: 6.4191 - val_loss: 82.0644 - val_mse: 82.0644 - val_mae: 6.4630\n",
      "Epoch 595/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6545 - mse: 85.6545 - mae: 6.4097 - val_loss: 81.9017 - val_mse: 81.9017 - val_mae: 6.5151\n",
      "Epoch 596/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8230 - mse: 85.8230 - mae: 6.4155 - val_loss: 81.9397 - val_mse: 81.9397 - val_mae: 6.4818\n",
      "Epoch 597/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7260 - mse: 85.7260 - mae: 6.4233 - val_loss: 82.0797 - val_mse: 82.0797 - val_mae: 6.4366\n",
      "Epoch 598/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6093 - mse: 85.6092 - mae: 6.4083 - val_loss: 82.0831 - val_mse: 82.0831 - val_mae: 6.5716\n",
      "Epoch 599/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7676 - mse: 85.7675 - mae: 6.4189 - val_loss: 82.2200 - val_mse: 82.2200 - val_mae: 6.5514\n",
      "Epoch 600/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7589 - mse: 85.7589 - mae: 6.4140 - val_loss: 81.8975 - val_mse: 81.8975 - val_mae: 6.4859\n",
      "Epoch 601/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7853 - mse: 85.7852 - mae: 6.4115 - val_loss: 81.9487 - val_mse: 81.9487 - val_mae: 6.5018\n",
      "Epoch 602/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.9290 - mse: 85.9288 - mae: 6.4242 - val_loss: 82.0122 - val_mse: 82.0122 - val_mae: 6.4925\n",
      "Epoch 603/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.5542 - mse: 85.5542 - mae: 6.4153 - val_loss: 82.3972 - val_mse: 82.3972 - val_mae: 6.5605\n",
      "Epoch 604/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.8629 - mse: 85.8629 - mae: 6.4227 - val_loss: 82.1144 - val_mse: 82.1144 - val_mae: 6.5253\n",
      "Epoch 605/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7883 - mse: 85.7883 - mae: 6.4129 - val_loss: 82.0147 - val_mse: 82.0146 - val_mae: 6.4811\n",
      "Epoch 606/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6071 - mse: 85.6070 - mae: 6.4194 - val_loss: 82.4754 - val_mse: 82.4754 - val_mae: 6.4454\n",
      "Epoch 607/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.6614 - mse: 85.6613 - mae: 6.4144 - val_loss: 82.7369 - val_mse: 82.7369 - val_mae: 6.5592\n",
      "Epoch 608/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7840 - mse: 85.7839 - mae: 6.4206 - val_loss: 83.2103 - val_mse: 83.2103 - val_mae: 6.3842\n",
      "Epoch 609/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7386 - mse: 85.7387 - mae: 6.4101 - val_loss: 82.4555 - val_mse: 82.4555 - val_mae: 6.4984\n",
      "Epoch 610/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.7924 - mse: 85.7924 - mae: 6.4252 - val_loss: 82.8859 - val_mse: 82.8859 - val_mae: 6.4213\n",
      "Epoch 611/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.7534 - mse: 85.7534 - mae: 6.4153 - val_loss: 81.8994 - val_mse: 81.8994 - val_mae: 6.4859\n",
      "Epoch 612/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.7909 - mse: 85.7908 - mae: 6.4191 - val_loss: 82.1283 - val_mse: 82.1283 - val_mae: 6.4703\n",
      "Epoch 613/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.7207 - mse: 85.7206 - mae: 6.4144 - val_loss: 82.1320 - val_mse: 82.1320 - val_mae: 6.4543\n",
      "Epoch 614/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 85.7588 - mse: 85.7587 - mae: 6.4157 - val_loss: 81.9556 - val_mse: 81.9556 - val_mae: 6.4864\n",
      "Epoch 615/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 85.6591 - mse: 85.6591 - mae: 6.4167 - val_loss: 82.3566 - val_mse: 82.3566 - val_mae: 6.5687\n",
      "Epoch 616/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7671 - mse: 85.7671 - mae: 6.4157 - val_loss: 81.9162 - val_mse: 81.9162 - val_mae: 6.4923\n",
      "Epoch 617/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8777 - mse: 85.8776 - mae: 6.4185 - val_loss: 82.2258 - val_mse: 82.2258 - val_mae: 6.5005\n",
      "Epoch 618/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8056 - mse: 85.8055 - mae: 6.4150 - val_loss: 82.2341 - val_mse: 82.2342 - val_mae: 6.5006\n",
      "Epoch 619/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8346 - mse: 85.8346 - mae: 6.4166 - val_loss: 82.1882 - val_mse: 82.1881 - val_mae: 6.4306\n",
      "Epoch 620/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7213 - mse: 85.7213 - mae: 6.4192 - val_loss: 82.8205 - val_mse: 82.8204 - val_mae: 6.4277\n",
      "Epoch 621/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7250 - mse: 85.7250 - mae: 6.4170 - val_loss: 82.1238 - val_mse: 82.1238 - val_mae: 6.5372\n",
      "Epoch 622/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.8046 - mse: 85.8046 - mae: 6.4241 - val_loss: 82.0703 - val_mse: 82.0703 - val_mae: 6.4397\n",
      "Epoch 623/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.6502 - mse: 85.6502 - mae: 6.4094 - val_loss: 81.9800 - val_mse: 81.9800 - val_mae: 6.4347\n",
      "Epoch 624/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7343 - mse: 85.7344 - mae: 6.4196 - val_loss: 82.3670 - val_mse: 82.3670 - val_mae: 6.5791\n",
      "Epoch 625/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.8633 - mse: 85.8633 - mae: 6.4192 - val_loss: 82.6933 - val_mse: 82.6933 - val_mae: 6.5931\n",
      "Epoch 626/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.8332 - mse: 85.8332 - mae: 6.4209 - val_loss: 82.0413 - val_mse: 82.0413 - val_mae: 6.5022\n",
      "Epoch 627/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7318 - mse: 85.7319 - mae: 6.4168 - val_loss: 82.1339 - val_mse: 82.1339 - val_mae: 6.5074\n",
      "Epoch 628/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7535 - mse: 85.7535 - mae: 6.4123 - val_loss: 82.0548 - val_mse: 82.0548 - val_mae: 6.4991\n",
      "Epoch 629/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7571 - mse: 85.7571 - mae: 6.4258 - val_loss: 82.0239 - val_mse: 82.0239 - val_mae: 6.4645\n",
      "Epoch 630/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7743 - mse: 85.7743 - mae: 6.4140 - val_loss: 82.3338 - val_mse: 82.3338 - val_mae: 6.5234\n",
      "Epoch 631/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.8081 - mse: 85.8081 - mae: 6.4164 - val_loss: 81.9484 - val_mse: 81.9484 - val_mae: 6.5132\n",
      "Epoch 632/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8208 - mse: 85.8208 - mae: 6.4164 - val_loss: 82.3700 - val_mse: 82.3700 - val_mae: 6.5530\n",
      "Epoch 633/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7727 - mse: 85.7727 - mae: 6.4188 - val_loss: 82.3255 - val_mse: 82.3255 - val_mae: 6.4328\n",
      "Epoch 634/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.6695 - mse: 85.6695 - mae: 6.4137 - val_loss: 84.2753 - val_mse: 84.2753 - val_mae: 6.3958\n",
      "Epoch 635/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7202 - mse: 85.7202 - mae: 6.4075 - val_loss: 82.2256 - val_mse: 82.2255 - val_mae: 6.5516\n",
      "Epoch 636/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7917 - mse: 85.7918 - mae: 6.4166 - val_loss: 82.2061 - val_mse: 82.2061 - val_mae: 6.4215\n",
      "Epoch 637/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.8054 - mse: 85.8055 - mae: 6.4203 - val_loss: 83.5146 - val_mse: 83.5146 - val_mae: 6.4626\n",
      "Epoch 638/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.8313 - mse: 85.8314 - mae: 6.4160 - val_loss: 82.1600 - val_mse: 82.1600 - val_mae: 6.3844\n",
      "Epoch 639/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.7134 - mse: 85.7134 - mae: 6.4109 - val_loss: 81.9538 - val_mse: 81.9538 - val_mae: 6.4394\n",
      "Epoch 640/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.6506 - mse: 85.6507 - mae: 6.4209 - val_loss: 82.6609 - val_mse: 82.6609 - val_mae: 6.4614\n",
      "Epoch 641/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7584 - mse: 85.7585 - mae: 6.4184 - val_loss: 82.5661 - val_mse: 82.5661 - val_mae: 6.4148\n",
      "Epoch 642/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7274 - mse: 85.7274 - mae: 6.4183 - val_loss: 82.1271 - val_mse: 82.1272 - val_mae: 6.4474\n",
      "Epoch 643/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.7745 - mse: 85.7744 - mae: 6.4189 - val_loss: 83.1136 - val_mse: 83.1137 - val_mae: 6.4186\n",
      "Epoch 644/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.8499 - mse: 85.8500 - mae: 6.4198 - val_loss: 82.6847 - val_mse: 82.6847 - val_mae: 6.4944\n",
      "Epoch 645/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7530 - mse: 85.7530 - mae: 6.4222 - val_loss: 82.6447 - val_mse: 82.6447 - val_mae: 6.5351\n",
      "Epoch 646/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.6725 - mse: 85.6725 - mae: 6.4132 - val_loss: 81.8994 - val_mse: 81.8994 - val_mae: 6.5263\n",
      "Epoch 647/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7936 - mse: 85.7936 - mae: 6.4231 - val_loss: 82.1279 - val_mse: 82.1279 - val_mae: 6.4539\n",
      "Epoch 648/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7361 - mse: 85.7362 - mae: 6.4115 - val_loss: 82.9399 - val_mse: 82.9399 - val_mae: 6.6574\n",
      "Epoch 649/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7303 - mse: 85.7303 - mae: 6.4192 - val_loss: 82.5792 - val_mse: 82.5792 - val_mae: 6.5088\n",
      "Epoch 650/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.8105 - mse: 85.8106 - mae: 6.4190 - val_loss: 82.4150 - val_mse: 82.4150 - val_mae: 6.5027\n",
      "Epoch 651/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7901 - mse: 85.7901 - mae: 6.4219 - val_loss: 82.3002 - val_mse: 82.3002 - val_mae: 6.5501\n",
      "Epoch 652/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7859 - mse: 85.7860 - mae: 6.4137 - val_loss: 82.2868 - val_mse: 82.2867 - val_mae: 6.4004\n",
      "Epoch 653/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.8420 - mse: 85.8419 - mae: 6.4145 - val_loss: 81.9847 - val_mse: 81.9847 - val_mae: 6.4762\n",
      "Epoch 654/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.6430 - mse: 85.6430 - mae: 6.4079 - val_loss: 82.2085 - val_mse: 82.2086 - val_mae: 6.4469\n",
      "Epoch 655/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 85.9159 - mse: 85.9159 - mae: 6.4118 - val_loss: 81.9906 - val_mse: 81.9907 - val_mae: 6.5277\n",
      "Epoch 656/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.7190 - mse: 85.7190 - mae: 6.4171 - val_loss: 82.4279 - val_mse: 82.4279 - val_mae: 6.5104\n",
      "Epoch 657/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7507 - mse: 85.7506 - mae: 6.4190 - val_loss: 82.0908 - val_mse: 82.0909 - val_mae: 6.4363\n",
      "Epoch 658/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.7397 - mse: 85.7397 - mae: 6.4142 - val_loss: 81.9387 - val_mse: 81.9387 - val_mae: 6.4621\n",
      "Epoch 659/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.7574 - mse: 85.7574 - mae: 6.4141 - val_loss: 82.0319 - val_mse: 82.0318 - val_mae: 6.5034\n",
      "Epoch 660/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7663 - mse: 85.7664 - mae: 6.4199 - val_loss: 82.4939 - val_mse: 82.4939 - val_mae: 6.4202\n",
      "Epoch 661/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7889 - mse: 85.7889 - mae: 6.4204 - val_loss: 82.4676 - val_mse: 82.4677 - val_mae: 6.4258\n",
      "Epoch 662/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6603 - mse: 85.6603 - mae: 6.4112 - val_loss: 82.0294 - val_mse: 82.0294 - val_mae: 6.4882\n",
      "Epoch 663/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6842 - mse: 85.6842 - mae: 6.4184 - val_loss: 82.5510 - val_mse: 82.5510 - val_mae: 6.4434\n",
      "Epoch 664/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.6362 - mse: 85.6362 - mae: 6.4126 - val_loss: 83.3340 - val_mse: 83.3340 - val_mae: 6.4061\n",
      "Epoch 665/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7682 - mse: 85.7682 - mae: 6.4219 - val_loss: 82.9498 - val_mse: 82.9497 - val_mae: 6.4416\n",
      "Epoch 666/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.6507 - mse: 85.6506 - mae: 6.4123 - val_loss: 83.4479 - val_mse: 83.4479 - val_mae: 6.6675\n",
      "Epoch 667/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.6088 - mse: 85.6088 - mae: 6.4202 - val_loss: 82.1458 - val_mse: 82.1458 - val_mae: 6.4366\n",
      "Epoch 668/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 85.8579 - mse: 85.8579 - mae: 6.4153 - val_loss: 82.2067 - val_mse: 82.2067 - val_mae: 6.5048\n",
      "Epoch 669/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.7410 - mse: 85.7410 - mae: 6.4228 - val_loss: 82.4477 - val_mse: 82.4478 - val_mae: 6.3729\n",
      "Epoch 670/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.7702 - mse: 85.7702 - mae: 6.4201 - val_loss: 82.3777 - val_mse: 82.3777 - val_mae: 6.4730\n",
      "Epoch 671/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.8474 - mse: 85.8474 - mae: 6.4197 - val_loss: 82.3924 - val_mse: 82.3924 - val_mae: 6.4946\n",
      "Epoch 672/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.5653 - mse: 85.5653 - mae: 6.4168 - val_loss: 82.0204 - val_mse: 82.0204 - val_mae: 6.4527\n",
      "Epoch 673/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.5473 - mse: 85.5474 - mae: 6.4091 - val_loss: 82.2887 - val_mse: 82.2887 - val_mae: 6.5916\n",
      "Epoch 674/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.6500 - mse: 85.6500 - mae: 6.4190 - val_loss: 82.3047 - val_mse: 82.3047 - val_mae: 6.5148\n",
      "Epoch 675/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7081 - mse: 85.7082 - mae: 6.4094 - val_loss: 82.1497 - val_mse: 82.1497 - val_mae: 6.5222\n",
      "Epoch 676/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7172 - mse: 85.7172 - mae: 6.4126 - val_loss: 82.3288 - val_mse: 82.3288 - val_mae: 6.4246\n",
      "Epoch 677/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.5061 - mse: 85.5061 - mae: 6.4042 - val_loss: 82.4778 - val_mse: 82.4779 - val_mae: 6.5293\n",
      "Epoch 678/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 85.6736 - mse: 85.6735 - mae: 6.4175 - val_loss: 82.2158 - val_mse: 82.2158 - val_mae: 6.5263\n",
      "Epoch 679/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.8728 - mse: 85.8728 - mae: 6.4170 - val_loss: 82.4588 - val_mse: 82.4588 - val_mae: 6.5055\n",
      "Epoch 680/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6708 - mse: 85.6709 - mae: 6.4186 - val_loss: 82.1172 - val_mse: 82.1172 - val_mae: 6.4575\n",
      "Epoch 681/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.6452 - mse: 85.6451 - mae: 6.4177 - val_loss: 82.2177 - val_mse: 82.2177 - val_mae: 6.5396\n",
      "Epoch 682/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 85.6094 - mse: 85.6094 - mae: 6.4137 - val_loss: 82.5998 - val_mse: 82.5998 - val_mae: 6.5222\n",
      "Epoch 683/1000\n",
      "14194/14194 [==============================] - 3s 197us/step - loss: 85.6128 - mse: 85.6128 - mae: 6.4235 - val_loss: 82.3129 - val_mse: 82.3129 - val_mae: 6.3939\n",
      "Epoch 684/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7372 - mse: 85.7371 - mae: 6.4231 - val_loss: 82.2815 - val_mse: 82.2815 - val_mae: 6.4575\n",
      "Epoch 685/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 85.8042 - mse: 85.8042 - mae: 6.4151 - val_loss: 82.6915 - val_mse: 82.6915 - val_mae: 6.4323\n",
      "Epoch 686/1000\n",
      "14194/14194 [==============================] - 3s 190us/step - loss: 85.7649 - mse: 85.7649 - mae: 6.4095 - val_loss: 82.2350 - val_mse: 82.2350 - val_mae: 6.4814\n",
      "Epoch 687/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 85.7969 - mse: 85.7969 - mae: 6.4172 - val_loss: 82.4622 - val_mse: 82.4622 - val_mae: 6.5439\n",
      "Epoch 688/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6719 - mse: 85.6719 - mae: 6.4120 - val_loss: 82.8471 - val_mse: 82.8471 - val_mae: 6.6395\n",
      "Epoch 689/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 85.7757 - mse: 85.7757 - mae: 6.4222 - val_loss: 82.1690 - val_mse: 82.1690 - val_mae: 6.4421\n",
      "Epoch 690/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 85.6811 - mse: 85.6811 - mae: 6.4151 - val_loss: 82.9053 - val_mse: 82.9053 - val_mae: 6.6029\n",
      "Epoch 691/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 85.7585 - mse: 85.7585 - mae: 6.4160 - val_loss: 82.3755 - val_mse: 82.3755 - val_mae: 6.5386\n",
      "Epoch 692/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.8240 - mse: 85.8239 - mae: 6.4101 - val_loss: 82.2258 - val_mse: 82.2258 - val_mae: 6.5746\n",
      "Epoch 693/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.8313 - mse: 85.8312 - mae: 6.4187 - val_loss: 82.3597 - val_mse: 82.3597 - val_mae: 6.4939\n",
      "Epoch 694/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.7314 - mse: 85.7314 - mae: 6.4188 - val_loss: 82.2018 - val_mse: 82.2018 - val_mae: 6.4545\n",
      "Epoch 695/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.8212 - mse: 85.8212 - mae: 6.4227 - val_loss: 82.4822 - val_mse: 82.4822 - val_mae: 6.4005\n",
      "Epoch 696/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 85.7189 - mse: 85.7189 - mae: 6.4097 - val_loss: 82.3619 - val_mse: 82.3620 - val_mae: 6.5853\n",
      "Epoch 697/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 85.6583 - mse: 85.6583 - mae: 6.4173 - val_loss: 81.9207 - val_mse: 81.9206 - val_mae: 6.4487\n",
      "Epoch 698/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 85.7138 - mse: 85.7138 - mae: 6.4207 - val_loss: 82.3115 - val_mse: 82.3114 - val_mae: 6.4313\n",
      "Epoch 699/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 85.8089 - mse: 85.8090 - mae: 6.4141 - val_loss: 82.7977 - val_mse: 82.7977 - val_mae: 6.5864\n",
      "Epoch 700/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 85.5660 - mse: 85.5660 - mae: 6.4109 - val_loss: 83.1093 - val_mse: 83.1093 - val_mae: 6.4192\n",
      "Epoch 701/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 85.7330 - mse: 85.7330 - mae: 6.4148 - val_loss: 82.1530 - val_mse: 82.1530 - val_mae: 6.5201\n",
      "Epoch 702/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 85.6849 - mse: 85.6850 - mae: 6.4180 - val_loss: 82.4899 - val_mse: 82.4899 - val_mae: 6.3981\n",
      "Epoch 703/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 85.7002 - mse: 85.7001 - mae: 6.4145 - val_loss: 82.8672 - val_mse: 82.8672 - val_mae: 6.6290\n",
      "Epoch 704/1000\n",
      "14194/14194 [==============================] - 2s 150us/step - loss: 85.7591 - mse: 85.7590 - mae: 6.4223 - val_loss: 81.9998 - val_mse: 81.9998 - val_mae: 6.4717\n",
      "Epoch 705/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.6228 - mse: 85.6228 - mae: 6.4087 - val_loss: 82.4424 - val_mse: 82.4424 - val_mae: 6.4598\n",
      "Epoch 706/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.8037 - mse: 85.8036 - mae: 6.4122 - val_loss: 82.4212 - val_mse: 82.4212 - val_mae: 6.4611\n",
      "Epoch 707/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 85.7900 - mse: 85.7901 - mae: 6.4255 - val_loss: 82.6598 - val_mse: 82.6598 - val_mae: 6.4085\n",
      "Epoch 708/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.8201 - mse: 85.8202 - mae: 6.4216 - val_loss: 82.0342 - val_mse: 82.0342 - val_mae: 6.4557\n",
      "Epoch 709/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.7937 - mse: 85.7936 - mae: 6.4106 - val_loss: 82.2246 - val_mse: 82.2246 - val_mae: 6.4775\n",
      "Epoch 710/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.8037 - mse: 85.8038 - mae: 6.4235 - val_loss: 82.0870 - val_mse: 82.0870 - val_mae: 6.4887\n",
      "Epoch 711/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7445 - mse: 85.7445 - mae: 6.4162 - val_loss: 82.0048 - val_mse: 82.0048 - val_mae: 6.4472\n",
      "Epoch 712/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.8414 - mse: 85.8413 - mae: 6.4174 - val_loss: 82.0867 - val_mse: 82.0867 - val_mae: 6.4213\n",
      "Epoch 713/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7465 - mse: 85.7465 - mae: 6.4207 - val_loss: 82.6375 - val_mse: 82.6376 - val_mae: 6.4799\n",
      "Epoch 714/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.6820 - mse: 85.6820 - mae: 6.4100 - val_loss: 82.4413 - val_mse: 82.4413 - val_mae: 6.5835\n",
      "Epoch 715/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.8276 - mse: 85.8277 - mae: 6.4301 - val_loss: 82.8032 - val_mse: 82.8032 - val_mae: 6.4151\n",
      "Epoch 716/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.8366 - mse: 85.8367 - mae: 6.4146 - val_loss: 81.9072 - val_mse: 81.9072 - val_mae: 6.4395\n",
      "Epoch 717/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7667 - mse: 85.7666 - mae: 6.4144 - val_loss: 82.1829 - val_mse: 82.1829 - val_mae: 6.4509\n",
      "Epoch 718/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7083 - mse: 85.7083 - mae: 6.4105 - val_loss: 82.0868 - val_mse: 82.0868 - val_mae: 6.5695\n",
      "Epoch 719/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.6337 - mse: 85.6337 - mae: 6.4160 - val_loss: 82.6501 - val_mse: 82.6501 - val_mae: 6.4482\n",
      "Epoch 720/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.7001 - mse: 85.7002 - mae: 6.4128 - val_loss: 82.1604 - val_mse: 82.1604 - val_mae: 6.5047\n",
      "Epoch 721/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.5776 - mse: 85.5776 - mae: 6.4095 - val_loss: 82.5471 - val_mse: 82.5471 - val_mae: 6.5091\n",
      "Epoch 722/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7110 - mse: 85.7110 - mae: 6.4144 - val_loss: 82.0994 - val_mse: 82.0994 - val_mae: 6.5280\n",
      "Epoch 723/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.5986 - mse: 85.5986 - mae: 6.4207 - val_loss: 82.2461 - val_mse: 82.2461 - val_mae: 6.4981\n",
      "Epoch 724/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.7926 - mse: 85.7926 - mae: 6.4163 - val_loss: 83.2715 - val_mse: 83.2715 - val_mae: 6.4316\n",
      "Epoch 725/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.5688 - mse: 85.5688 - mae: 6.4094 - val_loss: 82.4877 - val_mse: 82.4877 - val_mae: 6.4527\n",
      "Epoch 726/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7231 - mse: 85.7231 - mae: 6.4160 - val_loss: 82.8557 - val_mse: 82.8557 - val_mae: 6.3980\n",
      "Epoch 727/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.6619 - mse: 85.6618 - mae: 6.4149 - val_loss: 81.8787 - val_mse: 81.8787 - val_mae: 6.4886\n",
      "Epoch 728/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7425 - mse: 85.7425 - mae: 6.4155 - val_loss: 81.9973 - val_mse: 81.9973 - val_mae: 6.4888\n",
      "Epoch 729/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.5903 - mse: 85.5903 - mae: 6.4063 - val_loss: 81.9678 - val_mse: 81.9678 - val_mae: 6.4368\n",
      "Epoch 730/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.5968 - mse: 85.5968 - mae: 6.4153 - val_loss: 82.5958 - val_mse: 82.5958 - val_mae: 6.3777\n",
      "Epoch 731/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.8135 - mse: 85.8135 - mae: 6.4095 - val_loss: 82.1766 - val_mse: 82.1767 - val_mae: 6.5402\n",
      "Epoch 732/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7379 - mse: 85.7378 - mae: 6.4172 - val_loss: 82.1065 - val_mse: 82.1065 - val_mae: 6.4217\n",
      "Epoch 733/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7339 - mse: 85.7339 - mae: 6.4087 - val_loss: 82.0765 - val_mse: 82.0765 - val_mae: 6.4212\n",
      "Epoch 734/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.6958 - mse: 85.6958 - mae: 6.4123 - val_loss: 82.6140 - val_mse: 82.6140 - val_mae: 6.4390\n",
      "Epoch 735/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7013 - mse: 85.7013 - mae: 6.4117 - val_loss: 81.9597 - val_mse: 81.9597 - val_mae: 6.5021\n",
      "Epoch 736/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.5645 - mse: 85.5645 - mae: 6.4092 - val_loss: 82.2790 - val_mse: 82.2790 - val_mae: 6.5410\n",
      "Epoch 737/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7860 - mse: 85.7861 - mae: 6.4140 - val_loss: 82.2893 - val_mse: 82.2893 - val_mae: 6.5282\n",
      "Epoch 738/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6706 - mse: 85.6707 - mae: 6.4132 - val_loss: 82.9935 - val_mse: 82.9934 - val_mae: 6.5482\n",
      "Epoch 739/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7052 - mse: 85.7053 - mae: 6.4090 - val_loss: 82.3062 - val_mse: 82.3062 - val_mae: 6.5283\n",
      "Epoch 740/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7777 - mse: 85.7776 - mae: 6.4199 - val_loss: 81.9295 - val_mse: 81.9295 - val_mae: 6.4869\n",
      "Epoch 741/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7763 - mse: 85.7763 - mae: 6.4246 - val_loss: 82.3761 - val_mse: 82.3761 - val_mae: 6.4663\n",
      "Epoch 742/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6714 - mse: 85.6715 - mae: 6.4146 - val_loss: 82.5061 - val_mse: 82.5061 - val_mae: 6.4240\n",
      "Epoch 743/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.5893 - mse: 85.5893 - mae: 6.4155 - val_loss: 82.3300 - val_mse: 82.3300 - val_mae: 6.5809\n",
      "Epoch 744/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 85.7017 - mse: 85.7017 - mae: 6.4128 - val_loss: 82.3496 - val_mse: 82.3496 - val_mae: 6.4689\n",
      "Epoch 745/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 85.9138 - mse: 85.9138 - mae: 6.4122 - val_loss: 82.5715 - val_mse: 82.5715 - val_mae: 6.3802\n",
      "Epoch 746/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.7995 - mse: 85.7995 - mae: 6.4227 - val_loss: 82.2672 - val_mse: 82.2672 - val_mae: 6.4163\n",
      "Epoch 747/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.4993 - mse: 85.4994 - mae: 6.4020 - val_loss: 82.4379 - val_mse: 82.4378 - val_mae: 6.5691\n",
      "Epoch 748/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.7044 - mse: 85.7043 - mae: 6.4238 - val_loss: 82.1066 - val_mse: 82.1066 - val_mae: 6.5176\n",
      "Epoch 749/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7362 - mse: 85.7361 - mae: 6.4118 - val_loss: 82.2493 - val_mse: 82.2493 - val_mae: 6.5852\n",
      "Epoch 750/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7478 - mse: 85.7478 - mae: 6.4190 - val_loss: 82.7952 - val_mse: 82.7952 - val_mae: 6.5851\n",
      "Epoch 751/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7247 - mse: 85.7247 - mae: 6.4128 - val_loss: 81.8330 - val_mse: 81.8331 - val_mae: 6.4892\n",
      "Epoch 752/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6979 - mse: 85.6980 - mae: 6.4224 - val_loss: 82.3496 - val_mse: 82.3496 - val_mae: 6.4104\n",
      "Epoch 753/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.8125 - mse: 85.8125 - mae: 6.4121 - val_loss: 82.2526 - val_mse: 82.2526 - val_mae: 6.4983\n",
      "Epoch 754/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6865 - mse: 85.6865 - mae: 6.4106 - val_loss: 82.6301 - val_mse: 82.6301 - val_mae: 6.4214\n",
      "Epoch 755/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.5771 - mse: 85.5770 - mae: 6.4062 - val_loss: 82.1073 - val_mse: 82.1073 - val_mae: 6.5211\n",
      "Epoch 756/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.5013 - mse: 85.5013 - mae: 6.4099 - val_loss: 82.3264 - val_mse: 82.3264 - val_mae: 6.5860\n",
      "Epoch 757/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6469 - mse: 85.6470 - mae: 6.4042 - val_loss: 82.0105 - val_mse: 82.0105 - val_mae: 6.5018\n",
      "Epoch 758/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7704 - mse: 85.7704 - mae: 6.4105 - val_loss: 83.2482 - val_mse: 83.2482 - val_mae: 6.6983\n",
      "Epoch 759/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.5923 - mse: 85.5923 - mae: 6.4190 - val_loss: 81.9656 - val_mse: 81.9656 - val_mae: 6.4443\n",
      "Epoch 760/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7298 - mse: 85.7298 - mae: 6.4107 - val_loss: 82.1963 - val_mse: 82.1963 - val_mae: 6.4725\n",
      "Epoch 761/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7835 - mse: 85.7835 - mae: 6.4220 - val_loss: 81.9060 - val_mse: 81.9060 - val_mae: 6.4688\n",
      "Epoch 762/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6871 - mse: 85.6871 - mae: 6.4133 - val_loss: 82.2719 - val_mse: 82.2719 - val_mae: 6.4993\n",
      "Epoch 763/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7015 - mse: 85.7014 - mae: 6.4174 - val_loss: 82.0571 - val_mse: 82.0571 - val_mae: 6.5318\n",
      "Epoch 764/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7042 - mse: 85.7042 - mae: 6.4186 - val_loss: 82.8430 - val_mse: 82.8430 - val_mae: 6.5772\n",
      "Epoch 765/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.5699 - mse: 85.5698 - mae: 6.4092 - val_loss: 81.9123 - val_mse: 81.9123 - val_mae: 6.4527\n",
      "Epoch 766/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7291 - mse: 85.7291 - mae: 6.4151 - val_loss: 82.1330 - val_mse: 82.1330 - val_mae: 6.5080\n",
      "Epoch 767/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6742 - mse: 85.6743 - mae: 6.4175 - val_loss: 82.6965 - val_mse: 82.6965 - val_mae: 6.4474\n",
      "Epoch 768/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.6131 - mse: 85.6131 - mae: 6.4082 - val_loss: 82.0273 - val_mse: 82.0273 - val_mae: 6.4817\n",
      "Epoch 769/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 85.7713 - mse: 85.7714 - mae: 6.4125 - val_loss: 82.6924 - val_mse: 82.6924 - val_mae: 6.3991\n",
      "Epoch 770/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.7493 - mse: 85.7493 - mae: 6.4109 - val_loss: 82.2850 - val_mse: 82.2850 - val_mae: 6.5409\n",
      "Epoch 771/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.6128 - mse: 85.6128 - mae: 6.4168 - val_loss: 82.1803 - val_mse: 82.1802 - val_mae: 6.4487\n",
      "Epoch 772/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.6056 - mse: 85.6056 - mae: 6.4156 - val_loss: 82.1106 - val_mse: 82.1106 - val_mae: 6.4462\n",
      "Epoch 773/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7039 - mse: 85.7039 - mae: 6.4132 - val_loss: 81.9968 - val_mse: 81.9968 - val_mae: 6.4267\n",
      "Epoch 774/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7788 - mse: 85.7789 - mae: 6.4143 - val_loss: 82.0001 - val_mse: 82.0001 - val_mae: 6.4492\n",
      "Epoch 775/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7587 - mse: 85.7587 - mae: 6.4212 - val_loss: 81.9890 - val_mse: 81.9890 - val_mae: 6.4259\n",
      "Epoch 776/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.7017 - mse: 85.7017 - mae: 6.4093 - val_loss: 82.5630 - val_mse: 82.5630 - val_mae: 6.5761\n",
      "Epoch 777/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6839 - mse: 85.6838 - mae: 6.4136 - val_loss: 82.0349 - val_mse: 82.0348 - val_mae: 6.5614\n",
      "Epoch 778/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8169 - mse: 85.8169 - mae: 6.4256 - val_loss: 82.1830 - val_mse: 82.1830 - val_mae: 6.4441\n",
      "Epoch 779/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7392 - mse: 85.7392 - mae: 6.4096 - val_loss: 82.8026 - val_mse: 82.8026 - val_mae: 6.6120\n",
      "Epoch 780/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.6473 - mse: 85.6474 - mae: 6.4067 - val_loss: 83.3509 - val_mse: 83.3508 - val_mae: 6.6243\n",
      "Epoch 781/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7680 - mse: 85.7681 - mae: 6.4152 - val_loss: 82.6353 - val_mse: 82.6353 - val_mae: 6.4117\n",
      "Epoch 782/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7383 - mse: 85.7384 - mae: 6.4117 - val_loss: 82.1707 - val_mse: 82.1707 - val_mae: 6.4495\n",
      "Epoch 783/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6198 - mse: 85.6199 - mae: 6.4151 - val_loss: 82.1195 - val_mse: 82.1195 - val_mae: 6.4804\n",
      "Epoch 784/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7652 - mse: 85.7652 - mae: 6.4129 - val_loss: 82.0135 - val_mse: 82.0135 - val_mae: 6.4916\n",
      "Epoch 785/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7221 - mse: 85.7221 - mae: 6.4152 - val_loss: 82.0897 - val_mse: 82.0897 - val_mae: 6.4388\n",
      "Epoch 786/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7188 - mse: 85.7189 - mae: 6.4151 - val_loss: 81.8295 - val_mse: 81.8295 - val_mae: 6.4246\n",
      "Epoch 787/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.6235 - mse: 85.6237 - mae: 6.4049 - val_loss: 82.7968 - val_mse: 82.7968 - val_mae: 6.5986\n",
      "Epoch 788/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6566 - mse: 85.6567 - mae: 6.4147 - val_loss: 81.9780 - val_mse: 81.9780 - val_mae: 6.4696\n",
      "Epoch 789/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7356 - mse: 85.7355 - mae: 6.4151 - val_loss: 82.1837 - val_mse: 82.1837 - val_mae: 6.5460\n",
      "Epoch 790/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7757 - mse: 85.7757 - mae: 6.4182 - val_loss: 81.9375 - val_mse: 81.9374 - val_mae: 6.4171\n",
      "Epoch 791/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7708 - mse: 85.7708 - mae: 6.4089 - val_loss: 82.1086 - val_mse: 82.1086 - val_mae: 6.4081\n",
      "Epoch 792/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.7397 - mse: 85.7398 - mae: 6.4172 - val_loss: 82.1759 - val_mse: 82.1759 - val_mae: 6.5726\n",
      "Epoch 793/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6767 - mse: 85.6768 - mae: 6.4137 - val_loss: 82.3075 - val_mse: 82.3075 - val_mae: 6.4993\n",
      "Epoch 794/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.6356 - mse: 85.6356 - mae: 6.4144 - val_loss: 82.2250 - val_mse: 82.2250 - val_mae: 6.5027\n",
      "Epoch 795/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7681 - mse: 85.7681 - mae: 6.4140 - val_loss: 82.6026 - val_mse: 82.6026 - val_mae: 6.5670\n",
      "Epoch 796/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.6909 - mse: 85.6909 - mae: 6.4142 - val_loss: 82.4881 - val_mse: 82.4881 - val_mae: 6.4479\n",
      "Epoch 797/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7631 - mse: 85.7632 - mae: 6.4182 - val_loss: 82.4599 - val_mse: 82.4599 - val_mae: 6.4100\n",
      "Epoch 798/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.7823 - mse: 85.7822 - mae: 6.4121 - val_loss: 82.5860 - val_mse: 82.5860 - val_mae: 6.4056\n",
      "Epoch 799/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7671 - mse: 85.7671 - mae: 6.4196 - val_loss: 82.0367 - val_mse: 82.0367 - val_mae: 6.4847\n",
      "Epoch 800/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.6224 - mse: 85.6224 - mae: 6.4133 - val_loss: 82.1025 - val_mse: 82.1025 - val_mae: 6.4501\n",
      "Epoch 801/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.8462 - mse: 85.8463 - mae: 6.4202 - val_loss: 82.2655 - val_mse: 82.2655 - val_mae: 6.5262\n",
      "Epoch 802/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6654 - mse: 85.6654 - mae: 6.4162 - val_loss: 82.0868 - val_mse: 82.0868 - val_mae: 6.4930\n",
      "Epoch 803/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.6769 - mse: 85.6769 - mae: 6.4180 - val_loss: 82.1674 - val_mse: 82.1675 - val_mae: 6.4561\n",
      "Epoch 804/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.8643 - mse: 85.8642 - mae: 6.4233 - val_loss: 82.4192 - val_mse: 82.4193 - val_mae: 6.4441\n",
      "Epoch 805/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.5490 - mse: 85.5491 - mae: 6.4104 - val_loss: 82.5007 - val_mse: 82.5007 - val_mae: 6.5331\n",
      "Epoch 806/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7342 - mse: 85.7342 - mae: 6.4081 - val_loss: 82.0168 - val_mse: 82.0168 - val_mae: 6.4916\n",
      "Epoch 807/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.5558 - mse: 85.5559 - mae: 6.4147 - val_loss: 82.8838 - val_mse: 82.8839 - val_mae: 6.4810\n",
      "Epoch 808/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.8031 - mse: 85.8032 - mae: 6.4134 - val_loss: 82.2592 - val_mse: 82.2592 - val_mae: 6.5600\n",
      "Epoch 809/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8241 - mse: 85.8241 - mae: 6.4193 - val_loss: 81.9154 - val_mse: 81.9154 - val_mae: 6.4697\n",
      "Epoch 810/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7214 - mse: 85.7214 - mae: 6.4177 - val_loss: 82.2861 - val_mse: 82.2860 - val_mae: 6.5517\n",
      "Epoch 811/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7845 - mse: 85.7845 - mae: 6.4193 - val_loss: 82.1575 - val_mse: 82.1575 - val_mae: 6.5350\n",
      "Epoch 812/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7345 - mse: 85.7345 - mae: 6.4199 - val_loss: 82.6584 - val_mse: 82.6585 - val_mae: 6.5032\n",
      "Epoch 813/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.9019 - mse: 85.9019 - mae: 6.4139 - val_loss: 81.8431 - val_mse: 81.8431 - val_mae: 6.4626\n",
      "Epoch 814/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6990 - mse: 85.6991 - mae: 6.4184 - val_loss: 82.4527 - val_mse: 82.4526 - val_mae: 6.4677\n",
      "Epoch 815/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.7265 - mse: 85.7265 - mae: 6.4109 - val_loss: 81.9990 - val_mse: 81.9990 - val_mae: 6.5182\n",
      "Epoch 816/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7385 - mse: 85.7385 - mae: 6.4208 - val_loss: 82.6442 - val_mse: 82.6442 - val_mae: 6.5876\n",
      "Epoch 817/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7329 - mse: 85.7329 - mae: 6.4184 - val_loss: 82.2814 - val_mse: 82.2815 - val_mae: 6.4231\n",
      "Epoch 818/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.6661 - mse: 85.6662 - mae: 6.4229 - val_loss: 82.4987 - val_mse: 82.4987 - val_mae: 6.4233\n",
      "Epoch 819/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.5843 - mse: 85.5843 - mae: 6.4158 - val_loss: 81.9604 - val_mse: 81.9604 - val_mae: 6.5181\n",
      "Epoch 820/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7432 - mse: 85.7432 - mae: 6.4145 - val_loss: 82.0221 - val_mse: 82.0222 - val_mae: 6.4478\n",
      "Epoch 821/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7329 - mse: 85.7329 - mae: 6.4159 - val_loss: 81.9215 - val_mse: 81.9215 - val_mae: 6.4338\n",
      "Epoch 822/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.6438 - mse: 85.6438 - mae: 6.4168 - val_loss: 82.2915 - val_mse: 82.2915 - val_mae: 6.5088\n",
      "Epoch 823/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8281 - mse: 85.8281 - mae: 6.4243 - val_loss: 82.1338 - val_mse: 82.1339 - val_mae: 6.5089\n",
      "Epoch 824/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7579 - mse: 85.7579 - mae: 6.4234 - val_loss: 82.3891 - val_mse: 82.3891 - val_mae: 6.4063\n",
      "Epoch 825/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7557 - mse: 85.7558 - mae: 6.4152 - val_loss: 82.0040 - val_mse: 82.0041 - val_mae: 6.4317\n",
      "Epoch 826/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7867 - mse: 85.7867 - mae: 6.4147 - val_loss: 82.2655 - val_mse: 82.2654 - val_mae: 6.5314\n",
      "Epoch 827/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.5925 - mse: 85.5925 - mae: 6.4172 - val_loss: 82.8853 - val_mse: 82.8853 - val_mae: 6.5809\n",
      "Epoch 828/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.7907 - mse: 85.7907 - mae: 6.4250 - val_loss: 82.3538 - val_mse: 82.3539 - val_mae: 6.4754\n",
      "Epoch 829/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6141 - mse: 85.6141 - mae: 6.4079 - val_loss: 82.7307 - val_mse: 82.7307 - val_mae: 6.6053\n",
      "Epoch 830/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.5520 - mse: 85.5521 - mae: 6.4116 - val_loss: 82.4503 - val_mse: 82.4503 - val_mae: 6.5028\n",
      "Epoch 831/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6319 - mse: 85.6318 - mae: 6.4152 - val_loss: 83.5375 - val_mse: 83.5375 - val_mae: 6.6769\n",
      "Epoch 832/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7222 - mse: 85.7223 - mae: 6.4142 - val_loss: 82.2205 - val_mse: 82.2204 - val_mae: 6.4518\n",
      "Epoch 833/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 85.6810 - mse: 85.6810 - mae: 6.4200 - val_loss: 82.7592 - val_mse: 82.7591 - val_mae: 6.4088\n",
      "Epoch 834/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8131 - mse: 85.8131 - mae: 6.4148 - val_loss: 81.9126 - val_mse: 81.9126 - val_mae: 6.4767\n",
      "Epoch 835/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7800 - mse: 85.7800 - mae: 6.4257 - val_loss: 82.1013 - val_mse: 82.1013 - val_mae: 6.4574\n",
      "Epoch 836/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.7687 - mse: 85.7687 - mae: 6.4150 - val_loss: 81.9977 - val_mse: 81.9977 - val_mae: 6.4320\n",
      "Epoch 837/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6657 - mse: 85.6658 - mae: 6.4148 - val_loss: 82.8874 - val_mse: 82.8874 - val_mae: 6.5802\n",
      "Epoch 838/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7252 - mse: 85.7252 - mae: 6.4202 - val_loss: 82.2525 - val_mse: 82.2525 - val_mae: 6.5003\n",
      "Epoch 839/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.7366 - mse: 85.7365 - mae: 6.4186 - val_loss: 81.9127 - val_mse: 81.9128 - val_mae: 6.5060\n",
      "Epoch 840/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6768 - mse: 85.6768 - mae: 6.4076 - val_loss: 82.2181 - val_mse: 82.2182 - val_mae: 6.5271\n",
      "Epoch 841/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.5313 - mse: 85.5312 - mae: 6.4054 - val_loss: 82.9458 - val_mse: 82.9458 - val_mae: 6.6755\n",
      "Epoch 842/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.6493 - mse: 85.6493 - mae: 6.4229 - val_loss: 82.3596 - val_mse: 82.3596 - val_mae: 6.5631\n",
      "Epoch 843/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8412 - mse: 85.8413 - mae: 6.4184 - val_loss: 82.2217 - val_mse: 82.2217 - val_mae: 6.4542\n",
      "Epoch 844/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.6888 - mse: 85.6888 - mae: 6.4197 - val_loss: 82.0721 - val_mse: 82.0721 - val_mae: 6.5118\n",
      "Epoch 845/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.7148 - mse: 85.7148 - mae: 6.4169 - val_loss: 83.0779 - val_mse: 83.0779 - val_mae: 6.4497\n",
      "Epoch 846/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7777 - mse: 85.7776 - mae: 6.4130 - val_loss: 81.9337 - val_mse: 81.9337 - val_mae: 6.4853\n",
      "Epoch 847/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7366 - mse: 85.7366 - mae: 6.4159 - val_loss: 81.9968 - val_mse: 81.9968 - val_mae: 6.4743\n",
      "Epoch 848/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.7177 - mse: 85.7177 - mae: 6.4150 - val_loss: 82.0590 - val_mse: 82.0590 - val_mae: 6.4879\n",
      "Epoch 849/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7700 - mse: 85.7700 - mae: 6.4146 - val_loss: 82.1648 - val_mse: 82.1648 - val_mae: 6.4376\n",
      "Epoch 850/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.6540 - mse: 85.6540 - mae: 6.4093 - val_loss: 82.7576 - val_mse: 82.7576 - val_mae: 6.5696\n",
      "Epoch 851/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7565 - mse: 85.7565 - mae: 6.4148 - val_loss: 81.9958 - val_mse: 81.9958 - val_mae: 6.4798\n",
      "Epoch 852/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.5947 - mse: 85.5947 - mae: 6.4157 - val_loss: 82.9346 - val_mse: 82.9346 - val_mae: 6.5736\n",
      "Epoch 853/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7076 - mse: 85.7076 - mae: 6.4200 - val_loss: 82.4663 - val_mse: 82.4663 - val_mae: 6.5249\n",
      "Epoch 854/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.6020 - mse: 85.6020 - mae: 6.4173 - val_loss: 82.1050 - val_mse: 82.1050 - val_mae: 6.4940\n",
      "Epoch 855/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.7320 - mse: 85.7321 - mae: 6.4112 - val_loss: 82.5480 - val_mse: 82.5480 - val_mae: 6.5799\n",
      "Epoch 856/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.7548 - mse: 85.7548 - mae: 6.4223 - val_loss: 82.1106 - val_mse: 82.1105 - val_mae: 6.4741\n",
      "Epoch 857/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.6652 - mse: 85.6651 - mae: 6.4209 - val_loss: 81.9076 - val_mse: 81.9076 - val_mae: 6.4774\n",
      "Epoch 858/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.5688 - mse: 85.5688 - mae: 6.4096 - val_loss: 82.3397 - val_mse: 82.3397 - val_mae: 6.5586\n",
      "Epoch 859/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 85.7228 - mse: 85.7229 - mae: 6.4204 - val_loss: 82.2207 - val_mse: 82.2207 - val_mae: 6.4796\n",
      "Epoch 860/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.4713 - mse: 85.4713 - mae: 6.4085 - val_loss: 81.9119 - val_mse: 81.9119 - val_mae: 6.4496\n",
      "Epoch 861/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 85.6944 - mse: 85.6945 - mae: 6.4127 - val_loss: 81.9021 - val_mse: 81.9021 - val_mae: 6.4569\n",
      "Epoch 862/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 85.7348 - mse: 85.7348 - mae: 6.4062 - val_loss: 82.1376 - val_mse: 82.1376 - val_mae: 6.5638\n",
      "Epoch 863/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.6396 - mse: 85.6395 - mae: 6.4161 - val_loss: 82.4431 - val_mse: 82.4431 - val_mae: 6.4167\n",
      "Epoch 864/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 85.6490 - mse: 85.6490 - mae: 6.4126 - val_loss: 82.1925 - val_mse: 82.1925 - val_mae: 6.5494\n",
      "Epoch 865/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.6150 - mse: 85.6150 - mae: 6.4168 - val_loss: 82.1081 - val_mse: 82.1081 - val_mae: 6.4623\n",
      "Epoch 866/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 85.5618 - mse: 85.5619 - mae: 6.4086 - val_loss: 82.1740 - val_mse: 82.1740 - val_mae: 6.4336\n",
      "Epoch 867/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.6817 - mse: 85.6817 - mae: 6.4060 - val_loss: 82.6367 - val_mse: 82.6367 - val_mae: 6.5421\n",
      "Epoch 868/1000\n",
      "14194/14194 [==============================] - 1s 82us/step - loss: 85.5839 - mse: 85.5839 - mae: 6.4184 - val_loss: 83.6433 - val_mse: 83.6433 - val_mae: 6.6080\n",
      "Epoch 869/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.6515 - mse: 85.6515 - mae: 6.4134 - val_loss: 81.8924 - val_mse: 81.8924 - val_mae: 6.4794\n",
      "Epoch 870/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.7433 - mse: 85.7433 - mae: 6.4170 - val_loss: 82.4263 - val_mse: 82.4263 - val_mae: 6.3908\n",
      "Epoch 871/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.6302 - mse: 85.6302 - mae: 6.4152 - val_loss: 82.0219 - val_mse: 82.0219 - val_mae: 6.4522\n",
      "Epoch 872/1000\n",
      "14194/14194 [==============================] - 1s 82us/step - loss: 85.7289 - mse: 85.7288 - mae: 6.4122 - val_loss: 82.2676 - val_mse: 82.2676 - val_mae: 6.4470\n",
      "Epoch 873/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 85.7254 - mse: 85.7254 - mae: 6.4079 - val_loss: 82.2018 - val_mse: 82.2018 - val_mae: 6.5625\n",
      "Epoch 874/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 85.7188 - mse: 85.7188 - mae: 6.4180 - val_loss: 82.5887 - val_mse: 82.5887 - val_mae: 6.4496\n",
      "Epoch 875/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.7517 - mse: 85.7517 - mae: 6.4180 - val_loss: 81.8943 - val_mse: 81.8943 - val_mae: 6.4643\n",
      "Epoch 876/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 85.7144 - mse: 85.7143 - mae: 6.4160 - val_loss: 82.3682 - val_mse: 82.3683 - val_mae: 6.4411\n",
      "Epoch 877/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.6400 - mse: 85.6400 - mae: 6.4097 - val_loss: 82.3486 - val_mse: 82.3486 - val_mae: 6.4983\n",
      "Epoch 878/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 85.6013 - mse: 85.6012 - mae: 6.4094 - val_loss: 82.1416 - val_mse: 82.1416 - val_mae: 6.4882\n",
      "Epoch 879/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 85.7296 - mse: 85.7295 - mae: 6.4168 - val_loss: 82.2335 - val_mse: 82.2335 - val_mae: 6.5019\n",
      "Epoch 880/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.7338 - mse: 85.7337 - mae: 6.4180 - val_loss: 81.9147 - val_mse: 81.9147 - val_mae: 6.4740\n",
      "Epoch 881/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.6997 - mse: 85.6996 - mae: 6.4154 - val_loss: 82.4012 - val_mse: 82.4012 - val_mae: 6.4137\n",
      "Epoch 882/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 85.6766 - mse: 85.6767 - mae: 6.4058 - val_loss: 82.3724 - val_mse: 82.3724 - val_mae: 6.3839\n",
      "Epoch 883/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.6850 - mse: 85.6849 - mae: 6.4138 - val_loss: 81.9816 - val_mse: 81.9816 - val_mae: 6.5031\n",
      "Epoch 884/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 85.7798 - mse: 85.7798 - mae: 6.4185 - val_loss: 82.2572 - val_mse: 82.2573 - val_mae: 6.4136\n",
      "Epoch 885/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.7277 - mse: 85.7276 - mae: 6.4189 - val_loss: 82.7583 - val_mse: 82.7583 - val_mae: 6.6027\n",
      "Epoch 886/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7364 - mse: 85.7364 - mae: 6.4187 - val_loss: 81.9817 - val_mse: 81.9817 - val_mae: 6.5134\n",
      "Epoch 887/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 85.7317 - mse: 85.7318 - mae: 6.4161 - val_loss: 82.0329 - val_mse: 82.0329 - val_mae: 6.4563\n",
      "Epoch 888/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.6041 - mse: 85.6040 - mae: 6.4169 - val_loss: 82.2525 - val_mse: 82.2525 - val_mae: 6.4775\n",
      "Epoch 889/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7362 - mse: 85.7362 - mae: 6.4218 - val_loss: 82.3065 - val_mse: 82.3065 - val_mae: 6.4742\n",
      "Epoch 890/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 85.6635 - mse: 85.6635 - mae: 6.4164 - val_loss: 82.2226 - val_mse: 82.2226 - val_mae: 6.5121\n",
      "Epoch 891/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.7251 - mse: 85.7251 - mae: 6.4203 - val_loss: 82.1310 - val_mse: 82.1310 - val_mae: 6.5296\n",
      "Epoch 892/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 85.5114 - mse: 85.5114 - mae: 6.4185 - val_loss: 82.5330 - val_mse: 82.5330 - val_mae: 6.5157\n",
      "Epoch 893/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6910 - mse: 85.6909 - mae: 6.4136 - val_loss: 82.3099 - val_mse: 82.3099 - val_mae: 6.4606\n",
      "Epoch 894/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7215 - mse: 85.7216 - mae: 6.4145 - val_loss: 82.0541 - val_mse: 82.0541 - val_mae: 6.4303\n",
      "Epoch 895/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.6231 - mse: 85.6230 - mae: 6.4151 - val_loss: 82.2294 - val_mse: 82.2294 - val_mae: 6.4467\n",
      "Epoch 896/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6593 - mse: 85.6593 - mae: 6.4104 - val_loss: 81.9994 - val_mse: 81.9994 - val_mae: 6.5004\n",
      "Epoch 897/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8054 - mse: 85.8053 - mae: 6.4171 - val_loss: 82.6477 - val_mse: 82.6477 - val_mae: 6.4177\n",
      "Epoch 898/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6944 - mse: 85.6944 - mae: 6.4141 - val_loss: 82.2012 - val_mse: 82.2012 - val_mae: 6.4360\n",
      "Epoch 899/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7260 - mse: 85.7261 - mae: 6.4125 - val_loss: 81.9678 - val_mse: 81.9678 - val_mae: 6.4994\n",
      "Epoch 900/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6458 - mse: 85.6457 - mae: 6.4162 - val_loss: 82.6302 - val_mse: 82.6302 - val_mae: 6.4423\n",
      "Epoch 901/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 85.5612 - mse: 85.5612 - mae: 6.4109 - val_loss: 82.4793 - val_mse: 82.4793 - val_mae: 6.5656\n",
      "Epoch 902/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.6273 - mse: 85.6273 - mae: 6.4131 - val_loss: 82.0358 - val_mse: 82.0358 - val_mae: 6.4444\n",
      "Epoch 903/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7098 - mse: 85.7098 - mae: 6.4236 - val_loss: 83.1765 - val_mse: 83.1765 - val_mae: 6.3739\n",
      "Epoch 904/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6823 - mse: 85.6823 - mae: 6.4080 - val_loss: 82.0989 - val_mse: 82.0989 - val_mae: 6.4025\n",
      "Epoch 905/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7474 - mse: 85.7474 - mae: 6.4208 - val_loss: 82.3634 - val_mse: 82.3634 - val_mae: 6.4269\n",
      "Epoch 906/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7336 - mse: 85.7336 - mae: 6.4171 - val_loss: 81.9700 - val_mse: 81.9700 - val_mae: 6.4500\n",
      "Epoch 907/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.7766 - mse: 85.7765 - mae: 6.4173 - val_loss: 81.9049 - val_mse: 81.9048 - val_mae: 6.4612\n",
      "Epoch 908/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 85.7800 - mse: 85.7800 - mae: 6.4209 - val_loss: 82.2458 - val_mse: 82.2458 - val_mae: 6.4944\n",
      "Epoch 909/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 85.6985 - mse: 85.6984 - mae: 6.4143 - val_loss: 82.3294 - val_mse: 82.3294 - val_mae: 6.4202\n",
      "Epoch 910/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6303 - mse: 85.6303 - mae: 6.4124 - val_loss: 82.3725 - val_mse: 82.3725 - val_mae: 6.5892\n",
      "Epoch 911/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7509 - mse: 85.7510 - mae: 6.4163 - val_loss: 82.0203 - val_mse: 82.0203 - val_mae: 6.4199\n",
      "Epoch 912/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7389 - mse: 85.7388 - mae: 6.4166 - val_loss: 81.9280 - val_mse: 81.9280 - val_mae: 6.4534\n",
      "Epoch 913/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.6940 - mse: 85.6940 - mae: 6.4143 - val_loss: 82.0346 - val_mse: 82.0346 - val_mae: 6.5065\n",
      "Epoch 914/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7963 - mse: 85.7964 - mae: 6.4221 - val_loss: 82.0754 - val_mse: 82.0754 - val_mae: 6.4831\n",
      "Epoch 915/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6850 - mse: 85.6850 - mae: 6.4192 - val_loss: 82.3932 - val_mse: 82.3932 - val_mae: 6.3989\n",
      "Epoch 916/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.7271 - mse: 85.7271 - mae: 6.4196 - val_loss: 82.0130 - val_mse: 82.0130 - val_mae: 6.4926\n",
      "Epoch 917/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.7896 - mse: 85.7896 - mae: 6.4166 - val_loss: 82.0868 - val_mse: 82.0868 - val_mae: 6.4289\n",
      "Epoch 918/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.6072 - mse: 85.6072 - mae: 6.4091 - val_loss: 82.6789 - val_mse: 82.6789 - val_mae: 6.5749\n",
      "Epoch 919/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 85.7858 - mse: 85.7858 - mae: 6.4163 - val_loss: 81.8977 - val_mse: 81.8977 - val_mae: 6.5009\n",
      "Epoch 920/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.4269 - mse: 85.4269 - mae: 6.4111 - val_loss: 83.9861 - val_mse: 83.9861 - val_mae: 6.6848\n",
      "Epoch 921/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7870 - mse: 85.7870 - mae: 6.4158 - val_loss: 83.0214 - val_mse: 83.0214 - val_mae: 6.3955\n",
      "Epoch 922/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.8499 - mse: 85.8499 - mae: 6.4203 - val_loss: 82.4556 - val_mse: 82.4556 - val_mae: 6.4404\n",
      "Epoch 923/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.5383 - mse: 85.5383 - mae: 6.4125 - val_loss: 82.3751 - val_mse: 82.3751 - val_mae: 6.5101\n",
      "Epoch 924/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.6257 - mse: 85.6256 - mae: 6.4183 - val_loss: 82.2985 - val_mse: 82.2985 - val_mae: 6.4061\n",
      "Epoch 925/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7161 - mse: 85.7161 - mae: 6.4115 - val_loss: 82.4985 - val_mse: 82.4985 - val_mae: 6.4821\n",
      "Epoch 926/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7405 - mse: 85.7405 - mae: 6.4158 - val_loss: 82.3009 - val_mse: 82.3009 - val_mae: 6.4503\n",
      "Epoch 927/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.7200 - mse: 85.7200 - mae: 6.4240 - val_loss: 81.9207 - val_mse: 81.9207 - val_mae: 6.4823\n",
      "Epoch 928/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6044 - mse: 85.6043 - mae: 6.4113 - val_loss: 82.1672 - val_mse: 82.1672 - val_mae: 6.4565\n",
      "Epoch 929/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6745 - mse: 85.6746 - mae: 6.4199 - val_loss: 82.1128 - val_mse: 82.1128 - val_mae: 6.4169\n",
      "Epoch 930/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6382 - mse: 85.6382 - mae: 6.4095 - val_loss: 82.2390 - val_mse: 82.2390 - val_mae: 6.4505\n",
      "Epoch 931/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.5910 - mse: 85.5909 - mae: 6.4142 - val_loss: 82.6809 - val_mse: 82.6809 - val_mae: 6.6101\n",
      "Epoch 932/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.5979 - mse: 85.5978 - mae: 6.4195 - val_loss: 81.9830 - val_mse: 81.9830 - val_mae: 6.4813\n",
      "Epoch 933/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.7414 - mse: 85.7414 - mae: 6.4058 - val_loss: 81.9413 - val_mse: 81.9413 - val_mae: 6.5136\n",
      "Epoch 934/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 85.6925 - mse: 85.6925 - mae: 6.4114 - val_loss: 82.4895 - val_mse: 82.4895 - val_mae: 6.5180\n",
      "Epoch 935/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6216 - mse: 85.6216 - mae: 6.4114 - val_loss: 82.7037 - val_mse: 82.7037 - val_mae: 6.5717\n",
      "Epoch 936/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.7413 - mse: 85.7413 - mae: 6.4217 - val_loss: 82.4251 - val_mse: 82.4251 - val_mae: 6.4801\n",
      "Epoch 937/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7418 - mse: 85.7418 - mae: 6.4110 - val_loss: 82.5238 - val_mse: 82.5238 - val_mae: 6.5185\n",
      "Epoch 938/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.5586 - mse: 85.5586 - mae: 6.4152 - val_loss: 82.2016 - val_mse: 82.2016 - val_mae: 6.4488\n",
      "Epoch 939/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7309 - mse: 85.7309 - mae: 6.4103 - val_loss: 82.0926 - val_mse: 82.0926 - val_mae: 6.5487\n",
      "Epoch 940/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.6262 - mse: 85.6263 - mae: 6.4128 - val_loss: 82.8754 - val_mse: 82.8754 - val_mae: 6.5315\n",
      "Epoch 941/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.6934 - mse: 85.6934 - mae: 6.4110 - val_loss: 82.2191 - val_mse: 82.2192 - val_mae: 6.5502\n",
      "Epoch 942/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.6458 - mse: 85.6458 - mae: 6.4183 - val_loss: 81.9692 - val_mse: 81.9692 - val_mae: 6.4577\n",
      "Epoch 943/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7431 - mse: 85.7431 - mae: 6.4207 - val_loss: 82.0354 - val_mse: 82.0354 - val_mae: 6.4787\n",
      "Epoch 944/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.6216 - mse: 85.6216 - mae: 6.4157 - val_loss: 81.9768 - val_mse: 81.9769 - val_mae: 6.4948\n",
      "Epoch 945/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6645 - mse: 85.6645 - mae: 6.4129 - val_loss: 82.5117 - val_mse: 82.5117 - val_mae: 6.4773\n",
      "Epoch 946/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.6701 - mse: 85.6701 - mae: 6.4111 - val_loss: 83.1224 - val_mse: 83.1224 - val_mae: 6.6293\n",
      "Epoch 947/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.6191 - mse: 85.6191 - mae: 6.4185 - val_loss: 82.3633 - val_mse: 82.3633 - val_mae: 6.5263\n",
      "Epoch 948/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6698 - mse: 85.6698 - mae: 6.4088 - val_loss: 82.2129 - val_mse: 82.2130 - val_mae: 6.5532\n",
      "Epoch 949/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.8164 - mse: 85.8164 - mae: 6.4175 - val_loss: 82.2531 - val_mse: 82.2531 - val_mae: 6.5338\n",
      "Epoch 950/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.6509 - mse: 85.6509 - mae: 6.4138 - val_loss: 82.9882 - val_mse: 82.9882 - val_mae: 6.4453\n",
      "Epoch 951/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.6670 - mse: 85.6670 - mae: 6.4218 - val_loss: 82.1315 - val_mse: 82.1314 - val_mae: 6.4665\n",
      "Epoch 952/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6992 - mse: 85.6991 - mae: 6.4186 - val_loss: 82.3861 - val_mse: 82.3861 - val_mae: 6.4552\n",
      "Epoch 953/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7541 - mse: 85.7541 - mae: 6.4103 - val_loss: 82.2944 - val_mse: 82.2944 - val_mae: 6.4487\n",
      "Epoch 954/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6082 - mse: 85.6081 - mae: 6.4115 - val_loss: 81.8524 - val_mse: 81.8524 - val_mae: 6.4486\n",
      "Epoch 955/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7331 - mse: 85.7330 - mae: 6.4157 - val_loss: 82.1169 - val_mse: 82.1169 - val_mae: 6.4459\n",
      "Epoch 956/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6711 - mse: 85.6711 - mae: 6.4137 - val_loss: 81.7772 - val_mse: 81.7772 - val_mae: 6.4644\n",
      "Epoch 957/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7523 - mse: 85.7523 - mae: 6.4205 - val_loss: 82.0692 - val_mse: 82.0692 - val_mae: 6.5002\n",
      "Epoch 958/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6891 - mse: 85.6891 - mae: 6.4124 - val_loss: 82.3274 - val_mse: 82.3274 - val_mae: 6.5055\n",
      "Epoch 959/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7052 - mse: 85.7052 - mae: 6.4194 - val_loss: 82.2258 - val_mse: 82.2258 - val_mae: 6.4199\n",
      "Epoch 960/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.6353 - mse: 85.6353 - mae: 6.4125 - val_loss: 82.7468 - val_mse: 82.7469 - val_mae: 6.4173\n",
      "Epoch 961/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.6093 - mse: 85.6093 - mae: 6.4056 - val_loss: 83.4453 - val_mse: 83.4453 - val_mae: 6.5511\n",
      "Epoch 962/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7116 - mse: 85.7115 - mae: 6.4142 - val_loss: 82.1459 - val_mse: 82.1459 - val_mae: 6.4667\n",
      "Epoch 963/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6889 - mse: 85.6889 - mae: 6.4091 - val_loss: 82.2889 - val_mse: 82.2889 - val_mae: 6.5282\n",
      "Epoch 964/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6325 - mse: 85.6325 - mae: 6.4225 - val_loss: 83.0374 - val_mse: 83.0374 - val_mae: 6.4505\n",
      "Epoch 965/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7465 - mse: 85.7465 - mae: 6.4065 - val_loss: 82.1230 - val_mse: 82.1230 - val_mae: 6.5702\n",
      "Epoch 966/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6679 - mse: 85.6679 - mae: 6.4171 - val_loss: 82.1209 - val_mse: 82.1209 - val_mae: 6.5117\n",
      "Epoch 967/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.8082 - mse: 85.8082 - mae: 6.4258 - val_loss: 81.9516 - val_mse: 81.9516 - val_mae: 6.4604\n",
      "Epoch 968/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.8210 - mse: 85.8209 - mae: 6.4210 - val_loss: 81.8733 - val_mse: 81.8733 - val_mae: 6.4415\n",
      "Epoch 969/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6769 - mse: 85.6769 - mae: 6.4096 - val_loss: 82.3330 - val_mse: 82.3331 - val_mae: 6.5495\n",
      "Epoch 970/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6939 - mse: 85.6939 - mae: 6.4179 - val_loss: 82.0386 - val_mse: 82.0385 - val_mae: 6.4650\n",
      "Epoch 971/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.5180 - mse: 85.5181 - mae: 6.4111 - val_loss: 83.1428 - val_mse: 83.1428 - val_mae: 6.6320\n",
      "Epoch 972/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7063 - mse: 85.7064 - mae: 6.4253 - val_loss: 82.5140 - val_mse: 82.5140 - val_mae: 6.4644\n",
      "Epoch 973/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7193 - mse: 85.7193 - mae: 6.4222 - val_loss: 81.8863 - val_mse: 81.8863 - val_mae: 6.4447\n",
      "Epoch 974/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6427 - mse: 85.6428 - mae: 6.4093 - val_loss: 82.3082 - val_mse: 82.3082 - val_mae: 6.6007\n",
      "Epoch 975/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6495 - mse: 85.6495 - mae: 6.4132 - val_loss: 82.0514 - val_mse: 82.0514 - val_mae: 6.5280\n",
      "Epoch 976/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.5756 - mse: 85.5756 - mae: 6.4174 - val_loss: 81.9927 - val_mse: 81.9927 - val_mae: 6.4452\n",
      "Epoch 977/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.4355 - mse: 85.4354 - mae: 6.4129 - val_loss: 82.8556 - val_mse: 82.8556 - val_mae: 6.3762\n",
      "Epoch 978/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7720 - mse: 85.7720 - mae: 6.4180 - val_loss: 82.0703 - val_mse: 82.0703 - val_mae: 6.5001\n",
      "Epoch 979/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.6904 - mse: 85.6904 - mae: 6.4132 - val_loss: 82.1868 - val_mse: 82.1868 - val_mae: 6.4421\n",
      "Epoch 980/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.5782 - mse: 85.5783 - mae: 6.4141 - val_loss: 82.5522 - val_mse: 82.5522 - val_mae: 6.4326\n",
      "Epoch 981/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.7705 - mse: 85.7704 - mae: 6.4143 - val_loss: 81.8813 - val_mse: 81.8813 - val_mae: 6.4698\n",
      "Epoch 982/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.7109 - mse: 85.7109 - mae: 6.4184 - val_loss: 81.9090 - val_mse: 81.9090 - val_mae: 6.4989\n",
      "Epoch 983/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6133 - mse: 85.6133 - mae: 6.4098 - val_loss: 82.1113 - val_mse: 82.1113 - val_mae: 6.4361\n",
      "Epoch 984/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6091 - mse: 85.6092 - mae: 6.4187 - val_loss: 81.9080 - val_mse: 81.9081 - val_mae: 6.4625\n",
      "Epoch 985/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6899 - mse: 85.6899 - mae: 6.4214 - val_loss: 82.4877 - val_mse: 82.4877 - val_mae: 6.3940\n",
      "Epoch 986/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.7483 - mse: 85.7481 - mae: 6.4230 - val_loss: 82.1763 - val_mse: 82.1763 - val_mae: 6.5270\n",
      "Epoch 987/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7217 - mse: 85.7217 - mae: 6.4199 - val_loss: 82.1197 - val_mse: 82.1197 - val_mae: 6.4666\n",
      "Epoch 988/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6540 - mse: 85.6540 - mae: 6.4084 - val_loss: 82.4182 - val_mse: 82.4182 - val_mae: 6.5240\n",
      "Epoch 989/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6438 - mse: 85.6437 - mae: 6.4173 - val_loss: 82.2604 - val_mse: 82.2604 - val_mae: 6.3933\n",
      "Epoch 990/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6352 - mse: 85.6351 - mae: 6.4131 - val_loss: 81.9011 - val_mse: 81.9011 - val_mae: 6.4362\n",
      "Epoch 991/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.7164 - mse: 85.7163 - mae: 6.4138 - val_loss: 82.2553 - val_mse: 82.2553 - val_mae: 6.4581\n",
      "Epoch 992/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6876 - mse: 85.6876 - mae: 6.4150 - val_loss: 82.5397 - val_mse: 82.5397 - val_mae: 6.4583\n",
      "Epoch 993/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.6566 - mse: 85.6567 - mae: 6.4191 - val_loss: 82.2062 - val_mse: 82.2061 - val_mae: 6.5419\n",
      "Epoch 994/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6436 - mse: 85.6437 - mae: 6.4161 - val_loss: 82.0919 - val_mse: 82.0919 - val_mae: 6.4464\n",
      "Epoch 995/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6542 - mse: 85.6543 - mae: 6.4242 - val_loss: 82.2970 - val_mse: 82.2970 - val_mae: 6.4173\n",
      "Epoch 996/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7114 - mse: 85.7114 - mae: 6.4126 - val_loss: 82.0073 - val_mse: 82.0072 - val_mae: 6.4633\n",
      "Epoch 997/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.6191 - mse: 85.6191 - mae: 6.4125 - val_loss: 82.3279 - val_mse: 82.3279 - val_mae: 6.4107\n",
      "Epoch 998/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.6499 - mse: 85.6499 - mae: 6.4075 - val_loss: 82.1431 - val_mse: 82.1431 - val_mae: 6.5006\n",
      "Epoch 999/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 85.6961 - mse: 85.6962 - mae: 6.4174 - val_loss: 82.1343 - val_mse: 82.1343 - val_mae: 6.4727\n",
      "Epoch 1000/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.6722 - mse: 85.6722 - mae: 6.4107 - val_loss: 82.5368 - val_mse: 82.5369 - val_mae: 6.6093\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt,asarray\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=1000, batch_size=10,  verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82.49384114127591, 82.49384307861328, 6.554439067840576] [9.08261202 9.08261213 2.56016388]\n"
     ]
    }
   ],
   "source": [
    "error = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "# print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
    "print(error, sqrt(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model7b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# d = defaultdict(LabelEncoder)\n",
    "\n",
    "# labeled_df = X_df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# labeled_df\n",
    "# labeled_df.apply(lambda x: d[x.name].inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.expand_dims(X_test_scaled[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: [[35.14664]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted price: {model.predict(test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
