{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>subsubregion</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14.1</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This white has an expert level of intensity an...</td>\n",
       "      <td>Dutton Ranch Walker Hill Vineyard</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Green Valley</td>\n",
       "      <td>Dutton-Goldfield 2016 Dutton Ranch Walker Hill...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/dutton-go...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Dutton-Goldfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>12.6</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>Stunning aromatics combine spicy citrus, tangy...</td>\n",
       "      <td>Maresh Vineyard Old Vine</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Dundee Hills</td>\n",
       "      <td>Harper Voit 2016 Maresh Vineyard Old Vine Ries...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/harper-vo...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "      <td>Harper Voit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a structured and remarkable wine, burs...</td>\n",
       "      <td>Year of the Monkey Single Vineyard</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Carneros</td>\n",
       "      <td>Donum 2016 Year of the Monkey Single Vineyard ...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/donum-201...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Donum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>The wine's aromas are arresting in notes of le...</td>\n",
       "      <td>Chaleur Blanc</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>DeLille 2017 Chaleur Blanc White (Columbia Val...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/delille-2...</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "      <td>DeLille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>14.7</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a grainy, structured and textured whit...</td>\n",
       "      <td>Lewis MacGregor Estate Vineyard</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Russian River Valley</td>\n",
       "      <td>Williams Selyem 2016 Lewis MacGregor Estate Vi...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/williams-...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Williams Selyem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  alcohol category country  \\\n",
       "0   4     14.1    White      US   \n",
       "1  39     12.6    White      US   \n",
       "2  53     13.8    White      US   \n",
       "3  56     13.8    White      US   \n",
       "4  74     14.7    White      US   \n",
       "\n",
       "                                         description  \\\n",
       "0  This white has an expert level of intensity an...   \n",
       "1  Stunning aromatics combine spicy citrus, tangy...   \n",
       "2  This is a structured and remarkable wine, burs...   \n",
       "3  The wine's aromas are arresting in notes of le...   \n",
       "4  This is a grainy, structured and textured whit...   \n",
       "\n",
       "                          designation  price  rating      region  \\\n",
       "0   Dutton Ranch Walker Hill Vineyard   50.0      94  California   \n",
       "1            Maresh Vineyard Old Vine   30.0      94      Oregon   \n",
       "2  Year of the Monkey Single Vineyard   60.0      94  California   \n",
       "3                       Chaleur Blanc   35.0      94  Washington   \n",
       "4     Lewis MacGregor Estate Vineyard   65.0      94  California   \n",
       "\n",
       "           subregion          subsubregion  \\\n",
       "0             Sonoma          Green Valley   \n",
       "1  Willamette Valley          Dundee Hills   \n",
       "2        Napa-Sonoma              Carneros   \n",
       "3    Columbia Valley  Columbia Valley (WA)   \n",
       "4             Sonoma  Russian River Valley   \n",
       "\n",
       "                                               title  \\\n",
       "0  Dutton-Goldfield 2016 Dutton Ranch Walker Hill...   \n",
       "1  Harper Voit 2016 Maresh Vineyard Old Vine Ries...   \n",
       "2  Donum 2016 Year of the Monkey Single Vineyard ...   \n",
       "3  DeLille 2017 Chaleur Blanc White (Columbia Val...   \n",
       "4  Williams Selyem 2016 Lewis MacGregor Estate Vi...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.winemag.com/buying-guide/dutton-go...   \n",
       "1  https://www.winemag.com/buying-guide/harper-vo...   \n",
       "2  https://www.winemag.com/buying-guide/donum-201...   \n",
       "3  https://www.winemag.com/buying-guide/delille-2...   \n",
       "4  https://www.winemag.com/buying-guide/williams-...   \n",
       "\n",
       "                     varietal  vintage            winery  \n",
       "0                  Chardonnay     2016  Dutton-Goldfield  \n",
       "1                    Riesling     2016       Harper Voit  \n",
       "2                  Chardonnay     2016             Donum  \n",
       "3  Bordeaux-style White Blend     2017           DeLille  \n",
       "4                  Chardonnay     2016   Williams Selyem  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Resources/uswhites.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  price  rating      region          subregion  \\\n",
       "0     14.1   50.0      94  California             Sonoma   \n",
       "1     12.6   30.0      94      Oregon  Willamette Valley   \n",
       "2     13.8   60.0      94  California        Napa-Sonoma   \n",
       "3     13.8   35.0      94  Washington    Columbia Valley   \n",
       "4     14.7   65.0      94  California             Sonoma   \n",
       "\n",
       "                     varietal  vintage  \n",
       "0                  Chardonnay     2016  \n",
       "1                    Riesling     2016  \n",
       "2                  Chardonnay     2016  \n",
       "3  Bordeaux-style White Blend     2017  \n",
       "4                  Chardonnay     2016  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['ID', 'category', 'country', 'description', 'designation', 'subsubregion', 'title', 'url', 'winery'], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(df)\n",
    "X_df['subregion'] = X_df['subregion'].astype(str)\n",
    "X_df['vintage'] = X_df['vintage'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23653</td>\n",
       "      <td>147</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23654</td>\n",
       "      <td>147</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23655</td>\n",
       "      <td>159</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23656</td>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>138</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23657</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23658 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       alcohol  rating  region  subregion  varietal  vintage\n",
       "0          130      14       2         42        17       22\n",
       "1           68      14      17         56        87       22\n",
       "2          118      14       2         28        17       22\n",
       "3          118      14      24          7        10       23\n",
       "4          152      14       2         42        17       22\n",
       "...        ...     ...     ...        ...       ...      ...\n",
       "23653      147      11       2          3        17       11\n",
       "23654      147      12       2          3        17       11\n",
       "23655      159      12       2          3        17       11\n",
       "23656      176      12       2         42       138       10\n",
       "23657      139      12       2          2        17       11\n",
       "\n",
       "[23658 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= X.apply(LabelEncoder().fit_transform)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130.  14.   2.  42.  17.  22.]\n",
      " [ 68.  14.  17.  56.  87.  22.]\n",
      " [118.  14.   2.  28.  17.  22.]\n",
      " ...\n",
      " [159.  12.   2.   3.  17.  11.]\n",
      " [176.  12.   2.  42. 138.  10.]\n",
      " [139.  12.   2.   2.  17.  11.]]\n",
      "[50. 30. 60. ... 29. 24. 55.]\n"
     ]
    }
   ],
   "source": [
    "X = X.values.astype(\"float32\")\n",
    "print(X)\n",
    "y = y.values.astype(\"float32\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915,)\n",
      "(5915, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = scaler_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 197\n",
      "Trainable params: 197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', name='dense_1', kernel_initializer='random_uniform', input_dim=(input_dims)))\n",
    "model.add(Dense(8, activation='relu', name='dense_2', kernel_initializer='random_uniform'))\n",
    "model.add(Dense(1, activation='linear', name='predictions'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14194 samples, validate on 3549 samples\n",
      "Epoch 1/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 255.6637 - mse: 255.6637 - mae: 11.5369 - val_loss: 94.4804 - val_mse: 94.4804 - val_mae: 7.0632\n",
      "Epoch 2/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 98.2828 - mse: 98.2828 - mae: 6.9865 - val_loss: 92.2160 - val_mse: 92.2160 - val_mae: 6.9784\n",
      "Epoch 3/1000\n",
      "14194/14194 [==============================] - 1s 82us/step - loss: 96.7563 - mse: 96.7563 - mae: 6.9225 - val_loss: 90.1130 - val_mse: 90.1130 - val_mae: 6.8525\n",
      "Epoch 4/1000\n",
      "14194/14194 [==============================] - 1s 72us/step - loss: 95.2794 - mse: 95.2794 - mae: 6.8570 - val_loss: 89.6469 - val_mse: 89.6469 - val_mae: 6.8583\n",
      "Epoch 5/1000\n",
      "14194/14194 [==============================] - 1s 75us/step - loss: 94.1586 - mse: 94.1586 - mae: 6.8120 - val_loss: 88.2307 - val_mse: 88.2307 - val_mae: 6.7853\n",
      "Epoch 6/1000\n",
      "14194/14194 [==============================] - 1s 74us/step - loss: 93.4197 - mse: 93.4196 - mae: 6.7798 - val_loss: 87.6936 - val_mse: 87.6936 - val_mae: 6.6598\n",
      "Epoch 7/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 92.6919 - mse: 92.6918 - mae: 6.7345 - val_loss: 87.0307 - val_mse: 87.0306 - val_mae: 6.6353\n",
      "Epoch 8/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 91.9693 - mse: 91.9694 - mae: 6.7042 - val_loss: 86.6050 - val_mse: 86.6050 - val_mae: 6.6898\n",
      "Epoch 9/1000\n",
      "14194/14194 [==============================] - 1s 75us/step - loss: 91.3683 - mse: 91.3682 - mae: 6.6764 - val_loss: 86.7597 - val_mse: 86.7597 - val_mae: 6.6724\n",
      "Epoch 10/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 90.9876 - mse: 90.9876 - mae: 6.6594 - val_loss: 85.7563 - val_mse: 85.7562 - val_mae: 6.6202\n",
      "Epoch 11/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 90.5321 - mse: 90.5322 - mae: 6.6395 - val_loss: 85.6989 - val_mse: 85.6989 - val_mae: 6.5787\n",
      "Epoch 12/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 90.1556 - mse: 90.1555 - mae: 6.6204 - val_loss: 85.7019 - val_mse: 85.7019 - val_mae: 6.6628\n",
      "Epoch 13/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 89.8923 - mse: 89.8923 - mae: 6.6084 - val_loss: 85.0239 - val_mse: 85.0239 - val_mae: 6.6474\n",
      "Epoch 14/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 89.6041 - mse: 89.6041 - mae: 6.6007 - val_loss: 85.0182 - val_mse: 85.0182 - val_mae: 6.5717\n",
      "Epoch 15/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 89.5062 - mse: 89.5063 - mae: 6.5925 - val_loss: 84.9667 - val_mse: 84.9667 - val_mae: 6.6255\n",
      "Epoch 16/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 89.5196 - mse: 89.5197 - mae: 6.5876 - val_loss: 84.7193 - val_mse: 84.7193 - val_mae: 6.6315\n",
      "Epoch 17/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 89.2295 - mse: 89.2294 - mae: 6.5865 - val_loss: 84.8257 - val_mse: 84.8257 - val_mae: 6.5578\n",
      "Epoch 18/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 89.1563 - mse: 89.1563 - mae: 6.5717 - val_loss: 85.5513 - val_mse: 85.5513 - val_mae: 6.7306\n",
      "Epoch 19/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.0973 - mse: 89.0972 - mae: 6.5746 - val_loss: 85.0267 - val_mse: 85.0267 - val_mae: 6.5317\n",
      "Epoch 20/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 89.0073 - mse: 89.0074 - mae: 6.5636 - val_loss: 84.6213 - val_mse: 84.6213 - val_mae: 6.6221\n",
      "Epoch 21/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.9666 - mse: 88.9666 - mae: 6.5687 - val_loss: 84.5392 - val_mse: 84.5392 - val_mae: 6.5538\n",
      "Epoch 22/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.9141 - mse: 88.9141 - mae: 6.5623 - val_loss: 84.4123 - val_mse: 84.4124 - val_mae: 6.6245\n",
      "Epoch 23/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.8413 - mse: 88.8412 - mae: 6.5589 - val_loss: 85.0904 - val_mse: 85.0904 - val_mae: 6.4883\n",
      "Epoch 24/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.7610 - mse: 88.7609 - mae: 6.5556 - val_loss: 84.4101 - val_mse: 84.4101 - val_mae: 6.5388\n",
      "Epoch 25/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.6983 - mse: 88.6984 - mae: 6.5564 - val_loss: 84.4920 - val_mse: 84.4920 - val_mae: 6.5457\n",
      "Epoch 26/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.7416 - mse: 88.7416 - mae: 6.5482 - val_loss: 84.5098 - val_mse: 84.5098 - val_mae: 6.5790\n",
      "Epoch 27/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.6427 - mse: 88.6426 - mae: 6.5498 - val_loss: 84.4159 - val_mse: 84.4159 - val_mae: 6.5244\n",
      "Epoch 28/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.5274 - mse: 88.5275 - mae: 6.5496 - val_loss: 85.8400 - val_mse: 85.8400 - val_mae: 6.4646\n",
      "Epoch 29/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 88.6437 - mse: 88.6437 - mae: 6.5374 - val_loss: 84.2953 - val_mse: 84.2953 - val_mae: 6.5367\n",
      "Epoch 30/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.4501 - mse: 88.4501 - mae: 6.5381 - val_loss: 84.7922 - val_mse: 84.7922 - val_mae: 6.5267\n",
      "Epoch 31/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.3548 - mse: 88.3548 - mae: 6.5299 - val_loss: 85.4872 - val_mse: 85.4872 - val_mae: 6.6966\n",
      "Epoch 32/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 88.5547 - mse: 88.5547 - mae: 6.5433 - val_loss: 84.0459 - val_mse: 84.0460 - val_mae: 6.5337\n",
      "Epoch 33/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.3488 - mse: 88.3489 - mae: 6.5404 - val_loss: 85.3039 - val_mse: 85.3039 - val_mae: 6.4959\n",
      "Epoch 34/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.3495 - mse: 88.3494 - mae: 6.5397 - val_loss: 84.2528 - val_mse: 84.2528 - val_mae: 6.5483\n",
      "Epoch 35/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.2723 - mse: 88.2722 - mae: 6.5294 - val_loss: 84.1025 - val_mse: 84.1025 - val_mae: 6.5474\n",
      "Epoch 36/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 88.2612 - mse: 88.2612 - mae: 6.5360 - val_loss: 84.2333 - val_mse: 84.2333 - val_mae: 6.5149\n",
      "Epoch 37/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 88.3461 - mse: 88.3460 - mae: 6.5292 - val_loss: 83.9767 - val_mse: 83.9767 - val_mae: 6.4959\n",
      "Epoch 38/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 88.0868 - mse: 88.0868 - mae: 6.5264 - val_loss: 84.3676 - val_mse: 84.3676 - val_mae: 6.4980\n",
      "Epoch 39/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.2571 - mse: 88.2571 - mae: 6.5314 - val_loss: 84.1454 - val_mse: 84.1454 - val_mae: 6.4693\n",
      "Epoch 40/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.1262 - mse: 88.1261 - mae: 6.5223 - val_loss: 84.1132 - val_mse: 84.1132 - val_mae: 6.5867\n",
      "Epoch 41/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.1634 - mse: 88.1635 - mae: 6.5190 - val_loss: 84.1220 - val_mse: 84.1220 - val_mae: 6.4920\n",
      "Epoch 42/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.3435 - mse: 88.3435 - mae: 6.5308 - val_loss: 83.9247 - val_mse: 83.9247 - val_mae: 6.5667\n",
      "Epoch 43/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 88.0813 - mse: 88.0812 - mae: 6.5241 - val_loss: 84.0748 - val_mse: 84.0748 - val_mae: 6.4797\n",
      "Epoch 44/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.2035 - mse: 88.2035 - mae: 6.5202 - val_loss: 83.8210 - val_mse: 83.8210 - val_mae: 6.4865\n",
      "Epoch 45/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.0942 - mse: 88.0942 - mae: 6.5115 - val_loss: 83.6953 - val_mse: 83.6953 - val_mae: 6.5266\n",
      "Epoch 46/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.0829 - mse: 88.0829 - mae: 6.5186 - val_loss: 84.6329 - val_mse: 84.6329 - val_mae: 6.4459\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 134us/step - loss: 88.1325 - mse: 88.1325 - mae: 6.5169 - val_loss: 83.8000 - val_mse: 83.8000 - val_mae: 6.4886\n",
      "Epoch 48/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.1374 - mse: 88.1374 - mae: 6.5171 - val_loss: 83.6552 - val_mse: 83.6552 - val_mae: 6.5002\n",
      "Epoch 49/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.0521 - mse: 88.0521 - mae: 6.5131 - val_loss: 83.7166 - val_mse: 83.7166 - val_mae: 6.5546\n",
      "Epoch 50/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.0235 - mse: 88.0236 - mae: 6.5187 - val_loss: 83.8065 - val_mse: 83.8065 - val_mae: 6.5473\n",
      "Epoch 51/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 87.9378 - mse: 87.9378 - mae: 6.5098 - val_loss: 83.8996 - val_mse: 83.8996 - val_mae: 6.4904\n",
      "Epoch 52/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.8428 - mse: 87.8429 - mae: 6.5051 - val_loss: 84.1690 - val_mse: 84.1690 - val_mae: 6.5414\n",
      "Epoch 53/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 87.7773 - mse: 87.7773 - mae: 6.5103 - val_loss: 84.8282 - val_mse: 84.8283 - val_mae: 6.4341\n",
      "Epoch 54/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.8746 - mse: 87.8746 - mae: 6.5106 - val_loss: 84.9254 - val_mse: 84.9254 - val_mae: 6.4798\n",
      "Epoch 55/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 87.8679 - mse: 87.8680 - mae: 6.5021 - val_loss: 83.5671 - val_mse: 83.5671 - val_mae: 6.4743\n",
      "Epoch 56/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.8987 - mse: 87.8987 - mae: 6.5085 - val_loss: 83.6344 - val_mse: 83.6344 - val_mae: 6.4711\n",
      "Epoch 57/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.6401 - mse: 87.6401 - mae: 6.4969 - val_loss: 83.8212 - val_mse: 83.8213 - val_mae: 6.6183\n",
      "Epoch 58/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.7885 - mse: 87.7884 - mae: 6.5044 - val_loss: 83.5984 - val_mse: 83.5983 - val_mae: 6.6002\n",
      "Epoch 59/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 87.6198 - mse: 87.6198 - mae: 6.5036 - val_loss: 83.3147 - val_mse: 83.3148 - val_mae: 6.5583\n",
      "Epoch 60/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.6701 - mse: 87.6701 - mae: 6.5001 - val_loss: 84.1316 - val_mse: 84.1317 - val_mae: 6.6289\n",
      "Epoch 61/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.7339 - mse: 87.7339 - mae: 6.5028 - val_loss: 83.7345 - val_mse: 83.7345 - val_mae: 6.5616\n",
      "Epoch 62/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 87.7004 - mse: 87.7004 - mae: 6.5009 - val_loss: 83.8352 - val_mse: 83.8352 - val_mae: 6.5677\n",
      "Epoch 63/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 87.7145 - mse: 87.7146 - mae: 6.5036 - val_loss: 83.2715 - val_mse: 83.2714 - val_mae: 6.5355\n",
      "Epoch 64/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.5998 - mse: 87.5998 - mae: 6.5032 - val_loss: 83.0646 - val_mse: 83.0645 - val_mae: 6.4874\n",
      "Epoch 65/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 87.5574 - mse: 87.5575 - mae: 6.5011 - val_loss: 83.4823 - val_mse: 83.4823 - val_mae: 6.5209\n",
      "Epoch 66/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.6090 - mse: 87.6090 - mae: 6.5009 - val_loss: 83.4130 - val_mse: 83.4130 - val_mae: 6.5503\n",
      "Epoch 67/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 87.4587 - mse: 87.4588 - mae: 6.4964 - val_loss: 83.2332 - val_mse: 83.2332 - val_mae: 6.5113\n",
      "Epoch 68/1000\n",
      "14194/14194 [==============================] - 3s 198us/step - loss: 87.5824 - mse: 87.5823 - mae: 6.5015 - val_loss: 83.2354 - val_mse: 83.2354 - val_mae: 6.4801\n",
      "Epoch 69/1000\n",
      "14194/14194 [==============================] - 3s 189us/step - loss: 87.3679 - mse: 87.3679 - mae: 6.4833 - val_loss: 83.5821 - val_mse: 83.5821 - val_mae: 6.6061\n",
      "Epoch 70/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 87.5302 - mse: 87.5302 - mae: 6.4958 - val_loss: 83.1213 - val_mse: 83.1213 - val_mae: 6.5802\n",
      "Epoch 71/1000\n",
      "14194/14194 [==============================] - 2s 154us/step - loss: 87.4293 - mse: 87.4294 - mae: 6.4967 - val_loss: 83.2062 - val_mse: 83.2062 - val_mae: 6.4733\n",
      "Epoch 72/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.5252 - mse: 87.5252 - mae: 6.4833 - val_loss: 83.1288 - val_mse: 83.1288 - val_mae: 6.5297\n",
      "Epoch 73/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 87.3829 - mse: 87.3829 - mae: 6.4982 - val_loss: 83.9315 - val_mse: 83.9316 - val_mae: 6.4323\n",
      "Epoch 74/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 87.3841 - mse: 87.3842 - mae: 6.4924 - val_loss: 83.0102 - val_mse: 83.0102 - val_mae: 6.4667\n",
      "Epoch 75/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 87.3871 - mse: 87.3871 - mae: 6.4821 - val_loss: 82.9076 - val_mse: 82.9076 - val_mae: 6.4717\n",
      "Epoch 76/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 87.3109 - mse: 87.3109 - mae: 6.4935 - val_loss: 82.9797 - val_mse: 82.9797 - val_mae: 6.5308\n",
      "Epoch 77/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.3579 - mse: 87.3579 - mae: 6.4885 - val_loss: 82.9901 - val_mse: 82.9901 - val_mae: 6.5108\n",
      "Epoch 78/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 87.3218 - mse: 87.3218 - mae: 6.4870 - val_loss: 82.6997 - val_mse: 82.6997 - val_mae: 6.5034\n",
      "Epoch 79/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.2816 - mse: 87.2816 - mae: 6.4811 - val_loss: 83.1039 - val_mse: 83.1039 - val_mae: 6.6110\n",
      "Epoch 80/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.3151 - mse: 87.3151 - mae: 6.4882 - val_loss: 82.7749 - val_mse: 82.7750 - val_mae: 6.5171\n",
      "Epoch 81/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.3568 - mse: 87.3567 - mae: 6.4889 - val_loss: 83.0669 - val_mse: 83.0669 - val_mae: 6.5211\n",
      "Epoch 82/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.1546 - mse: 87.1545 - mae: 6.4881 - val_loss: 83.5390 - val_mse: 83.5390 - val_mae: 6.4206\n",
      "Epoch 83/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 87.3429 - mse: 87.3429 - mae: 6.4918 - val_loss: 83.1063 - val_mse: 83.1064 - val_mae: 6.5644\n",
      "Epoch 84/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 87.3596 - mse: 87.3596 - mae: 6.4909 - val_loss: 82.8493 - val_mse: 82.8493 - val_mae: 6.5634\n",
      "Epoch 85/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.3461 - mse: 87.3462 - mae: 6.4866 - val_loss: 82.8581 - val_mse: 82.8581 - val_mae: 6.5131\n",
      "Epoch 86/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.3009 - mse: 87.3009 - mae: 6.4910 - val_loss: 82.6607 - val_mse: 82.6608 - val_mae: 6.5648\n",
      "Epoch 87/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.3509 - mse: 87.3510 - mae: 6.4918 - val_loss: 82.7148 - val_mse: 82.7148 - val_mae: 6.5221\n",
      "Epoch 88/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.2242 - mse: 87.2242 - mae: 6.4883 - val_loss: 82.4726 - val_mse: 82.4726 - val_mae: 6.4484\n",
      "Epoch 89/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.1020 - mse: 87.1020 - mae: 6.4788 - val_loss: 82.8232 - val_mse: 82.8232 - val_mae: 6.5122\n",
      "Epoch 90/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.2011 - mse: 87.2011 - mae: 6.4827 - val_loss: 82.9263 - val_mse: 82.9263 - val_mae: 6.4792\n",
      "Epoch 91/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.1603 - mse: 87.1602 - mae: 6.4838 - val_loss: 82.9391 - val_mse: 82.9391 - val_mae: 6.4591\n",
      "Epoch 92/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.1599 - mse: 87.1600 - mae: 6.4767 - val_loss: 82.6367 - val_mse: 82.6367 - val_mae: 6.5244\n",
      "Epoch 93/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.2149 - mse: 87.2149 - mae: 6.4865 - val_loss: 82.4551 - val_mse: 82.4551 - val_mae: 6.4866\n",
      "Epoch 94/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 87.2681 - mse: 87.2682 - mae: 6.4868 - val_loss: 82.8813 - val_mse: 82.8813 - val_mae: 6.5934\n",
      "Epoch 95/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 87.1815 - mse: 87.1815 - mae: 6.4883 - val_loss: 82.4294 - val_mse: 82.4294 - val_mae: 6.5053\n",
      "Epoch 96/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 87.0840 - mse: 87.0840 - mae: 6.4832 - val_loss: 82.4998 - val_mse: 82.4998 - val_mae: 6.4946\n",
      "Epoch 97/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 87.0071 - mse: 87.0071 - mae: 6.4818 - val_loss: 82.9042 - val_mse: 82.9042 - val_mae: 6.4617\n",
      "Epoch 98/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.0804 - mse: 87.0804 - mae: 6.4810 - val_loss: 82.4920 - val_mse: 82.4920 - val_mae: 6.5186\n",
      "Epoch 99/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.1131 - mse: 87.1130 - mae: 6.4803 - val_loss: 83.1056 - val_mse: 83.1056 - val_mae: 6.3998\n",
      "Epoch 100/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 86.9747 - mse: 86.9747 - mae: 6.4822 - val_loss: 82.9495 - val_mse: 82.9495 - val_mae: 6.4116\n",
      "Epoch 101/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 87.1013 - mse: 87.1013 - mae: 6.4738 - val_loss: 82.4066 - val_mse: 82.4066 - val_mae: 6.4707\n",
      "Epoch 102/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 87.0087 - mse: 87.0087 - mae: 6.4728 - val_loss: 82.7258 - val_mse: 82.7259 - val_mae: 6.5013\n",
      "Epoch 103/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.9323 - mse: 86.9323 - mae: 6.4762 - val_loss: 83.4870 - val_mse: 83.4870 - val_mae: 6.4269\n",
      "Epoch 104/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 87.0059 - mse: 87.0059 - mae: 6.4764 - val_loss: 82.9014 - val_mse: 82.9014 - val_mae: 6.5664\n",
      "Epoch 105/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 86.9122 - mse: 86.9122 - mae: 6.4728 - val_loss: 82.7532 - val_mse: 82.7533 - val_mae: 6.4220\n",
      "Epoch 106/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 86.9405 - mse: 86.9406 - mae: 6.4678 - val_loss: 82.2889 - val_mse: 82.2889 - val_mae: 6.4834\n",
      "Epoch 107/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 86.9142 - mse: 86.9142 - mae: 6.4726 - val_loss: 82.1887 - val_mse: 82.1888 - val_mae: 6.5005\n",
      "Epoch 108/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 87.0087 - mse: 87.0088 - mae: 6.4795 - val_loss: 82.2147 - val_mse: 82.2147 - val_mae: 6.4718\n",
      "Epoch 109/1000\n",
      "14194/14194 [==============================] - 2s 150us/step - loss: 86.9640 - mse: 86.9640 - mae: 6.4725 - val_loss: 82.2097 - val_mse: 82.2097 - val_mae: 6.5196\n",
      "Epoch 110/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 86.8191 - mse: 86.8192 - mae: 6.4721 - val_loss: 82.4829 - val_mse: 82.4829 - val_mae: 6.5572\n",
      "Epoch 111/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 86.7931 - mse: 86.7931 - mae: 6.4741 - val_loss: 82.3606 - val_mse: 82.3606 - val_mae: 6.4919\n",
      "Epoch 112/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 86.7257 - mse: 86.7256 - mae: 6.4620 - val_loss: 83.6416 - val_mse: 83.6416 - val_mae: 6.6578\n",
      "Epoch 113/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 86.9135 - mse: 86.9135 - mae: 6.4794 - val_loss: 82.4448 - val_mse: 82.4448 - val_mae: 6.4970\n",
      "Epoch 114/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 86.8254 - mse: 86.8254 - mae: 6.4731 - val_loss: 82.5742 - val_mse: 82.5741 - val_mae: 6.3847\n",
      "Epoch 115/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 86.8522 - mse: 86.8522 - mae: 6.4596 - val_loss: 82.7887 - val_mse: 82.7886 - val_mae: 6.5069\n",
      "Epoch 116/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 86.7952 - mse: 86.7952 - mae: 6.4601 - val_loss: 82.0626 - val_mse: 82.0626 - val_mae: 6.4876\n",
      "Epoch 117/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 86.7801 - mse: 86.7801 - mae: 6.4710 - val_loss: 82.4045 - val_mse: 82.4045 - val_mae: 6.5226\n",
      "Epoch 118/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.7360 - mse: 86.7360 - mae: 6.4559 - val_loss: 82.2914 - val_mse: 82.2914 - val_mae: 6.5307\n",
      "Epoch 119/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.6796 - mse: 86.6796 - mae: 6.4641 - val_loss: 82.0351 - val_mse: 82.0351 - val_mae: 6.4874\n",
      "Epoch 120/1000\n",
      "14194/14194 [==============================] - 3s 182us/step - loss: 86.7693 - mse: 86.7693 - mae: 6.4662 - val_loss: 82.2480 - val_mse: 82.2480 - val_mae: 6.4863\n",
      "Epoch 121/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 86.8061 - mse: 86.8062 - mae: 6.4608 - val_loss: 82.2676 - val_mse: 82.2676 - val_mae: 6.4975\n",
      "Epoch 122/1000\n",
      "14194/14194 [==============================] - 2s 176us/step - loss: 86.6864 - mse: 86.6864 - mae: 6.4606 - val_loss: 82.2059 - val_mse: 82.2059 - val_mae: 6.5298\n",
      "Epoch 123/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.7591 - mse: 86.7591 - mae: 6.4641 - val_loss: 82.1070 - val_mse: 82.1070 - val_mae: 6.4936\n",
      "Epoch 124/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.6741 - mse: 86.6741 - mae: 6.4646 - val_loss: 82.3828 - val_mse: 82.3828 - val_mae: 6.4554\n",
      "Epoch 125/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.6364 - mse: 86.6364 - mae: 6.4605 - val_loss: 82.7758 - val_mse: 82.7758 - val_mae: 6.5960\n",
      "Epoch 126/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.7052 - mse: 86.7052 - mae: 6.4653 - val_loss: 82.7773 - val_mse: 82.7773 - val_mae: 6.3878\n",
      "Epoch 127/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.6548 - mse: 86.6548 - mae: 6.4541 - val_loss: 82.1243 - val_mse: 82.1243 - val_mae: 6.4480\n",
      "Epoch 128/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.6757 - mse: 86.6757 - mae: 6.4579 - val_loss: 82.0806 - val_mse: 82.0806 - val_mae: 6.4441\n",
      "Epoch 129/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.6035 - mse: 86.6036 - mae: 6.4612 - val_loss: 82.8098 - val_mse: 82.8098 - val_mae: 6.3801\n",
      "Epoch 130/1000\n",
      "14194/14194 [==============================] - 3s 182us/step - loss: 86.6547 - mse: 86.6547 - mae: 6.4627 - val_loss: 82.9654 - val_mse: 82.9655 - val_mae: 6.3963\n",
      "Epoch 131/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.6280 - mse: 86.6280 - mae: 6.4554 - val_loss: 82.2927 - val_mse: 82.2927 - val_mae: 6.4786\n",
      "Epoch 132/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.5764 - mse: 86.5765 - mae: 6.4569 - val_loss: 82.9244 - val_mse: 82.9244 - val_mae: 6.5902\n",
      "Epoch 133/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.5408 - mse: 86.5407 - mae: 6.4488 - val_loss: 82.3624 - val_mse: 82.3624 - val_mae: 6.4556\n",
      "Epoch 134/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 86.7459 - mse: 86.7459 - mae: 6.4609 - val_loss: 82.0321 - val_mse: 82.0321 - val_mae: 6.4512\n",
      "Epoch 135/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.5762 - mse: 86.5762 - mae: 6.4502 - val_loss: 82.3101 - val_mse: 82.3101 - val_mae: 6.5450\n",
      "Epoch 136/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.5980 - mse: 86.5980 - mae: 6.4593 - val_loss: 82.1879 - val_mse: 82.1879 - val_mae: 6.5159\n",
      "Epoch 137/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.5633 - mse: 86.5633 - mae: 6.4581 - val_loss: 82.6817 - val_mse: 82.6817 - val_mae: 6.4319\n",
      "Epoch 138/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 86.5727 - mse: 86.5727 - mae: 6.4571 - val_loss: 82.3010 - val_mse: 82.3010 - val_mae: 6.4267\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 3s 178us/step - loss: 86.5947 - mse: 86.5947 - mae: 6.4542 - val_loss: 82.0992 - val_mse: 82.0992 - val_mae: 6.5068\n",
      "Epoch 140/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 86.5894 - mse: 86.5894 - mae: 6.4576 - val_loss: 82.1853 - val_mse: 82.1853 - val_mae: 6.4977\n",
      "Epoch 141/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.5813 - mse: 86.5813 - mae: 6.4537 - val_loss: 82.3440 - val_mse: 82.3440 - val_mae: 6.4038\n",
      "Epoch 142/1000\n",
      "14194/14194 [==============================] - 3s 178us/step - loss: 86.5201 - mse: 86.5201 - mae: 6.4487 - val_loss: 83.6968 - val_mse: 83.6968 - val_mae: 6.6702\n",
      "Epoch 143/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.5192 - mse: 86.5191 - mae: 6.4490 - val_loss: 82.6009 - val_mse: 82.6009 - val_mae: 6.6185\n",
      "Epoch 144/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.4520 - mse: 86.4520 - mae: 6.4533 - val_loss: 82.0119 - val_mse: 82.0119 - val_mae: 6.4303\n",
      "Epoch 145/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.5118 - mse: 86.5118 - mae: 6.4507 - val_loss: 82.3150 - val_mse: 82.3150 - val_mae: 6.4480\n",
      "Epoch 146/1000\n",
      "14194/14194 [==============================] - 3s 181us/step - loss: 86.5053 - mse: 86.5053 - mae: 6.4447 - val_loss: 81.9571 - val_mse: 81.9571 - val_mae: 6.4814\n",
      "Epoch 147/1000\n",
      "14194/14194 [==============================] - 3s 204us/step - loss: 86.5280 - mse: 86.5280 - mae: 6.4476 - val_loss: 82.1362 - val_mse: 82.1362 - val_mae: 6.4518\n",
      "Epoch 148/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.5158 - mse: 86.5158 - mae: 6.4599 - val_loss: 82.2577 - val_mse: 82.2577 - val_mae: 6.5125\n",
      "Epoch 149/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.6191 - mse: 86.6191 - mae: 6.4505 - val_loss: 81.9639 - val_mse: 81.9639 - val_mae: 6.4534\n",
      "Epoch 150/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 86.4649 - mse: 86.4648 - mae: 6.4577 - val_loss: 82.0259 - val_mse: 82.0259 - val_mae: 6.4332\n",
      "Epoch 151/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 86.5662 - mse: 86.5662 - mae: 6.4530 - val_loss: 82.1652 - val_mse: 82.1652 - val_mae: 6.4609\n",
      "Epoch 152/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 86.4399 - mse: 86.4399 - mae: 6.4587 - val_loss: 82.4112 - val_mse: 82.4112 - val_mae: 6.4140\n",
      "Epoch 153/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 86.6010 - mse: 86.6010 - mae: 6.4549 - val_loss: 82.1349 - val_mse: 82.1349 - val_mae: 6.4690\n",
      "Epoch 154/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 86.4556 - mse: 86.4556 - mae: 6.4501 - val_loss: 83.3591 - val_mse: 83.3591 - val_mae: 6.6411\n",
      "Epoch 155/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.4360 - mse: 86.4361 - mae: 6.4527 - val_loss: 82.5206 - val_mse: 82.5206 - val_mae: 6.5042\n",
      "Epoch 156/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 86.5042 - mse: 86.5042 - mae: 6.4448 - val_loss: 83.1242 - val_mse: 83.1242 - val_mae: 6.6453\n",
      "Epoch 157/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 86.4566 - mse: 86.4565 - mae: 6.4564 - val_loss: 82.0756 - val_mse: 82.0756 - val_mae: 6.4565\n",
      "Epoch 158/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.4746 - mse: 86.4746 - mae: 6.4486 - val_loss: 81.9187 - val_mse: 81.9187 - val_mae: 6.4663\n",
      "Epoch 159/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 86.4946 - mse: 86.4947 - mae: 6.4464 - val_loss: 82.2169 - val_mse: 82.2170 - val_mae: 6.5352\n",
      "Epoch 160/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 86.4365 - mse: 86.4366 - mae: 6.4523 - val_loss: 82.8984 - val_mse: 82.8984 - val_mae: 6.6193\n",
      "Epoch 161/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 86.5181 - mse: 86.5181 - mae: 6.4526 - val_loss: 83.1638 - val_mse: 83.1638 - val_mae: 6.5629\n",
      "Epoch 162/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 86.4704 - mse: 86.4703 - mae: 6.4456 - val_loss: 82.0522 - val_mse: 82.0522 - val_mae: 6.4540\n",
      "Epoch 163/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 86.4048 - mse: 86.4048 - mae: 6.4449 - val_loss: 82.1717 - val_mse: 82.1717 - val_mae: 6.4869\n",
      "Epoch 164/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 86.4533 - mse: 86.4533 - mae: 6.4560 - val_loss: 81.9427 - val_mse: 81.9427 - val_mae: 6.4707\n",
      "Epoch 165/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 86.4017 - mse: 86.4016 - mae: 6.4456 - val_loss: 81.9942 - val_mse: 81.9942 - val_mae: 6.4616\n",
      "Epoch 166/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.5058 - mse: 86.5059 - mae: 6.4523 - val_loss: 81.9964 - val_mse: 81.9964 - val_mae: 6.4350\n",
      "Epoch 167/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.4760 - mse: 86.4760 - mae: 6.4545 - val_loss: 81.9357 - val_mse: 81.9358 - val_mae: 6.4315\n",
      "Epoch 168/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 86.4838 - mse: 86.4838 - mae: 6.4481 - val_loss: 82.8426 - val_mse: 82.8427 - val_mae: 6.6158\n",
      "Epoch 169/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.4818 - mse: 86.4818 - mae: 6.4520 - val_loss: 82.2074 - val_mse: 82.2074 - val_mae: 6.5297\n",
      "Epoch 170/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 86.4153 - mse: 86.4154 - mae: 6.4408 - val_loss: 82.0874 - val_mse: 82.0874 - val_mae: 6.5105\n",
      "Epoch 171/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 86.4542 - mse: 86.4542 - mae: 6.4484 - val_loss: 81.9952 - val_mse: 81.9951 - val_mae: 6.4515\n",
      "Epoch 172/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.2512 - mse: 86.2513 - mae: 6.4459 - val_loss: 81.9732 - val_mse: 81.9732 - val_mae: 6.4462\n",
      "Epoch 173/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 86.4907 - mse: 86.4906 - mae: 6.4495 - val_loss: 81.8615 - val_mse: 81.8614 - val_mae: 6.4549\n",
      "Epoch 174/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 86.4199 - mse: 86.4199 - mae: 6.4461 - val_loss: 82.4887 - val_mse: 82.4887 - val_mae: 6.5862\n",
      "Epoch 175/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.3978 - mse: 86.3979 - mae: 6.4489 - val_loss: 82.4971 - val_mse: 82.4971 - val_mae: 6.5487\n",
      "Epoch 176/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.4593 - mse: 86.4593 - mae: 6.4475 - val_loss: 81.9109 - val_mse: 81.9109 - val_mae: 6.4729\n",
      "Epoch 177/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 86.3550 - mse: 86.3549 - mae: 6.4539 - val_loss: 82.3152 - val_mse: 82.3152 - val_mae: 6.5241\n",
      "Epoch 178/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.3477 - mse: 86.3477 - mae: 6.4478 - val_loss: 82.1173 - val_mse: 82.1173 - val_mae: 6.4915\n",
      "Epoch 179/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 86.4389 - mse: 86.4389 - mae: 6.4444 - val_loss: 82.2077 - val_mse: 82.2077 - val_mae: 6.4921\n",
      "Epoch 180/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 86.2738 - mse: 86.2737 - mae: 6.4494 - val_loss: 81.9118 - val_mse: 81.9118 - val_mae: 6.4425\n",
      "Epoch 181/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 86.3540 - mse: 86.3540 - mae: 6.4486 - val_loss: 81.9475 - val_mse: 81.9475 - val_mae: 6.4374\n",
      "Epoch 182/1000\n",
      "14194/14194 [==============================] - 2s 150us/step - loss: 86.3653 - mse: 86.3653 - mae: 6.4509 - val_loss: 82.6950 - val_mse: 82.6950 - val_mae: 6.3952\n",
      "Epoch 183/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 86.4277 - mse: 86.4276 - mae: 6.4443 - val_loss: 82.0731 - val_mse: 82.0731 - val_mae: 6.4021\n",
      "Epoch 184/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 86.4027 - mse: 86.4026 - mae: 6.4393 - val_loss: 81.9757 - val_mse: 81.9757 - val_mae: 6.4393\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.2854 - mse: 86.2855 - mae: 6.4466 - val_loss: 81.8834 - val_mse: 81.8834 - val_mae: 6.4874\n",
      "Epoch 186/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 86.3091 - mse: 86.3091 - mae: 6.4439 - val_loss: 81.8986 - val_mse: 81.8986 - val_mae: 6.4552\n",
      "Epoch 187/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.3255 - mse: 86.3256 - mae: 6.4459 - val_loss: 81.7394 - val_mse: 81.7394 - val_mae: 6.4414\n",
      "Epoch 188/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.2921 - mse: 86.2921 - mae: 6.4421 - val_loss: 82.1534 - val_mse: 82.1534 - val_mae: 6.4288\n",
      "Epoch 189/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.3570 - mse: 86.3571 - mae: 6.4461 - val_loss: 82.6101 - val_mse: 82.6101 - val_mae: 6.4420\n",
      "Epoch 190/1000\n",
      "14194/14194 [==============================] - 3s 178us/step - loss: 86.2874 - mse: 86.2874 - mae: 6.4358 - val_loss: 82.1165 - val_mse: 82.1164 - val_mae: 6.4669\n",
      "Epoch 191/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.3412 - mse: 86.3412 - mae: 6.4454 - val_loss: 81.8793 - val_mse: 81.8793 - val_mae: 6.4619\n",
      "Epoch 192/1000\n",
      "14194/14194 [==============================] - 3s 186us/step - loss: 86.2907 - mse: 86.2908 - mae: 6.4399 - val_loss: 82.9800 - val_mse: 82.9800 - val_mae: 6.5793\n",
      "Epoch 193/1000\n",
      "14194/14194 [==============================] - 3s 180us/step - loss: 86.1855 - mse: 86.1855 - mae: 6.4446 - val_loss: 82.6788 - val_mse: 82.6788 - val_mae: 6.5838\n",
      "Epoch 194/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.3557 - mse: 86.3557 - mae: 6.4484 - val_loss: 81.8499 - val_mse: 81.8500 - val_mae: 6.4828\n",
      "Epoch 195/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2832 - mse: 86.2832 - mae: 6.4457 - val_loss: 81.9891 - val_mse: 81.9891 - val_mae: 6.4661\n",
      "Epoch 196/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.3213 - mse: 86.3214 - mae: 6.4459 - val_loss: 82.0755 - val_mse: 82.0754 - val_mae: 6.5442\n",
      "Epoch 197/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.3569 - mse: 86.3568 - mae: 6.4432 - val_loss: 82.3630 - val_mse: 82.3630 - val_mae: 6.5042\n",
      "Epoch 198/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.3608 - mse: 86.3607 - mae: 6.4410 - val_loss: 81.9421 - val_mse: 81.9421 - val_mae: 6.4446\n",
      "Epoch 199/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.3879 - mse: 86.3879 - mae: 6.4438 - val_loss: 81.7612 - val_mse: 81.7613 - val_mae: 6.4473\n",
      "Epoch 200/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2492 - mse: 86.2493 - mae: 6.4360 - val_loss: 82.0355 - val_mse: 82.0355 - val_mae: 6.4754\n",
      "Epoch 201/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.3521 - mse: 86.3521 - mae: 6.4441 - val_loss: 81.7643 - val_mse: 81.7643 - val_mae: 6.5030\n",
      "Epoch 202/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.2511 - mse: 86.2511 - mae: 6.4439 - val_loss: 81.8112 - val_mse: 81.8112 - val_mae: 6.5053\n",
      "Epoch 203/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.2672 - mse: 86.2671 - mae: 6.4488 - val_loss: 83.1345 - val_mse: 83.1345 - val_mae: 6.6616\n",
      "Epoch 204/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.3109 - mse: 86.3109 - mae: 6.4405 - val_loss: 81.8053 - val_mse: 81.8053 - val_mae: 6.4540\n",
      "Epoch 205/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.3971 - mse: 86.3971 - mae: 6.4430 - val_loss: 81.9539 - val_mse: 81.9539 - val_mae: 6.4448\n",
      "Epoch 206/1000\n",
      "14194/14194 [==============================] - 3s 178us/step - loss: 86.2926 - mse: 86.2926 - mae: 6.4456 - val_loss: 81.8440 - val_mse: 81.8440 - val_mae: 6.4584\n",
      "Epoch 207/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2427 - mse: 86.2428 - mae: 6.4496 - val_loss: 81.8164 - val_mse: 81.8164 - val_mae: 6.4673\n",
      "Epoch 208/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.2014 - mse: 86.2014 - mae: 6.4365 - val_loss: 82.0029 - val_mse: 82.0029 - val_mae: 6.4946\n",
      "Epoch 209/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.2232 - mse: 86.2232 - mae: 6.4460 - val_loss: 81.9320 - val_mse: 81.9320 - val_mae: 6.5280\n",
      "Epoch 210/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 86.1922 - mse: 86.1922 - mae: 6.4473 - val_loss: 82.1542 - val_mse: 82.1542 - val_mae: 6.4048\n",
      "Epoch 211/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.1722 - mse: 86.1722 - mae: 6.4388 - val_loss: 82.3345 - val_mse: 82.3344 - val_mae: 6.5817\n",
      "Epoch 212/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.3223 - mse: 86.3223 - mae: 6.4499 - val_loss: 82.0072 - val_mse: 82.0072 - val_mae: 6.4203\n",
      "Epoch 213/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.3009 - mse: 86.3008 - mae: 6.4365 - val_loss: 81.7861 - val_mse: 81.7861 - val_mae: 6.4712\n",
      "Epoch 214/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2511 - mse: 86.2511 - mae: 6.4424 - val_loss: 81.8571 - val_mse: 81.8571 - val_mae: 6.5492\n",
      "Epoch 215/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.2711 - mse: 86.2711 - mae: 6.4398 - val_loss: 82.6393 - val_mse: 82.6393 - val_mae: 6.6097\n",
      "Epoch 216/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.1635 - mse: 86.1634 - mae: 6.4457 - val_loss: 81.8587 - val_mse: 81.8587 - val_mae: 6.4519\n",
      "Epoch 217/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1361 - mse: 86.1361 - mae: 6.4343 - val_loss: 81.7789 - val_mse: 81.7789 - val_mae: 6.5184\n",
      "Epoch 218/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2448 - mse: 86.2448 - mae: 6.4480 - val_loss: 82.2412 - val_mse: 82.2412 - val_mae: 6.4515\n",
      "Epoch 219/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.2254 - mse: 86.2254 - mae: 6.4385 - val_loss: 82.2238 - val_mse: 82.2238 - val_mae: 6.4840\n",
      "Epoch 220/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.2623 - mse: 86.2623 - mae: 6.4377 - val_loss: 81.9089 - val_mse: 81.9090 - val_mae: 6.4493\n",
      "Epoch 221/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 86.2310 - mse: 86.2309 - mae: 6.4376 - val_loss: 82.4828 - val_mse: 82.4828 - val_mae: 6.4025\n",
      "Epoch 222/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.2636 - mse: 86.2636 - mae: 6.4388 - val_loss: 82.0719 - val_mse: 82.0719 - val_mae: 6.5043\n",
      "Epoch 223/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.1619 - mse: 86.1619 - mae: 6.4408 - val_loss: 81.6752 - val_mse: 81.6753 - val_mae: 6.4787\n",
      "Epoch 224/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.1570 - mse: 86.1570 - mae: 6.4391 - val_loss: 81.7331 - val_mse: 81.7330 - val_mae: 6.4339\n",
      "Epoch 225/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1396 - mse: 86.1396 - mae: 6.4395 - val_loss: 81.8790 - val_mse: 81.8790 - val_mae: 6.4557\n",
      "Epoch 226/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.2964 - mse: 86.2964 - mae: 6.4412 - val_loss: 81.7963 - val_mse: 81.7963 - val_mae: 6.4858\n",
      "Epoch 227/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 86.2503 - mse: 86.2503 - mae: 6.4400 - val_loss: 83.3272 - val_mse: 83.3271 - val_mae: 6.7039\n",
      "Epoch 228/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.1764 - mse: 86.1764 - mae: 6.4540 - val_loss: 81.7977 - val_mse: 81.7977 - val_mae: 6.4350\n",
      "Epoch 229/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.1642 - mse: 86.1642 - mae: 6.4356 - val_loss: 81.9614 - val_mse: 81.9614 - val_mae: 6.4832\n",
      "Epoch 230/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.1743 - mse: 86.1743 - mae: 6.4324 - val_loss: 82.2574 - val_mse: 82.2575 - val_mae: 6.5374\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 169us/step - loss: 86.1207 - mse: 86.1208 - mae: 6.4299 - val_loss: 81.6916 - val_mse: 81.6916 - val_mae: 6.4580\n",
      "Epoch 232/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.1394 - mse: 86.1395 - mae: 6.4444 - val_loss: 82.2281 - val_mse: 82.2281 - val_mae: 6.4149\n",
      "Epoch 233/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.0961 - mse: 86.0961 - mae: 6.4335 - val_loss: 82.1767 - val_mse: 82.1767 - val_mae: 6.5685\n",
      "Epoch 234/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.1714 - mse: 86.1714 - mae: 6.4488 - val_loss: 81.7746 - val_mse: 81.7746 - val_mae: 6.4473\n",
      "Epoch 235/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 86.1446 - mse: 86.1446 - mae: 6.4292 - val_loss: 82.3341 - val_mse: 82.3341 - val_mae: 6.5750\n",
      "Epoch 236/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 86.2003 - mse: 86.2002 - mae: 6.4455 - val_loss: 81.8963 - val_mse: 81.8963 - val_mae: 6.4145\n",
      "Epoch 237/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 86.1614 - mse: 86.1615 - mae: 6.4363 - val_loss: 81.7241 - val_mse: 81.7242 - val_mae: 6.4658\n",
      "Epoch 238/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 86.2296 - mse: 86.2296 - mae: 6.4464 - val_loss: 81.9289 - val_mse: 81.9289 - val_mae: 6.4812\n",
      "Epoch 239/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 86.1717 - mse: 86.1717 - mae: 6.4386 - val_loss: 81.8229 - val_mse: 81.8229 - val_mae: 6.4380\n",
      "Epoch 240/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 86.1327 - mse: 86.1327 - mae: 6.4411 - val_loss: 82.3904 - val_mse: 82.3904 - val_mae: 6.5588\n",
      "Epoch 241/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.1456 - mse: 86.1456 - mae: 6.4360 - val_loss: 81.9867 - val_mse: 81.9867 - val_mae: 6.4434\n",
      "Epoch 242/1000\n",
      "14194/14194 [==============================] - 2s 154us/step - loss: 86.0723 - mse: 86.0723 - mae: 6.4367 - val_loss: 82.3893 - val_mse: 82.3893 - val_mae: 6.5540\n",
      "Epoch 243/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 86.2089 - mse: 86.2089 - mae: 6.4387 - val_loss: 81.8555 - val_mse: 81.8555 - val_mae: 6.5053\n",
      "Epoch 244/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 86.1331 - mse: 86.1332 - mae: 6.4404 - val_loss: 81.9947 - val_mse: 81.9947 - val_mae: 6.4129\n",
      "Epoch 245/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.2423 - mse: 86.2423 - mae: 6.4367 - val_loss: 81.8208 - val_mse: 81.8208 - val_mae: 6.4535\n",
      "Epoch 246/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.0815 - mse: 86.0815 - mae: 6.4421 - val_loss: 81.9680 - val_mse: 81.9680 - val_mae: 6.4445\n",
      "Epoch 247/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.0171 - mse: 86.0170 - mae: 6.4398 - val_loss: 82.9308 - val_mse: 82.9308 - val_mae: 6.4092\n",
      "Epoch 248/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.9893 - mse: 85.9893 - mae: 6.4343 - val_loss: 82.1965 - val_mse: 82.1965 - val_mae: 6.4853\n",
      "Epoch 249/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 86.0890 - mse: 86.0890 - mae: 6.4383 - val_loss: 82.3102 - val_mse: 82.3102 - val_mae: 6.3857\n",
      "Epoch 250/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 86.0727 - mse: 86.0728 - mae: 6.4371 - val_loss: 81.8497 - val_mse: 81.8496 - val_mae: 6.4454\n",
      "Epoch 251/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 86.0882 - mse: 86.0882 - mae: 6.4328 - val_loss: 82.0168 - val_mse: 82.0168 - val_mae: 6.5705\n",
      "Epoch 252/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 86.1839 - mse: 86.1838 - mae: 6.4381 - val_loss: 82.0022 - val_mse: 82.0021 - val_mae: 6.5152\n",
      "Epoch 253/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 86.0683 - mse: 86.0683 - mae: 6.4380 - val_loss: 82.3264 - val_mse: 82.3264 - val_mae: 6.5728\n",
      "Epoch 254/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 86.0283 - mse: 86.0284 - mae: 6.4381 - val_loss: 82.4882 - val_mse: 82.4882 - val_mae: 6.4151\n",
      "Epoch 255/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 86.2302 - mse: 86.2303 - mae: 6.4432 - val_loss: 81.7078 - val_mse: 81.7078 - val_mae: 6.4634\n",
      "Epoch 256/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 86.1749 - mse: 86.1749 - mae: 6.4434 - val_loss: 82.3785 - val_mse: 82.3785 - val_mae: 6.4964\n",
      "Epoch 257/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 86.0908 - mse: 86.0908 - mae: 6.4381 - val_loss: 81.8494 - val_mse: 81.8494 - val_mae: 6.4576\n",
      "Epoch 258/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 86.1548 - mse: 86.1548 - mae: 6.4383 - val_loss: 81.9126 - val_mse: 81.9126 - val_mae: 6.4373\n",
      "Epoch 259/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 86.1026 - mse: 86.1026 - mae: 6.4458 - val_loss: 82.1511 - val_mse: 82.1511 - val_mae: 6.4018\n",
      "Epoch 260/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 86.1978 - mse: 86.1978 - mae: 6.4336 - val_loss: 81.7943 - val_mse: 81.7943 - val_mae: 6.4884\n",
      "Epoch 261/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.0104 - mse: 86.0105 - mae: 6.4296 - val_loss: 83.2417 - val_mse: 83.2417 - val_mae: 6.7119\n",
      "Epoch 262/1000\n",
      "14194/14194 [==============================] - 3s 187us/step - loss: 86.0875 - mse: 86.0875 - mae: 6.4331 - val_loss: 81.8542 - val_mse: 81.8542 - val_mae: 6.5183\n",
      "Epoch 263/1000\n",
      "14194/14194 [==============================] - 2s 176us/step - loss: 86.1428 - mse: 86.1428 - mae: 6.4349 - val_loss: 82.0286 - val_mse: 82.0287 - val_mae: 6.5198\n",
      "Epoch 264/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 85.9962 - mse: 85.9962 - mae: 6.4234 - val_loss: 81.9188 - val_mse: 81.9188 - val_mae: 6.5229\n",
      "Epoch 265/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.1466 - mse: 86.1465 - mae: 6.4403 - val_loss: 81.6692 - val_mse: 81.6692 - val_mae: 6.4785\n",
      "Epoch 266/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 86.0503 - mse: 86.0504 - mae: 6.4308 - val_loss: 82.1466 - val_mse: 82.1466 - val_mae: 6.5536\n",
      "Epoch 267/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 85.9558 - mse: 85.9558 - mae: 6.4397 - val_loss: 81.6562 - val_mse: 81.6562 - val_mae: 6.4688\n",
      "Epoch 268/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 86.1731 - mse: 86.1732 - mae: 6.4270 - val_loss: 82.0688 - val_mse: 82.0687 - val_mae: 6.5600\n",
      "Epoch 269/1000\n",
      "14194/14194 [==============================] - 2s 176us/step - loss: 86.0745 - mse: 86.0745 - mae: 6.4352 - val_loss: 81.8025 - val_mse: 81.8025 - val_mae: 6.4649\n",
      "Epoch 270/1000\n",
      "14194/14194 [==============================] - 3s 178us/step - loss: 86.0582 - mse: 86.0582 - mae: 6.4374 - val_loss: 82.0028 - val_mse: 82.0028 - val_mae: 6.3926\n",
      "Epoch 271/1000\n",
      "14194/14194 [==============================] - 2s 174us/step - loss: 86.0650 - mse: 86.0649 - mae: 6.4282 - val_loss: 82.0272 - val_mse: 82.0272 - val_mae: 6.4707\n",
      "Epoch 272/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1591 - mse: 86.1591 - mae: 6.4359 - val_loss: 81.7520 - val_mse: 81.7520 - val_mae: 6.4689\n",
      "Epoch 273/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 85.9035 - mse: 85.9035 - mae: 6.4304 - val_loss: 81.6554 - val_mse: 81.6554 - val_mae: 6.5049\n",
      "Epoch 274/1000\n",
      "14194/14194 [==============================] - 2s 171us/step - loss: 86.0485 - mse: 86.0484 - mae: 6.4443 - val_loss: 82.2340 - val_mse: 82.2340 - val_mae: 6.4095\n",
      "Epoch 275/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.0292 - mse: 86.0292 - mae: 6.4306 - val_loss: 82.0514 - val_mse: 82.0514 - val_mae: 6.4659\n",
      "Epoch 276/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 86.1122 - mse: 86.1121 - mae: 6.4336 - val_loss: 81.6695 - val_mse: 81.6695 - val_mae: 6.4458\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1380 - mse: 86.1380 - mae: 6.4361 - val_loss: 81.9160 - val_mse: 81.9160 - val_mae: 6.4532\n",
      "Epoch 278/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 86.1258 - mse: 86.1258 - mae: 6.4425 - val_loss: 82.5046 - val_mse: 82.5046 - val_mae: 6.5847\n",
      "Epoch 279/1000\n",
      "14194/14194 [==============================] - 3s 188us/step - loss: 86.1646 - mse: 86.1646 - mae: 6.4416 - val_loss: 81.7724 - val_mse: 81.7724 - val_mae: 6.4072\n",
      "Epoch 280/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1568 - mse: 86.1568 - mae: 6.4314 - val_loss: 81.7381 - val_mse: 81.7381 - val_mae: 6.4354\n",
      "Epoch 281/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 86.1215 - mse: 86.1215 - mae: 6.4391 - val_loss: 81.6999 - val_mse: 81.6999 - val_mae: 6.4499\n",
      "Epoch 282/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 85.9621 - mse: 85.9621 - mae: 6.4286 - val_loss: 82.0978 - val_mse: 82.0978 - val_mae: 6.5000\n",
      "Epoch 283/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.0733 - mse: 86.0733 - mae: 6.4303 - val_loss: 81.7464 - val_mse: 81.7464 - val_mae: 6.4676\n",
      "Epoch 284/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.0043 - mse: 86.0043 - mae: 6.4326 - val_loss: 81.8576 - val_mse: 81.8576 - val_mae: 6.4731\n",
      "Epoch 285/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.0483 - mse: 86.0482 - mae: 6.4327 - val_loss: 81.7714 - val_mse: 81.7714 - val_mae: 6.4528\n",
      "Epoch 286/1000\n",
      "14194/14194 [==============================] - 3s 183us/step - loss: 85.9139 - mse: 85.9140 - mae: 6.4311 - val_loss: 81.9275 - val_mse: 81.9275 - val_mae: 6.4743\n",
      "Epoch 287/1000\n",
      "14194/14194 [==============================] - 2s 176us/step - loss: 86.0151 - mse: 86.0151 - mae: 6.4358 - val_loss: 81.6151 - val_mse: 81.6151 - val_mae: 6.4849\n",
      "Epoch 288/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 85.9698 - mse: 85.9697 - mae: 6.4321 - val_loss: 81.6753 - val_mse: 81.6753 - val_mae: 6.4797\n",
      "Epoch 289/1000\n",
      "14194/14194 [==============================] - 2s 175us/step - loss: 86.0184 - mse: 86.0183 - mae: 6.4294 - val_loss: 82.6430 - val_mse: 82.6430 - val_mae: 6.6137\n",
      "Epoch 290/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.1176 - mse: 86.1177 - mae: 6.4422 - val_loss: 81.7166 - val_mse: 81.7166 - val_mae: 6.4829\n",
      "Epoch 291/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 86.0305 - mse: 86.0305 - mae: 6.4358 - val_loss: 81.6678 - val_mse: 81.6678 - val_mae: 6.4628\n",
      "Epoch 292/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 85.8812 - mse: 85.8812 - mae: 6.4366 - val_loss: 82.3458 - val_mse: 82.3458 - val_mae: 6.4266\n",
      "Epoch 293/1000\n",
      "14194/14194 [==============================] - 3s 183us/step - loss: 86.1949 - mse: 86.1950 - mae: 6.4352 - val_loss: 82.2652 - val_mse: 82.2652 - val_mae: 6.5775\n",
      "Epoch 294/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 85.9314 - mse: 85.9313 - mae: 6.4388 - val_loss: 82.3285 - val_mse: 82.3285 - val_mae: 6.3900\n",
      "Epoch 295/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 86.1833 - mse: 86.1833 - mae: 6.4315 - val_loss: 81.7511 - val_mse: 81.7511 - val_mae: 6.4849\n",
      "Epoch 296/1000\n",
      "14194/14194 [==============================] - 3s 191us/step - loss: 86.0607 - mse: 86.0608 - mae: 6.4345 - val_loss: 81.8930 - val_mse: 81.8930 - val_mae: 6.5195\n",
      "Epoch 297/1000\n",
      "14194/14194 [==============================] - 3s 188us/step - loss: 86.0732 - mse: 86.0731 - mae: 6.4357 - val_loss: 81.5475 - val_mse: 81.5474 - val_mae: 6.4875\n",
      "Epoch 298/1000\n",
      "14194/14194 [==============================] - 3s 206us/step - loss: 86.0045 - mse: 86.0044 - mae: 6.4370 - val_loss: 82.2362 - val_mse: 82.2362 - val_mae: 6.4783\n",
      "Epoch 299/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 85.9268 - mse: 85.9268 - mae: 6.4295 - val_loss: 81.5005 - val_mse: 81.5006 - val_mae: 6.4688\n",
      "Epoch 300/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 86.0388 - mse: 86.0388 - mae: 6.4326 - val_loss: 81.9580 - val_mse: 81.9580 - val_mae: 6.4017\n",
      "Epoch 301/1000\n",
      "14194/14194 [==============================] - 5s 350us/step - loss: 85.9439 - mse: 85.9438 - mae: 6.4353 - val_loss: 81.8029 - val_mse: 81.8029 - val_mae: 6.4831\n",
      "Epoch 302/1000\n",
      "14194/14194 [==============================] - 9s 601us/step - loss: 86.0154 - mse: 86.0155 - mae: 6.4354 - val_loss: 82.2139 - val_mse: 82.2139 - val_mae: 6.4656\n",
      "Epoch 303/1000\n",
      "14194/14194 [==============================] - 5s 363us/step - loss: 85.9888 - mse: 85.9887 - mae: 6.4347 - val_loss: 81.7936 - val_mse: 81.7936 - val_mae: 6.5029\n",
      "Epoch 304/1000\n",
      "14194/14194 [==============================] - 5s 317us/step - loss: 85.9764 - mse: 85.9764 - mae: 6.4365 - val_loss: 81.6782 - val_mse: 81.6781 - val_mae: 6.4228\n",
      "Epoch 305/1000\n",
      "14194/14194 [==============================] - 5s 323us/step - loss: 85.9974 - mse: 85.9975 - mae: 6.4380 - val_loss: 81.4724 - val_mse: 81.4724 - val_mae: 6.4474\n",
      "Epoch 306/1000\n",
      "14194/14194 [==============================] - 8097s 570ms/step - loss: 85.9408 - mse: 85.9408 - mae: 6.4287 - val_loss: 82.4136 - val_mse: 82.4137 - val_mae: 6.6098\n",
      "Epoch 307/1000\n",
      "14194/14194 [==============================] - 4s 272us/step - loss: 85.9621 - mse: 85.9622 - mae: 6.4416 - val_loss: 82.1111 - val_mse: 82.1111 - val_mae: 6.4024\n",
      "Epoch 308/1000\n",
      "14194/14194 [==============================] - 3s 236us/step - loss: 85.9643 - mse: 85.9644 - mae: 6.4287 - val_loss: 81.9658 - val_mse: 81.9658 - val_mae: 6.4125\n",
      "Epoch 309/1000\n",
      "14194/14194 [==============================] - 3s 209us/step - loss: 85.9580 - mse: 85.9579 - mae: 6.4329 - val_loss: 81.8417 - val_mse: 81.8417 - val_mae: 6.3985\n",
      "Epoch 310/1000\n",
      "14194/14194 [==============================] - 3s 223us/step - loss: 85.7046 - mse: 85.7047 - mae: 6.4219 - val_loss: 82.1518 - val_mse: 82.1518 - val_mae: 6.4094\n",
      "Epoch 311/1000\n",
      "14194/14194 [==============================] - 3s 205us/step - loss: 86.0240 - mse: 86.0241 - mae: 6.4345 - val_loss: 81.5525 - val_mse: 81.5525 - val_mae: 6.4686\n",
      "Epoch 312/1000\n",
      "14194/14194 [==============================] - 3s 186us/step - loss: 86.0398 - mse: 86.0398 - mae: 6.4306 - val_loss: 81.7783 - val_mse: 81.7783 - val_mae: 6.4979\n",
      "Epoch 313/1000\n",
      "14194/14194 [==============================] - 3s 205us/step - loss: 86.0075 - mse: 86.0075 - mae: 6.4348 - val_loss: 82.1651 - val_mse: 82.1651 - val_mae: 6.5861\n",
      "Epoch 314/1000\n",
      "14194/14194 [==============================] - 3s 184us/step - loss: 86.0117 - mse: 86.0117 - mae: 6.4421 - val_loss: 81.8952 - val_mse: 81.8952 - val_mae: 6.5487\n",
      "Epoch 315/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 86.0638 - mse: 86.0639 - mae: 6.4383 - val_loss: 81.6219 - val_mse: 81.6220 - val_mae: 6.4394\n",
      "Epoch 316/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.9609 - mse: 85.9609 - mae: 6.4271 - val_loss: 81.6073 - val_mse: 81.6073 - val_mae: 6.5084\n",
      "Epoch 317/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 85.9905 - mse: 85.9905 - mae: 6.4326 - val_loss: 81.7701 - val_mse: 81.7701 - val_mae: 6.4266\n",
      "Epoch 318/1000\n",
      "14194/14194 [==============================] - 2s 154us/step - loss: 86.0661 - mse: 86.0661 - mae: 6.4335 - val_loss: 81.5412 - val_mse: 81.5412 - val_mae: 6.4885\n",
      "Epoch 319/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 86.0177 - mse: 86.0176 - mae: 6.4296 - val_loss: 82.1665 - val_mse: 82.1665 - val_mae: 6.4623\n",
      "Epoch 320/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9378 - mse: 85.9378 - mae: 6.4387 - val_loss: 81.8841 - val_mse: 81.8841 - val_mae: 6.4843\n",
      "Epoch 321/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.9387 - mse: 85.9387 - mae: 6.4269 - val_loss: 82.3299 - val_mse: 82.3299 - val_mae: 6.5727\n",
      "Epoch 322/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 86.0277 - mse: 86.0276 - mae: 6.4374 - val_loss: 82.1273 - val_mse: 82.1273 - val_mae: 6.5332\n",
      "Epoch 323/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.9835 - mse: 85.9836 - mae: 6.4333 - val_loss: 81.5192 - val_mse: 81.5192 - val_mae: 6.4997\n",
      "Epoch 324/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9201 - mse: 85.9201 - mae: 6.4316 - val_loss: 81.3717 - val_mse: 81.3717 - val_mae: 6.4647\n",
      "Epoch 325/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 85.9710 - mse: 85.9710 - mae: 6.4353 - val_loss: 81.8347 - val_mse: 81.8346 - val_mae: 6.4091\n",
      "Epoch 326/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.9820 - mse: 85.9820 - mae: 6.4294 - val_loss: 81.8905 - val_mse: 81.8904 - val_mae: 6.4550\n",
      "Epoch 327/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 86.0864 - mse: 86.0864 - mae: 6.4330 - val_loss: 81.9662 - val_mse: 81.9662 - val_mae: 6.4804\n",
      "Epoch 328/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.9570 - mse: 85.9570 - mae: 6.4330 - val_loss: 81.9519 - val_mse: 81.9519 - val_mae: 6.4003\n",
      "Epoch 329/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.9565 - mse: 85.9564 - mae: 6.4207 - val_loss: 81.9040 - val_mse: 81.9039 - val_mae: 6.4585\n",
      "Epoch 330/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.0078 - mse: 86.0078 - mae: 6.4298 - val_loss: 82.0238 - val_mse: 82.0239 - val_mae: 6.5411\n",
      "Epoch 331/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9811 - mse: 85.9811 - mae: 6.4288 - val_loss: 81.6967 - val_mse: 81.6967 - val_mae: 6.4783\n",
      "Epoch 332/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9186 - mse: 85.9186 - mae: 6.4317 - val_loss: 81.8566 - val_mse: 81.8565 - val_mae: 6.5337\n",
      "Epoch 333/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.9171 - mse: 85.9171 - mae: 6.4346 - val_loss: 81.5537 - val_mse: 81.5536 - val_mae: 6.4436\n",
      "Epoch 334/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 86.0075 - mse: 86.0075 - mae: 6.4356 - val_loss: 82.0509 - val_mse: 82.0510 - val_mae: 6.5497\n",
      "Epoch 335/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.9994 - mse: 85.9996 - mae: 6.4421 - val_loss: 82.2802 - val_mse: 82.2802 - val_mae: 6.5413\n",
      "Epoch 336/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.8600 - mse: 85.8601 - mae: 6.4363 - val_loss: 82.7115 - val_mse: 82.7115 - val_mae: 6.3753\n",
      "Epoch 337/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.8703 - mse: 85.8703 - mae: 6.4249 - val_loss: 81.8392 - val_mse: 81.8392 - val_mae: 6.4090\n",
      "Epoch 338/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8690 - mse: 85.8691 - mae: 6.4360 - val_loss: 81.8453 - val_mse: 81.8453 - val_mae: 6.4327\n",
      "Epoch 339/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 86.0096 - mse: 86.0096 - mae: 6.4337 - val_loss: 81.4950 - val_mse: 81.4950 - val_mae: 6.4666\n",
      "Epoch 340/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.9989 - mse: 85.9989 - mae: 6.4366 - val_loss: 82.3385 - val_mse: 82.3385 - val_mae: 6.5592\n",
      "Epoch 341/1000\n",
      "14194/14194 [==============================] - 2s 150us/step - loss: 86.0282 - mse: 86.0283 - mae: 6.4423 - val_loss: 82.3437 - val_mse: 82.3437 - val_mae: 6.4341\n",
      "Epoch 342/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.9763 - mse: 85.9764 - mae: 6.4270 - val_loss: 81.6984 - val_mse: 81.6984 - val_mae: 6.4935\n",
      "Epoch 343/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.9519 - mse: 85.9519 - mae: 6.4375 - val_loss: 81.7412 - val_mse: 81.7413 - val_mae: 6.4651\n",
      "Epoch 344/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9160 - mse: 85.9159 - mae: 6.4319 - val_loss: 81.5333 - val_mse: 81.5333 - val_mae: 6.4598\n",
      "Epoch 345/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 85.9941 - mse: 85.9942 - mae: 6.4306 - val_loss: 81.5391 - val_mse: 81.5391 - val_mae: 6.4828\n",
      "Epoch 346/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.9923 - mse: 85.9923 - mae: 6.4323 - val_loss: 81.6805 - val_mse: 81.6805 - val_mae: 6.4484\n",
      "Epoch 347/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.9413 - mse: 85.9413 - mae: 6.4372 - val_loss: 81.7237 - val_mse: 81.7237 - val_mae: 6.4833\n",
      "Epoch 348/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.9248 - mse: 85.9248 - mae: 6.4328 - val_loss: 81.7150 - val_mse: 81.7149 - val_mae: 6.5024\n",
      "Epoch 349/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8920 - mse: 85.8921 - mae: 6.4248 - val_loss: 81.4696 - val_mse: 81.4696 - val_mae: 6.4853\n",
      "Epoch 350/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.8988 - mse: 85.8989 - mae: 6.4358 - val_loss: 81.7427 - val_mse: 81.7427 - val_mae: 6.4215\n",
      "Epoch 351/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.8810 - mse: 85.8811 - mae: 6.4323 - val_loss: 81.6710 - val_mse: 81.6710 - val_mae: 6.4470\n",
      "Epoch 352/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 85.9591 - mse: 85.9590 - mae: 6.4314 - val_loss: 81.7347 - val_mse: 81.7347 - val_mae: 6.4624\n",
      "Epoch 353/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.9050 - mse: 85.9050 - mae: 6.4415 - val_loss: 81.8315 - val_mse: 81.8315 - val_mae: 6.4197\n",
      "Epoch 354/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.8878 - mse: 85.8878 - mae: 6.4313 - val_loss: 81.4685 - val_mse: 81.4685 - val_mae: 6.4399\n",
      "Epoch 355/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.8420 - mse: 85.8420 - mae: 6.4347 - val_loss: 81.5971 - val_mse: 81.5971 - val_mae: 6.4556\n",
      "Epoch 356/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.8943 - mse: 85.8943 - mae: 6.4298 - val_loss: 81.7885 - val_mse: 81.7885 - val_mae: 6.5548\n",
      "Epoch 357/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.9527 - mse: 85.9527 - mae: 6.4395 - val_loss: 81.9136 - val_mse: 81.9136 - val_mae: 6.4283\n",
      "Epoch 358/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.9845 - mse: 85.9846 - mae: 6.4326 - val_loss: 81.5849 - val_mse: 81.5849 - val_mae: 6.4990\n",
      "Epoch 359/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 86.0144 - mse: 86.0144 - mae: 6.4285 - val_loss: 81.5742 - val_mse: 81.5742 - val_mae: 6.4535\n",
      "Epoch 360/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.8804 - mse: 85.8804 - mae: 6.4227 - val_loss: 82.5738 - val_mse: 82.5738 - val_mae: 6.6584\n",
      "Epoch 361/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.9565 - mse: 85.9565 - mae: 6.4402 - val_loss: 81.6348 - val_mse: 81.6348 - val_mae: 6.4577\n",
      "Epoch 362/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.8569 - mse: 85.8569 - mae: 6.4290 - val_loss: 81.6755 - val_mse: 81.6756 - val_mae: 6.4530\n",
      "Epoch 363/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 85.8986 - mse: 85.8986 - mae: 6.4348 - val_loss: 82.4813 - val_mse: 82.4814 - val_mae: 6.4009\n",
      "Epoch 364/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 86.0673 - mse: 86.0672 - mae: 6.4344 - val_loss: 81.6032 - val_mse: 81.6031 - val_mae: 6.5049\n",
      "Epoch 365/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.9761 - mse: 85.9761 - mae: 6.4316 - val_loss: 82.0064 - val_mse: 82.0064 - val_mae: 6.5416\n",
      "Epoch 366/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.8392 - mse: 85.8393 - mae: 6.4305 - val_loss: 81.7948 - val_mse: 81.7948 - val_mae: 6.4000\n",
      "Epoch 367/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9356 - mse: 85.9356 - mae: 6.4317 - val_loss: 81.3785 - val_mse: 81.3785 - val_mae: 6.4562\n",
      "Epoch 368/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7411 - mse: 85.7411 - mae: 6.4252 - val_loss: 81.6932 - val_mse: 81.6932 - val_mae: 6.5085\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 143us/step - loss: 86.0039 - mse: 86.0039 - mae: 6.4384 - val_loss: 81.9732 - val_mse: 81.9732 - val_mae: 6.5577\n",
      "Epoch 370/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.9407 - mse: 85.9408 - mae: 6.4408 - val_loss: 82.1601 - val_mse: 82.1601 - val_mae: 6.3958\n",
      "Epoch 371/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 85.8137 - mse: 85.8137 - mae: 6.4300 - val_loss: 81.6723 - val_mse: 81.6723 - val_mae: 6.4090\n",
      "Epoch 372/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8967 - mse: 85.8967 - mae: 6.4317 - val_loss: 81.6291 - val_mse: 81.6291 - val_mae: 6.5046\n",
      "Epoch 373/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 85.9401 - mse: 85.9401 - mae: 6.4443 - val_loss: 81.7041 - val_mse: 81.7041 - val_mae: 6.4089\n",
      "Epoch 374/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 85.9903 - mse: 85.9902 - mae: 6.4278 - val_loss: 81.5529 - val_mse: 81.5529 - val_mae: 6.5149\n",
      "Epoch 375/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.8883 - mse: 85.8883 - mae: 6.4313 - val_loss: 81.6148 - val_mse: 81.6148 - val_mae: 6.4712\n",
      "Epoch 376/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.9393 - mse: 85.9393 - mae: 6.4317 - val_loss: 81.4920 - val_mse: 81.4921 - val_mae: 6.4821\n",
      "Epoch 377/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8829 - mse: 85.8829 - mae: 6.4332 - val_loss: 81.6091 - val_mse: 81.6092 - val_mae: 6.3969\n",
      "Epoch 378/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8910 - mse: 85.8909 - mae: 6.4308 - val_loss: 81.8625 - val_mse: 81.8625 - val_mae: 6.3935\n",
      "Epoch 379/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8839 - mse: 85.8839 - mae: 6.4385 - val_loss: 81.7106 - val_mse: 81.7107 - val_mae: 6.4269\n",
      "Epoch 380/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.8937 - mse: 85.8936 - mae: 6.4311 - val_loss: 81.8045 - val_mse: 81.8045 - val_mae: 6.4474\n",
      "Epoch 381/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.9558 - mse: 85.9558 - mae: 6.4379 - val_loss: 81.5770 - val_mse: 81.5770 - val_mae: 6.4679\n",
      "Epoch 382/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.8394 - mse: 85.8394 - mae: 6.4374 - val_loss: 82.4529 - val_mse: 82.4529 - val_mae: 6.3903\n",
      "Epoch 383/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.8386 - mse: 85.8386 - mae: 6.4239 - val_loss: 81.7904 - val_mse: 81.7904 - val_mae: 6.4016\n",
      "Epoch 384/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.9268 - mse: 85.9269 - mae: 6.4339 - val_loss: 81.2818 - val_mse: 81.2818 - val_mae: 6.4685\n",
      "Epoch 385/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8051 - mse: 85.8051 - mae: 6.4334 - val_loss: 82.0387 - val_mse: 82.0387 - val_mae: 6.6028\n",
      "Epoch 386/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.9041 - mse: 85.9042 - mae: 6.4348 - val_loss: 81.7945 - val_mse: 81.7945 - val_mae: 6.4493\n",
      "Epoch 387/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.9762 - mse: 85.9762 - mae: 6.4292 - val_loss: 81.5189 - val_mse: 81.5189 - val_mae: 6.5186\n",
      "Epoch 388/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.7668 - mse: 85.7667 - mae: 6.4266 - val_loss: 81.7146 - val_mse: 81.7146 - val_mae: 6.4814\n",
      "Epoch 389/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.9225 - mse: 85.9225 - mae: 6.4327 - val_loss: 82.3788 - val_mse: 82.3789 - val_mae: 6.5957\n",
      "Epoch 390/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.8449 - mse: 85.8449 - mae: 6.4341 - val_loss: 81.7266 - val_mse: 81.7266 - val_mae: 6.5107\n",
      "Epoch 391/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9219 - mse: 85.9219 - mae: 6.4384 - val_loss: 81.7315 - val_mse: 81.7315 - val_mae: 6.4240\n",
      "Epoch 392/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.9555 - mse: 85.9554 - mae: 6.4337 - val_loss: 81.4860 - val_mse: 81.4860 - val_mae: 6.4112\n",
      "Epoch 393/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7821 - mse: 85.7821 - mae: 6.4253 - val_loss: 82.0724 - val_mse: 82.0724 - val_mae: 6.5563\n",
      "Epoch 394/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.9791 - mse: 85.9792 - mae: 6.4314 - val_loss: 81.7676 - val_mse: 81.7676 - val_mae: 6.4251\n",
      "Epoch 395/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8078 - mse: 85.8077 - mae: 6.4289 - val_loss: 81.7530 - val_mse: 81.7530 - val_mae: 6.4503\n",
      "Epoch 396/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8548 - mse: 85.8547 - mae: 6.4268 - val_loss: 81.7066 - val_mse: 81.7066 - val_mae: 6.5014\n",
      "Epoch 397/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7350 - mse: 85.7350 - mae: 6.4317 - val_loss: 81.5705 - val_mse: 81.5705 - val_mae: 6.4445\n",
      "Epoch 398/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.9317 - mse: 85.9318 - mae: 6.4346 - val_loss: 81.3539 - val_mse: 81.3539 - val_mae: 6.4579\n",
      "Epoch 399/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.9557 - mse: 85.9557 - mae: 6.4329 - val_loss: 81.3211 - val_mse: 81.3210 - val_mae: 6.4371\n",
      "Epoch 400/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7889 - mse: 85.7890 - mae: 6.4269 - val_loss: 81.5783 - val_mse: 81.5783 - val_mae: 6.4832\n",
      "Epoch 401/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.9279 - mse: 85.9279 - mae: 6.4290 - val_loss: 81.4731 - val_mse: 81.4731 - val_mae: 6.4761\n",
      "Epoch 402/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 85.8546 - mse: 85.8546 - mae: 6.4270 - val_loss: 81.7298 - val_mse: 81.7298 - val_mae: 6.4140\n",
      "Epoch 403/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7948 - mse: 85.7948 - mae: 6.4218 - val_loss: 81.5870 - val_mse: 81.5869 - val_mae: 6.5110\n",
      "Epoch 404/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.8739 - mse: 85.8739 - mae: 6.4320 - val_loss: 82.1316 - val_mse: 82.1315 - val_mae: 6.4307\n",
      "Epoch 405/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 85.9182 - mse: 85.9180 - mae: 6.4224 - val_loss: 81.9573 - val_mse: 81.9573 - val_mae: 6.5700\n",
      "Epoch 406/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.8897 - mse: 85.8897 - mae: 6.4340 - val_loss: 81.7352 - val_mse: 81.7353 - val_mae: 6.4300\n",
      "Epoch 407/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.8580 - mse: 85.8580 - mae: 6.4319 - val_loss: 81.9992 - val_mse: 81.9992 - val_mae: 6.4124\n",
      "Epoch 408/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.9465 - mse: 85.9465 - mae: 6.4311 - val_loss: 81.6326 - val_mse: 81.6326 - val_mae: 6.4021\n",
      "Epoch 409/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8392 - mse: 85.8392 - mae: 6.4353 - val_loss: 81.9182 - val_mse: 81.9182 - val_mae: 6.4011\n",
      "Epoch 410/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.8722 - mse: 85.8722 - mae: 6.4246 - val_loss: 82.6179 - val_mse: 82.6179 - val_mae: 6.6268\n",
      "Epoch 411/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.8626 - mse: 85.8627 - mae: 6.4318 - val_loss: 81.9291 - val_mse: 81.9291 - val_mae: 6.5715\n",
      "Epoch 412/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8288 - mse: 85.8289 - mae: 6.4295 - val_loss: 82.0179 - val_mse: 82.0179 - val_mae: 6.5614\n",
      "Epoch 413/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.8689 - mse: 85.8689 - mae: 6.4332 - val_loss: 81.4767 - val_mse: 81.4767 - val_mae: 6.5011\n",
      "Epoch 414/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.9162 - mse: 85.9162 - mae: 6.4303 - val_loss: 81.2631 - val_mse: 81.2631 - val_mae: 6.4677\n",
      "Epoch 415/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8365 - mse: 85.8365 - mae: 6.4256 - val_loss: 81.8652 - val_mse: 81.8652 - val_mae: 6.5111\n",
      "Epoch 416/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8684 - mse: 85.8683 - mae: 6.4286 - val_loss: 81.8167 - val_mse: 81.8166 - val_mae: 6.5107\n",
      "Epoch 417/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.9057 - mse: 85.9056 - mae: 6.4275 - val_loss: 81.5057 - val_mse: 81.5057 - val_mae: 6.4522\n",
      "Epoch 418/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.9685 - mse: 85.9686 - mae: 6.4322 - val_loss: 81.6197 - val_mse: 81.6198 - val_mae: 6.4236\n",
      "Epoch 419/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.9506 - mse: 85.9505 - mae: 6.4315 - val_loss: 81.6212 - val_mse: 81.6212 - val_mae: 6.4835\n",
      "Epoch 420/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8152 - mse: 85.8152 - mae: 6.4324 - val_loss: 82.3210 - val_mse: 82.3210 - val_mae: 6.3845\n",
      "Epoch 421/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.8649 - mse: 85.8649 - mae: 6.4340 - val_loss: 81.4520 - val_mse: 81.4520 - val_mae: 6.4454\n",
      "Epoch 422/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.9133 - mse: 85.9132 - mae: 6.4281 - val_loss: 81.4699 - val_mse: 81.4699 - val_mae: 6.5011\n",
      "Epoch 423/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.8769 - mse: 85.8768 - mae: 6.4327 - val_loss: 81.3659 - val_mse: 81.3659 - val_mae: 6.4335\n",
      "Epoch 424/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7204 - mse: 85.7204 - mae: 6.4260 - val_loss: 81.5238 - val_mse: 81.5239 - val_mae: 6.4697\n",
      "Epoch 425/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7915 - mse: 85.7915 - mae: 6.4326 - val_loss: 81.5763 - val_mse: 81.5763 - val_mae: 6.4469\n",
      "Epoch 426/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8308 - mse: 85.8309 - mae: 6.4314 - val_loss: 81.9798 - val_mse: 81.9798 - val_mae: 6.4996\n",
      "Epoch 427/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7725 - mse: 85.7726 - mae: 6.4238 - val_loss: 81.6449 - val_mse: 81.6449 - val_mae: 6.5136\n",
      "Epoch 428/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8407 - mse: 85.8406 - mae: 6.4256 - val_loss: 81.3230 - val_mse: 81.3230 - val_mae: 6.4500\n",
      "Epoch 429/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.8931 - mse: 85.8930 - mae: 6.4329 - val_loss: 81.3311 - val_mse: 81.3311 - val_mae: 6.4713\n",
      "Epoch 430/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 85.8592 - mse: 85.8591 - mae: 6.4246 - val_loss: 81.7029 - val_mse: 81.7029 - val_mae: 6.5575\n",
      "Epoch 431/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.9549 - mse: 85.9549 - mae: 6.4410 - val_loss: 81.3668 - val_mse: 81.3668 - val_mae: 6.4573\n",
      "Epoch 432/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8211 - mse: 85.8211 - mae: 6.4338 - val_loss: 81.7020 - val_mse: 81.7020 - val_mae: 6.4987\n",
      "Epoch 433/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8445 - mse: 85.8445 - mae: 6.4332 - val_loss: 81.6230 - val_mse: 81.6230 - val_mae: 6.4175\n",
      "Epoch 434/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8092 - mse: 85.8092 - mae: 6.4247 - val_loss: 81.6734 - val_mse: 81.6733 - val_mae: 6.5117\n",
      "Epoch 435/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8719 - mse: 85.8719 - mae: 6.4357 - val_loss: 81.3448 - val_mse: 81.3448 - val_mae: 6.4805\n",
      "Epoch 436/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.9785 - mse: 85.9785 - mae: 6.4298 - val_loss: 81.7745 - val_mse: 81.7744 - val_mae: 6.5126\n",
      "Epoch 437/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.8854 - mse: 85.8854 - mae: 6.4266 - val_loss: 81.4969 - val_mse: 81.4969 - val_mae: 6.4929\n",
      "Epoch 438/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7680 - mse: 85.7679 - mae: 6.4344 - val_loss: 81.6136 - val_mse: 81.6135 - val_mae: 6.4362\n",
      "Epoch 439/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.9135 - mse: 85.9135 - mae: 6.4315 - val_loss: 81.6636 - val_mse: 81.6636 - val_mae: 6.4347\n",
      "Epoch 440/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7950 - mse: 85.7950 - mae: 6.4178 - val_loss: 85.6672 - val_mse: 85.6672 - val_mae: 6.8681\n",
      "Epoch 441/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8335 - mse: 85.8335 - mae: 6.4413 - val_loss: 81.5486 - val_mse: 81.5485 - val_mae: 6.5205\n",
      "Epoch 442/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.8580 - mse: 85.8579 - mae: 6.4218 - val_loss: 81.8880 - val_mse: 81.8880 - val_mae: 6.4841\n",
      "Epoch 443/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7696 - mse: 85.7696 - mae: 6.4294 - val_loss: 82.5520 - val_mse: 82.5520 - val_mae: 6.4071\n",
      "Epoch 444/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7840 - mse: 85.7840 - mae: 6.4220 - val_loss: 81.5986 - val_mse: 81.5986 - val_mae: 6.5129\n",
      "Epoch 445/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8271 - mse: 85.8271 - mae: 6.4348 - val_loss: 81.5913 - val_mse: 81.5913 - val_mae: 6.4353\n",
      "Epoch 446/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7685 - mse: 85.7686 - mae: 6.4292 - val_loss: 81.8291 - val_mse: 81.8291 - val_mae: 6.4982\n",
      "Epoch 447/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.9003 - mse: 85.9003 - mae: 6.4357 - val_loss: 81.5233 - val_mse: 81.5233 - val_mae: 6.5426\n",
      "Epoch 448/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7022 - mse: 85.7022 - mae: 6.4277 - val_loss: 81.2720 - val_mse: 81.2720 - val_mae: 6.4707\n",
      "Epoch 449/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.8636 - mse: 85.8636 - mae: 6.4407 - val_loss: 81.8336 - val_mse: 81.8336 - val_mae: 6.3909\n",
      "Epoch 450/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.9225 - mse: 85.9226 - mae: 6.4329 - val_loss: 81.5249 - val_mse: 81.5249 - val_mae: 6.4835\n",
      "Epoch 451/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6646 - mse: 85.6647 - mae: 6.4260 - val_loss: 82.1700 - val_mse: 82.1700 - val_mae: 6.5905\n",
      "Epoch 452/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.7542 - mse: 85.7542 - mae: 6.4321 - val_loss: 81.7421 - val_mse: 81.7421 - val_mae: 6.4473\n",
      "Epoch 453/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.9388 - mse: 85.9388 - mae: 6.4252 - val_loss: 81.5017 - val_mse: 81.5017 - val_mae: 6.4452\n",
      "Epoch 454/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7412 - mse: 85.7411 - mae: 6.4286 - val_loss: 81.3710 - val_mse: 81.3710 - val_mae: 6.4313\n",
      "Epoch 455/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7235 - mse: 85.7235 - mae: 6.4286 - val_loss: 82.4295 - val_mse: 82.4295 - val_mae: 6.3866\n",
      "Epoch 456/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8578 - mse: 85.8577 - mae: 6.4233 - val_loss: 81.4253 - val_mse: 81.4253 - val_mae: 6.4425\n",
      "Epoch 457/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8143 - mse: 85.8142 - mae: 6.4309 - val_loss: 81.9352 - val_mse: 81.9352 - val_mae: 6.4022\n",
      "Epoch 458/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.8058 - mse: 85.8057 - mae: 6.4320 - val_loss: 81.5487 - val_mse: 81.5487 - val_mae: 6.4405\n",
      "Epoch 459/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8232 - mse: 85.8232 - mae: 6.4271 - val_loss: 81.4319 - val_mse: 81.4319 - val_mae: 6.4295\n",
      "Epoch 460/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8308 - mse: 85.8308 - mae: 6.4321 - val_loss: 81.3571 - val_mse: 81.3571 - val_mae: 6.4627\n",
      "Epoch 461/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.8022 - mse: 85.8022 - mae: 6.4277 - val_loss: 81.8224 - val_mse: 81.8224 - val_mae: 6.5223\n",
      "Epoch 462/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8287 - mse: 85.8288 - mae: 6.4278 - val_loss: 81.4392 - val_mse: 81.4392 - val_mae: 6.4566\n",
      "Epoch 463/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.7814 - mse: 85.7814 - mae: 6.4264 - val_loss: 81.5979 - val_mse: 81.5979 - val_mae: 6.4230\n",
      "Epoch 464/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8130 - mse: 85.8130 - mae: 6.4226 - val_loss: 81.5812 - val_mse: 81.5812 - val_mae: 6.4490\n",
      "Epoch 465/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8450 - mse: 85.8450 - mae: 6.4338 - val_loss: 81.3845 - val_mse: 81.3845 - val_mae: 6.4487\n",
      "Epoch 466/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8714 - mse: 85.8713 - mae: 6.4253 - val_loss: 81.6187 - val_mse: 81.6187 - val_mae: 6.3972\n",
      "Epoch 467/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.7951 - mse: 85.7951 - mae: 6.4242 - val_loss: 81.4543 - val_mse: 81.4543 - val_mae: 6.4522\n",
      "Epoch 468/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.7229 - mse: 85.7228 - mae: 6.4270 - val_loss: 82.0843 - val_mse: 82.0843 - val_mae: 6.5307\n",
      "Epoch 469/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.7357 - mse: 85.7357 - mae: 6.4244 - val_loss: 81.8519 - val_mse: 81.8520 - val_mae: 6.3951\n",
      "Epoch 470/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7419 - mse: 85.7419 - mae: 6.4229 - val_loss: 81.6520 - val_mse: 81.6520 - val_mae: 6.5676\n",
      "Epoch 471/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.8756 - mse: 85.8756 - mae: 6.4350 - val_loss: 81.4637 - val_mse: 81.4636 - val_mae: 6.4305\n",
      "Epoch 472/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6787 - mse: 85.6787 - mae: 6.4211 - val_loss: 82.1272 - val_mse: 82.1272 - val_mae: 6.5219\n",
      "Epoch 473/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8234 - mse: 85.8234 - mae: 6.4338 - val_loss: 81.8388 - val_mse: 81.8387 - val_mae: 6.5461\n",
      "Epoch 474/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8985 - mse: 85.8985 - mae: 6.4367 - val_loss: 81.2706 - val_mse: 81.2706 - val_mae: 6.4363\n",
      "Epoch 475/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8084 - mse: 85.8084 - mae: 6.4284 - val_loss: 81.5321 - val_mse: 81.5322 - val_mae: 6.4373\n",
      "Epoch 476/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.9287 - mse: 85.9287 - mae: 6.4203 - val_loss: 82.3727 - val_mse: 82.3727 - val_mae: 6.6194\n",
      "Epoch 477/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8099 - mse: 85.8099 - mae: 6.4342 - val_loss: 81.6335 - val_mse: 81.6335 - val_mae: 6.4255\n",
      "Epoch 478/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7199 - mse: 85.7199 - mae: 6.4219 - val_loss: 82.1082 - val_mse: 82.1083 - val_mae: 6.3979\n",
      "Epoch 479/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.8853 - mse: 85.8852 - mae: 6.4273 - val_loss: 81.2446 - val_mse: 81.2446 - val_mae: 6.4428\n",
      "Epoch 480/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7529 - mse: 85.7528 - mae: 6.4256 - val_loss: 81.8684 - val_mse: 81.8684 - val_mae: 6.5770\n",
      "Epoch 481/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7634 - mse: 85.7633 - mae: 6.4291 - val_loss: 81.6182 - val_mse: 81.6182 - val_mae: 6.4851\n",
      "Epoch 482/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.8428 - mse: 85.8427 - mae: 6.4316 - val_loss: 81.6124 - val_mse: 81.6125 - val_mae: 6.4016\n",
      "Epoch 483/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7962 - mse: 85.7961 - mae: 6.4176 - val_loss: 81.5802 - val_mse: 81.5801 - val_mae: 6.4509\n",
      "Epoch 484/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.7997 - mse: 85.7996 - mae: 6.4298 - val_loss: 81.2636 - val_mse: 81.2635 - val_mae: 6.4774\n",
      "Epoch 485/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.8451 - mse: 85.8451 - mae: 6.4309 - val_loss: 81.6300 - val_mse: 81.6300 - val_mae: 6.4943\n",
      "Epoch 486/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7853 - mse: 85.7854 - mae: 6.4323 - val_loss: 82.0908 - val_mse: 82.0908 - val_mae: 6.4176\n",
      "Epoch 487/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.6626 - mse: 85.6626 - mae: 6.4261 - val_loss: 81.3682 - val_mse: 81.3682 - val_mae: 6.4369\n",
      "Epoch 488/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.7475 - mse: 85.7474 - mae: 6.4267 - val_loss: 81.8153 - val_mse: 81.8154 - val_mae: 6.4038\n",
      "Epoch 489/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.8027 - mse: 85.8027 - mae: 6.4292 - val_loss: 81.5784 - val_mse: 81.5784 - val_mae: 6.4982\n",
      "Epoch 490/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.9252 - mse: 85.9252 - mae: 6.4305 - val_loss: 81.8987 - val_mse: 81.8987 - val_mae: 6.4004\n",
      "Epoch 491/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.8632 - mse: 85.8632 - mae: 6.4269 - val_loss: 81.9851 - val_mse: 81.9852 - val_mae: 6.5311\n",
      "Epoch 492/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6763 - mse: 85.6762 - mae: 6.4297 - val_loss: 81.4821 - val_mse: 81.4821 - val_mae: 6.4874\n",
      "Epoch 493/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.5937 - mse: 85.5937 - mae: 6.4176 - val_loss: 82.1650 - val_mse: 82.1650 - val_mae: 6.5772\n",
      "Epoch 494/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.8156 - mse: 85.8155 - mae: 6.4258 - val_loss: 81.5839 - val_mse: 81.5839 - val_mae: 6.5457\n",
      "Epoch 495/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.8838 - mse: 85.8838 - mae: 6.4311 - val_loss: 81.6873 - val_mse: 81.6873 - val_mae: 6.4201\n",
      "Epoch 496/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7235 - mse: 85.7235 - mae: 6.4210 - val_loss: 81.5918 - val_mse: 81.5918 - val_mae: 6.4254\n",
      "Epoch 497/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.7363 - mse: 85.7363 - mae: 6.4270 - val_loss: 81.7188 - val_mse: 81.7188 - val_mae: 6.4195\n",
      "Epoch 498/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7955 - mse: 85.7955 - mae: 6.4267 - val_loss: 81.4333 - val_mse: 81.4333 - val_mae: 6.4799\n",
      "Epoch 499/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.8482 - mse: 85.8483 - mae: 6.4217 - val_loss: 82.3565 - val_mse: 82.3565 - val_mae: 6.4276\n",
      "Epoch 500/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.8006 - mse: 85.8005 - mae: 6.4282 - val_loss: 81.6941 - val_mse: 81.6941 - val_mae: 6.5280\n",
      "Epoch 501/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.8084 - mse: 85.8084 - mae: 6.4282 - val_loss: 81.4818 - val_mse: 81.4819 - val_mae: 6.5141\n",
      "Epoch 502/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7742 - mse: 85.7742 - mae: 6.4275 - val_loss: 81.5812 - val_mse: 81.5812 - val_mae: 6.5264\n",
      "Epoch 503/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7608 - mse: 85.7609 - mae: 6.4271 - val_loss: 81.8284 - val_mse: 81.8284 - val_mae: 6.4378\n",
      "Epoch 504/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 85.7912 - mse: 85.7913 - mae: 6.4249 - val_loss: 81.4139 - val_mse: 81.4138 - val_mae: 6.4963\n",
      "Epoch 505/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7326 - mse: 85.7327 - mae: 6.4257 - val_loss: 82.0560 - val_mse: 82.0560 - val_mae: 6.4883\n",
      "Epoch 506/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7856 - mse: 85.7856 - mae: 6.4222 - val_loss: 81.3206 - val_mse: 81.3206 - val_mae: 6.4359\n",
      "Epoch 507/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.8963 - mse: 85.8962 - mae: 6.4294 - val_loss: 81.5122 - val_mse: 81.5122 - val_mae: 6.4787\n",
      "Epoch 508/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7084 - mse: 85.7083 - mae: 6.4328 - val_loss: 81.8662 - val_mse: 81.8661 - val_mae: 6.3757\n",
      "Epoch 509/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7413 - mse: 85.7413 - mae: 6.4231 - val_loss: 82.0903 - val_mse: 82.0904 - val_mae: 6.3938\n",
      "Epoch 510/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7861 - mse: 85.7861 - mae: 6.4244 - val_loss: 81.8180 - val_mse: 81.8180 - val_mae: 6.3995\n",
      "Epoch 511/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7216 - mse: 85.7216 - mae: 6.4228 - val_loss: 81.8426 - val_mse: 81.8427 - val_mae: 6.4068\n",
      "Epoch 512/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8799 - mse: 85.8799 - mae: 6.4232 - val_loss: 81.4983 - val_mse: 81.4984 - val_mae: 6.4275\n",
      "Epoch 513/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.8411 - mse: 85.8410 - mae: 6.4300 - val_loss: 81.6274 - val_mse: 81.6274 - val_mae: 6.3868\n",
      "Epoch 514/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.7518 - mse: 85.7518 - mae: 6.4219 - val_loss: 81.1859 - val_mse: 81.1860 - val_mae: 6.4339\n",
      "Epoch 515/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.8170 - mse: 85.8170 - mae: 6.4213 - val_loss: 81.2739 - val_mse: 81.2739 - val_mae: 6.4450\n",
      "Epoch 516/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7530 - mse: 85.7530 - mae: 6.4269 - val_loss: 81.9806 - val_mse: 81.9806 - val_mae: 6.5918\n",
      "Epoch 517/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7712 - mse: 85.7713 - mae: 6.4276 - val_loss: 81.5447 - val_mse: 81.5447 - val_mae: 6.5404\n",
      "Epoch 518/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7916 - mse: 85.7917 - mae: 6.4325 - val_loss: 81.4559 - val_mse: 81.4559 - val_mae: 6.4202\n",
      "Epoch 519/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8486 - mse: 85.8486 - mae: 6.4237 - val_loss: 81.5118 - val_mse: 81.5118 - val_mae: 6.4848\n",
      "Epoch 520/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.8027 - mse: 85.8027 - mae: 6.4308 - val_loss: 82.6651 - val_mse: 82.6651 - val_mae: 6.4015\n",
      "Epoch 521/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.9172 - mse: 85.9172 - mae: 6.4266 - val_loss: 81.3983 - val_mse: 81.3983 - val_mae: 6.4525\n",
      "Epoch 522/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7886 - mse: 85.7886 - mae: 6.4322 - val_loss: 81.4818 - val_mse: 81.4818 - val_mae: 6.4701\n",
      "Epoch 523/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7649 - mse: 85.7649 - mae: 6.4226 - val_loss: 81.6488 - val_mse: 81.6488 - val_mae: 6.3993\n",
      "Epoch 524/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.6453 - mse: 85.6453 - mae: 6.4227 - val_loss: 82.2244 - val_mse: 82.2243 - val_mae: 6.5591\n",
      "Epoch 525/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.6720 - mse: 85.6720 - mae: 6.4221 - val_loss: 81.6739 - val_mse: 81.6739 - val_mae: 6.4139\n",
      "Epoch 526/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7986 - mse: 85.7985 - mae: 6.4292 - val_loss: 82.2817 - val_mse: 82.2817 - val_mae: 6.5321\n",
      "Epoch 527/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.6792 - mse: 85.6792 - mae: 6.4309 - val_loss: 81.8816 - val_mse: 81.8816 - val_mae: 6.3954\n",
      "Epoch 528/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7658 - mse: 85.7657 - mae: 6.4242 - val_loss: 82.2424 - val_mse: 82.2424 - val_mae: 6.3943\n",
      "Epoch 529/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7358 - mse: 85.7359 - mae: 6.4266 - val_loss: 81.3511 - val_mse: 81.3511 - val_mae: 6.4265\n",
      "Epoch 530/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.8240 - mse: 85.8240 - mae: 6.4316 - val_loss: 81.4808 - val_mse: 81.4807 - val_mae: 6.4253\n",
      "Epoch 531/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.8409 - mse: 85.8409 - mae: 6.4325 - val_loss: 81.6517 - val_mse: 81.6517 - val_mae: 6.4652\n",
      "Epoch 532/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7651 - mse: 85.7651 - mae: 6.4205 - val_loss: 81.4709 - val_mse: 81.4709 - val_mae: 6.4878\n",
      "Epoch 533/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8970 - mse: 85.8970 - mae: 6.4267 - val_loss: 81.6598 - val_mse: 81.6598 - val_mae: 6.4287\n",
      "Epoch 534/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8857 - mse: 85.8857 - mae: 6.4247 - val_loss: 81.7267 - val_mse: 81.7267 - val_mae: 6.5479\n",
      "Epoch 535/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.6485 - mse: 85.6485 - mae: 6.4244 - val_loss: 81.8331 - val_mse: 81.8331 - val_mae: 6.5735\n",
      "Epoch 536/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8217 - mse: 85.8216 - mae: 6.4318 - val_loss: 81.7259 - val_mse: 81.7259 - val_mae: 6.5297\n",
      "Epoch 537/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.8333 - mse: 85.8333 - mae: 6.4268 - val_loss: 82.2654 - val_mse: 82.2654 - val_mae: 6.3887\n",
      "Epoch 538/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8073 - mse: 85.8073 - mae: 6.4171 - val_loss: 81.2888 - val_mse: 81.2888 - val_mae: 6.5109\n",
      "Epoch 539/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7229 - mse: 85.7230 - mae: 6.4278 - val_loss: 81.2226 - val_mse: 81.2226 - val_mae: 6.4601\n",
      "Epoch 540/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.8607 - mse: 85.8607 - mae: 6.4343 - val_loss: 81.3164 - val_mse: 81.3164 - val_mae: 6.4136\n",
      "Epoch 541/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.8526 - mse: 85.8527 - mae: 6.4315 - val_loss: 81.3313 - val_mse: 81.3312 - val_mae: 6.4167\n",
      "Epoch 542/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7356 - mse: 85.7355 - mae: 6.4119 - val_loss: 81.8255 - val_mse: 81.8255 - val_mae: 6.5808\n",
      "Epoch 543/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.7758 - mse: 85.7757 - mae: 6.4230 - val_loss: 81.7271 - val_mse: 81.7271 - val_mae: 6.5440\n",
      "Epoch 544/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7178 - mse: 85.7179 - mae: 6.4239 - val_loss: 81.9947 - val_mse: 81.9947 - val_mae: 6.4412\n",
      "Epoch 545/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6450 - mse: 85.6450 - mae: 6.4229 - val_loss: 82.3004 - val_mse: 82.3005 - val_mae: 6.3905\n",
      "Epoch 546/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7032 - mse: 85.7032 - mae: 6.4160 - val_loss: 81.5385 - val_mse: 81.5385 - val_mae: 6.5626\n",
      "Epoch 547/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.8047 - mse: 85.8047 - mae: 6.4400 - val_loss: 81.4197 - val_mse: 81.4197 - val_mae: 6.4476\n",
      "Epoch 548/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.8151 - mse: 85.8151 - mae: 6.4266 - val_loss: 81.8486 - val_mse: 81.8487 - val_mae: 6.5536\n",
      "Epoch 549/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7288 - mse: 85.7289 - mae: 6.4339 - val_loss: 81.5159 - val_mse: 81.5159 - val_mae: 6.4632\n",
      "Epoch 550/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7913 - mse: 85.7913 - mae: 6.4289 - val_loss: 81.5944 - val_mse: 81.5944 - val_mae: 6.4653\n",
      "Epoch 551/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.8282 - mse: 85.8282 - mae: 6.4343 - val_loss: 81.5193 - val_mse: 81.5193 - val_mae: 6.4592\n",
      "Epoch 552/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7975 - mse: 85.7975 - mae: 6.4204 - val_loss: 82.4949 - val_mse: 82.4950 - val_mae: 6.5822\n",
      "Epoch 553/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7000 - mse: 85.7000 - mae: 6.4206 - val_loss: 81.9968 - val_mse: 81.9968 - val_mae: 6.5488\n",
      "Epoch 554/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.8526 - mse: 85.8526 - mae: 6.4277 - val_loss: 81.5008 - val_mse: 81.5008 - val_mae: 6.4421\n",
      "Epoch 555/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7663 - mse: 85.7663 - mae: 6.4264 - val_loss: 81.3939 - val_mse: 81.3940 - val_mae: 6.4494\n",
      "Epoch 556/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7603 - mse: 85.7603 - mae: 6.4177 - val_loss: 81.4885 - val_mse: 81.4886 - val_mae: 6.5273\n",
      "Epoch 557/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.8193 - mse: 85.8193 - mae: 6.4270 - val_loss: 81.1386 - val_mse: 81.1386 - val_mae: 6.4657\n",
      "Epoch 558/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.7664 - mse: 85.7664 - mae: 6.4278 - val_loss: 82.2523 - val_mse: 82.2523 - val_mae: 6.3884\n",
      "Epoch 559/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.8424 - mse: 85.8424 - mae: 6.4273 - val_loss: 82.0911 - val_mse: 82.0912 - val_mae: 6.5266\n",
      "Epoch 560/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.8610 - mse: 85.8611 - mae: 6.4244 - val_loss: 81.2755 - val_mse: 81.2755 - val_mae: 6.4806\n",
      "Epoch 561/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6678 - mse: 85.6678 - mae: 6.4238 - val_loss: 82.2864 - val_mse: 82.2864 - val_mae: 6.5995\n",
      "Epoch 562/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.8381 - mse: 85.8381 - mae: 6.4371 - val_loss: 81.3115 - val_mse: 81.3115 - val_mae: 6.4393\n",
      "Epoch 563/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.8298 - mse: 85.8297 - mae: 6.4266 - val_loss: 82.1087 - val_mse: 82.1087 - val_mae: 6.4205\n",
      "Epoch 564/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7654 - mse: 85.7654 - mae: 6.4231 - val_loss: 81.5898 - val_mse: 81.5898 - val_mae: 6.4798\n",
      "Epoch 565/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7047 - mse: 85.7047 - mae: 6.4233 - val_loss: 81.5205 - val_mse: 81.5205 - val_mae: 6.4537\n",
      "Epoch 566/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.8457 - mse: 85.8458 - mae: 6.4334 - val_loss: 81.5759 - val_mse: 81.5759 - val_mae: 6.4148\n",
      "Epoch 567/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7852 - mse: 85.7852 - mae: 6.4215 - val_loss: 81.7900 - val_mse: 81.7900 - val_mae: 6.5944\n",
      "Epoch 568/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.5959 - mse: 85.5959 - mae: 6.4188 - val_loss: 81.6060 - val_mse: 81.6061 - val_mae: 6.4821\n",
      "Epoch 569/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7019 - mse: 85.7018 - mae: 6.4174 - val_loss: 82.2124 - val_mse: 82.2124 - val_mae: 6.5296\n",
      "Epoch 570/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.6688 - mse: 85.6688 - mae: 6.4207 - val_loss: 82.6077 - val_mse: 82.6077 - val_mae: 6.6790\n",
      "Epoch 571/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7850 - mse: 85.7851 - mae: 6.4284 - val_loss: 81.3050 - val_mse: 81.3050 - val_mae: 6.4758\n",
      "Epoch 572/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7042 - mse: 85.7042 - mae: 6.4269 - val_loss: 81.7985 - val_mse: 81.7986 - val_mae: 6.5584\n",
      "Epoch 573/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7041 - mse: 85.7040 - mae: 6.4268 - val_loss: 81.4401 - val_mse: 81.4401 - val_mae: 6.5123\n",
      "Epoch 574/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8263 - mse: 85.8263 - mae: 6.4332 - val_loss: 82.0505 - val_mse: 82.0505 - val_mae: 6.3995\n",
      "Epoch 575/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.6818 - mse: 85.6818 - mae: 6.4254 - val_loss: 82.0544 - val_mse: 82.0544 - val_mae: 6.4090\n",
      "Epoch 576/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.8343 - mse: 85.8344 - mae: 6.4245 - val_loss: 81.3222 - val_mse: 81.3223 - val_mae: 6.4319\n",
      "Epoch 577/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.7317 - mse: 85.7318 - mae: 6.4224 - val_loss: 81.4038 - val_mse: 81.4038 - val_mae: 6.4708\n",
      "Epoch 578/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.7508 - mse: 85.7508 - mae: 6.4249 - val_loss: 81.2814 - val_mse: 81.2815 - val_mae: 6.4884\n",
      "Epoch 579/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.7733 - mse: 85.7733 - mae: 6.4294 - val_loss: 81.2403 - val_mse: 81.2403 - val_mae: 6.4553\n",
      "Epoch 580/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.7889 - mse: 85.7889 - mae: 6.4240 - val_loss: 81.5355 - val_mse: 81.5355 - val_mae: 6.4521\n",
      "Epoch 581/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.8350 - mse: 85.8350 - mae: 6.4316 - val_loss: 81.3958 - val_mse: 81.3957 - val_mae: 6.4738\n",
      "Epoch 582/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.8455 - mse: 85.8455 - mae: 6.4236 - val_loss: 81.2885 - val_mse: 81.2886 - val_mae: 6.4368\n",
      "Epoch 583/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7828 - mse: 85.7827 - mae: 6.4238 - val_loss: 81.9204 - val_mse: 81.9204 - val_mae: 6.3941\n",
      "Epoch 584/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.7060 - mse: 85.7060 - mae: 6.4199 - val_loss: 81.6347 - val_mse: 81.6347 - val_mae: 6.4021\n",
      "Epoch 585/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7533 - mse: 85.7533 - mae: 6.4275 - val_loss: 81.2553 - val_mse: 81.2553 - val_mae: 6.4673\n",
      "Epoch 586/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8750 - mse: 85.8750 - mae: 6.4291 - val_loss: 81.2433 - val_mse: 81.2433 - val_mae: 6.4305\n",
      "Epoch 587/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.6292 - mse: 85.6291 - mae: 6.4080 - val_loss: 83.2784 - val_mse: 83.2784 - val_mae: 6.7198\n",
      "Epoch 588/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.8031 - mse: 85.8032 - mae: 6.4297 - val_loss: 82.0375 - val_mse: 82.0376 - val_mae: 6.5632\n",
      "Epoch 589/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.9029 - mse: 85.9029 - mae: 6.4301 - val_loss: 81.3735 - val_mse: 81.3735 - val_mae: 6.4869\n",
      "Epoch 590/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7121 - mse: 85.7121 - mae: 6.4328 - val_loss: 81.4173 - val_mse: 81.4173 - val_mae: 6.4345\n",
      "Epoch 591/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.8388 - mse: 85.8388 - mae: 6.4262 - val_loss: 81.2521 - val_mse: 81.2521 - val_mae: 6.4604\n",
      "Epoch 592/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7461 - mse: 85.7461 - mae: 6.4245 - val_loss: 81.5275 - val_mse: 81.5275 - val_mae: 6.5647\n",
      "Epoch 593/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7418 - mse: 85.7418 - mae: 6.4236 - val_loss: 81.4774 - val_mse: 81.4774 - val_mae: 6.5520\n",
      "Epoch 594/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8216 - mse: 85.8217 - mae: 6.4314 - val_loss: 81.5648 - val_mse: 81.5648 - val_mae: 6.5416\n",
      "Epoch 595/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7465 - mse: 85.7465 - mae: 6.4193 - val_loss: 82.0349 - val_mse: 82.0350 - val_mae: 6.5741\n",
      "Epoch 596/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7737 - mse: 85.7737 - mae: 6.4327 - val_loss: 81.9203 - val_mse: 81.9203 - val_mae: 6.4406\n",
      "Epoch 597/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7435 - mse: 85.7434 - mae: 6.4207 - val_loss: 81.5126 - val_mse: 81.5126 - val_mae: 6.5456\n",
      "Epoch 598/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7792 - mse: 85.7792 - mae: 6.4324 - val_loss: 81.2944 - val_mse: 81.2944 - val_mae: 6.5025\n",
      "Epoch 599/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8064 - mse: 85.8063 - mae: 6.4258 - val_loss: 81.1979 - val_mse: 81.1979 - val_mae: 6.4513\n",
      "Epoch 600/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8303 - mse: 85.8304 - mae: 6.4291 - val_loss: 81.4031 - val_mse: 81.4031 - val_mae: 6.4259\n",
      "Epoch 601/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7188 - mse: 85.7188 - mae: 6.4257 - val_loss: 81.4871 - val_mse: 81.4871 - val_mae: 6.5818\n",
      "Epoch 602/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.7870 - mse: 85.7869 - mae: 6.4326 - val_loss: 81.5096 - val_mse: 81.5097 - val_mae: 6.5123\n",
      "Epoch 603/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.8280 - mse: 85.8280 - mae: 6.4276 - val_loss: 81.6130 - val_mse: 81.6130 - val_mae: 6.5033\n",
      "Epoch 604/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7218 - mse: 85.7218 - mae: 6.4192 - val_loss: 81.3370 - val_mse: 81.3370 - val_mae: 6.4239\n",
      "Epoch 605/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.8079 - mse: 85.8080 - mae: 6.4223 - val_loss: 81.2516 - val_mse: 81.2516 - val_mae: 6.4783\n",
      "Epoch 606/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.8328 - mse: 85.8329 - mae: 6.4314 - val_loss: 82.0318 - val_mse: 82.0318 - val_mae: 6.4006\n",
      "Epoch 607/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6733 - mse: 85.6734 - mae: 6.4294 - val_loss: 81.9823 - val_mse: 81.9824 - val_mae: 6.5646\n",
      "Epoch 608/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.7386 - mse: 85.7387 - mae: 6.4180 - val_loss: 81.6496 - val_mse: 81.6496 - val_mae: 6.4066\n",
      "Epoch 609/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7478 - mse: 85.7478 - mae: 6.4262 - val_loss: 81.2153 - val_mse: 81.2153 - val_mae: 6.4538\n",
      "Epoch 610/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6703 - mse: 85.6703 - mae: 6.4277 - val_loss: 81.5800 - val_mse: 81.5800 - val_mae: 6.4058\n",
      "Epoch 611/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.7850 - mse: 85.7850 - mae: 6.4328 - val_loss: 81.3710 - val_mse: 81.3709 - val_mae: 6.4393\n",
      "Epoch 612/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.7224 - mse: 85.7223 - mae: 6.4202 - val_loss: 83.0350 - val_mse: 83.0350 - val_mae: 6.6733\n",
      "Epoch 613/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.7901 - mse: 85.7901 - mae: 6.4273 - val_loss: 81.2662 - val_mse: 81.2662 - val_mae: 6.4702\n",
      "Epoch 614/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6577 - mse: 85.6577 - mae: 6.4244 - val_loss: 81.5157 - val_mse: 81.5157 - val_mae: 6.4359\n",
      "Epoch 615/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7290 - mse: 85.7289 - mae: 6.4271 - val_loss: 81.7439 - val_mse: 81.7439 - val_mae: 6.3863\n",
      "Epoch 616/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.5848 - mse: 85.5848 - mae: 6.4108 - val_loss: 82.4001 - val_mse: 82.4001 - val_mae: 6.3714\n",
      "Epoch 617/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8210 - mse: 85.8210 - mae: 6.4233 - val_loss: 81.3473 - val_mse: 81.3473 - val_mae: 6.4268\n",
      "Epoch 618/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6562 - mse: 85.6562 - mae: 6.4208 - val_loss: 81.7571 - val_mse: 81.7571 - val_mae: 6.4844\n",
      "Epoch 619/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.7333 - mse: 85.7333 - mae: 6.4246 - val_loss: 81.4218 - val_mse: 81.4218 - val_mae: 6.4653\n",
      "Epoch 620/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6859 - mse: 85.6860 - mae: 6.4135 - val_loss: 81.6332 - val_mse: 81.6332 - val_mae: 6.5050\n",
      "Epoch 621/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6960 - mse: 85.6960 - mae: 6.4236 - val_loss: 82.4154 - val_mse: 82.4154 - val_mae: 6.3986\n",
      "Epoch 622/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7691 - mse: 85.7691 - mae: 6.4252 - val_loss: 81.4409 - val_mse: 81.4409 - val_mae: 6.5050\n",
      "Epoch 623/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.8052 - mse: 85.8052 - mae: 6.4212 - val_loss: 81.3675 - val_mse: 81.3674 - val_mae: 6.4712\n",
      "Epoch 624/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7380 - mse: 85.7380 - mae: 6.4227 - val_loss: 81.9735 - val_mse: 81.9735 - val_mae: 6.5522\n",
      "Epoch 625/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.8243 - mse: 85.8243 - mae: 6.4303 - val_loss: 81.5857 - val_mse: 81.5858 - val_mae: 6.3997\n",
      "Epoch 626/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.7112 - mse: 85.7112 - mae: 6.4242 - val_loss: 81.6826 - val_mse: 81.6826 - val_mae: 6.5248\n",
      "Epoch 627/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.6517 - mse: 85.6517 - mae: 6.4238 - val_loss: 82.3074 - val_mse: 82.3074 - val_mae: 6.5663\n",
      "Epoch 628/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7015 - mse: 85.7015 - mae: 6.4324 - val_loss: 81.2902 - val_mse: 81.2902 - val_mae: 6.4634\n",
      "Epoch 629/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.6493 - mse: 85.6493 - mae: 6.4258 - val_loss: 82.4118 - val_mse: 82.4118 - val_mae: 6.3731\n",
      "Epoch 630/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8195 - mse: 85.8195 - mae: 6.4233 - val_loss: 81.1639 - val_mse: 81.1639 - val_mae: 6.4380\n",
      "Epoch 631/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.6950 - mse: 85.6950 - mae: 6.4279 - val_loss: 81.4199 - val_mse: 81.4200 - val_mae: 6.4569\n",
      "Epoch 632/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7395 - mse: 85.7395 - mae: 6.4305 - val_loss: 81.9623 - val_mse: 81.9622 - val_mae: 6.3861\n",
      "Epoch 633/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7551 - mse: 85.7551 - mae: 6.4189 - val_loss: 82.3468 - val_mse: 82.3468 - val_mae: 6.6400\n",
      "Epoch 634/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.8256 - mse: 85.8256 - mae: 6.4349 - val_loss: 81.3493 - val_mse: 81.3493 - val_mae: 6.4329\n",
      "Epoch 635/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7334 - mse: 85.7334 - mae: 6.4223 - val_loss: 81.8539 - val_mse: 81.8539 - val_mae: 6.5120\n",
      "Epoch 636/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.7854 - mse: 85.7854 - mae: 6.4275 - val_loss: 81.5324 - val_mse: 81.5324 - val_mae: 6.4107\n",
      "Epoch 637/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6988 - mse: 85.6988 - mae: 6.4252 - val_loss: 81.7060 - val_mse: 81.7060 - val_mae: 6.5359\n",
      "Epoch 638/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8124 - mse: 85.8124 - mae: 6.4262 - val_loss: 81.9335 - val_mse: 81.9335 - val_mae: 6.5864\n",
      "Epoch 639/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7366 - mse: 85.7366 - mae: 6.4277 - val_loss: 81.3298 - val_mse: 81.3298 - val_mae: 6.4591\n",
      "Epoch 640/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.7849 - mse: 85.7849 - mae: 6.4262 - val_loss: 81.9893 - val_mse: 81.9892 - val_mae: 6.5337\n",
      "Epoch 641/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.8374 - mse: 85.8373 - mae: 6.4250 - val_loss: 81.2749 - val_mse: 81.2749 - val_mae: 6.4769\n",
      "Epoch 642/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7172 - mse: 85.7172 - mae: 6.4205 - val_loss: 81.7949 - val_mse: 81.7949 - val_mae: 6.5127\n",
      "Epoch 643/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7396 - mse: 85.7397 - mae: 6.4248 - val_loss: 81.3269 - val_mse: 81.3269 - val_mae: 6.4707\n",
      "Epoch 644/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7092 - mse: 85.7092 - mae: 6.4233 - val_loss: 81.9889 - val_mse: 81.9889 - val_mae: 6.4642\n",
      "Epoch 645/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.8303 - mse: 85.8303 - mae: 6.4224 - val_loss: 81.3385 - val_mse: 81.3384 - val_mae: 6.4401\n",
      "Epoch 646/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.8418 - mse: 85.8418 - mae: 6.4311 - val_loss: 81.3167 - val_mse: 81.3167 - val_mae: 6.4697\n",
      "Epoch 647/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.8045 - mse: 85.8044 - mae: 6.4243 - val_loss: 81.2540 - val_mse: 81.2540 - val_mae: 6.4959\n",
      "Epoch 648/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7166 - mse: 85.7166 - mae: 6.4164 - val_loss: 81.7359 - val_mse: 81.7359 - val_mae: 6.5770\n",
      "Epoch 649/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7736 - mse: 85.7735 - mae: 6.4337 - val_loss: 81.1874 - val_mse: 81.1874 - val_mae: 6.4478\n",
      "Epoch 650/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7773 - mse: 85.7773 - mae: 6.4287 - val_loss: 81.3469 - val_mse: 81.3469 - val_mae: 6.4666\n",
      "Epoch 651/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7486 - mse: 85.7486 - mae: 6.4310 - val_loss: 81.3875 - val_mse: 81.3875 - val_mae: 6.4505\n",
      "Epoch 652/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 85.7845 - mse: 85.7845 - mae: 6.4267 - val_loss: 81.3298 - val_mse: 81.3298 - val_mae: 6.4248\n",
      "Epoch 653/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7350 - mse: 85.7351 - mae: 6.4183 - val_loss: 81.6494 - val_mse: 81.6495 - val_mae: 6.5428\n",
      "Epoch 654/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7370 - mse: 85.7370 - mae: 6.4359 - val_loss: 81.8951 - val_mse: 81.8951 - val_mae: 6.3928\n",
      "Epoch 655/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6949 - mse: 85.6948 - mae: 6.4254 - val_loss: 82.5753 - val_mse: 82.5753 - val_mae: 6.4322\n",
      "Epoch 656/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.7860 - mse: 85.7860 - mae: 6.4312 - val_loss: 81.2824 - val_mse: 81.2824 - val_mae: 6.4325\n",
      "Epoch 657/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7603 - mse: 85.7604 - mae: 6.4268 - val_loss: 81.8542 - val_mse: 81.8542 - val_mae: 6.5330\n",
      "Epoch 658/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.6000 - mse: 85.5999 - mae: 6.4333 - val_loss: 82.2268 - val_mse: 82.2268 - val_mae: 6.3655\n",
      "Epoch 659/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6289 - mse: 85.6288 - mae: 6.4248 - val_loss: 82.0242 - val_mse: 82.0242 - val_mae: 6.3986\n",
      "Epoch 660/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7082 - mse: 85.7083 - mae: 6.4214 - val_loss: 83.3793 - val_mse: 83.3792 - val_mae: 6.7009\n",
      "Epoch 661/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7116 - mse: 85.7116 - mae: 6.4336 - val_loss: 81.5810 - val_mse: 81.5810 - val_mae: 6.4032\n",
      "Epoch 662/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7047 - mse: 85.7047 - mae: 6.4330 - val_loss: 81.3786 - val_mse: 81.3786 - val_mae: 6.4545\n",
      "Epoch 663/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7696 - mse: 85.7695 - mae: 6.4223 - val_loss: 81.7223 - val_mse: 81.7223 - val_mae: 6.5240\n",
      "Epoch 664/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7170 - mse: 85.7170 - mae: 6.4295 - val_loss: 81.3364 - val_mse: 81.3364 - val_mae: 6.4781\n",
      "Epoch 665/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.6909 - mse: 85.6909 - mae: 6.4269 - val_loss: 81.5275 - val_mse: 81.5275 - val_mae: 6.4363\n",
      "Epoch 666/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.6841 - mse: 85.6841 - mae: 6.4321 - val_loss: 81.4307 - val_mse: 81.4307 - val_mae: 6.5106\n",
      "Epoch 667/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.5294 - mse: 85.5294 - mae: 6.4185 - val_loss: 82.2840 - val_mse: 82.2840 - val_mae: 6.5772\n",
      "Epoch 668/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6813 - mse: 85.6812 - mae: 6.4244 - val_loss: 81.3449 - val_mse: 81.3449 - val_mae: 6.4829\n",
      "Epoch 669/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7471 - mse: 85.7471 - mae: 6.4240 - val_loss: 82.4678 - val_mse: 82.4679 - val_mae: 6.6071\n",
      "Epoch 670/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7747 - mse: 85.7747 - mae: 6.4297 - val_loss: 81.5478 - val_mse: 81.5478 - val_mae: 6.4231\n",
      "Epoch 671/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.7599 - mse: 85.7598 - mae: 6.4324 - val_loss: 82.4496 - val_mse: 82.4496 - val_mae: 6.3771\n",
      "Epoch 672/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.8865 - mse: 85.8865 - mae: 6.4247 - val_loss: 81.3631 - val_mse: 81.3631 - val_mae: 6.4832\n",
      "Epoch 673/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.5864 - mse: 85.5865 - mae: 6.4206 - val_loss: 82.2153 - val_mse: 82.2153 - val_mae: 6.5506\n",
      "Epoch 674/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 85.5903 - mse: 85.5903 - mae: 6.4191 - val_loss: 81.6846 - val_mse: 81.6846 - val_mae: 6.4461\n",
      "Epoch 675/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.6975 - mse: 85.6975 - mae: 6.4207 - val_loss: 81.5348 - val_mse: 81.5348 - val_mae: 6.4416\n",
      "Epoch 676/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6716 - mse: 85.6717 - mae: 6.4268 - val_loss: 81.8685 - val_mse: 81.8685 - val_mae: 6.5041\n",
      "Epoch 677/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7460 - mse: 85.7460 - mae: 6.4294 - val_loss: 82.0281 - val_mse: 82.0281 - val_mae: 6.4078\n",
      "Epoch 678/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.8804 - mse: 85.8804 - mae: 6.4304 - val_loss: 81.4233 - val_mse: 81.4233 - val_mae: 6.4173\n",
      "Epoch 679/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.6328 - mse: 85.6329 - mae: 6.4233 - val_loss: 81.8687 - val_mse: 81.8687 - val_mae: 6.4153\n",
      "Epoch 680/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7720 - mse: 85.7720 - mae: 6.4255 - val_loss: 81.5483 - val_mse: 81.5482 - val_mae: 6.4291\n",
      "Epoch 681/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 85.7508 - mse: 85.7508 - mae: 6.4282 - val_loss: 81.4993 - val_mse: 81.4993 - val_mae: 6.4251\n",
      "Epoch 682/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.7398 - mse: 85.7398 - mae: 6.4240 - val_loss: 81.3563 - val_mse: 81.3563 - val_mae: 6.4592\n",
      "Epoch 683/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.6303 - mse: 85.6303 - mae: 6.4207 - val_loss: 82.4768 - val_mse: 82.4768 - val_mae: 6.5876\n",
      "Epoch 684/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6926 - mse: 85.6925 - mae: 6.4275 - val_loss: 81.3675 - val_mse: 81.3675 - val_mae: 6.4255\n",
      "Epoch 685/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7057 - mse: 85.7056 - mae: 6.4245 - val_loss: 81.5010 - val_mse: 81.5010 - val_mae: 6.4270\n",
      "Epoch 686/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7745 - mse: 85.7745 - mae: 6.4288 - val_loss: 81.3775 - val_mse: 81.3775 - val_mae: 6.4240\n",
      "Epoch 687/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7401 - mse: 85.7401 - mae: 6.4185 - val_loss: 81.2839 - val_mse: 81.2838 - val_mae: 6.4529\n",
      "Epoch 688/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.8182 - mse: 85.8183 - mae: 6.4270 - val_loss: 81.4727 - val_mse: 81.4727 - val_mae: 6.5125\n",
      "Epoch 689/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.7906 - mse: 85.7907 - mae: 6.4228 - val_loss: 82.0835 - val_mse: 82.0835 - val_mae: 6.3987\n",
      "Epoch 690/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.6896 - mse: 85.6895 - mae: 6.4235 - val_loss: 81.5401 - val_mse: 81.5401 - val_mae: 6.4223\n",
      "Epoch 691/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6454 - mse: 85.6454 - mae: 6.4193 - val_loss: 81.5949 - val_mse: 81.5949 - val_mae: 6.4527\n",
      "Epoch 692/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7107 - mse: 85.7106 - mae: 6.4285 - val_loss: 81.2765 - val_mse: 81.2765 - val_mae: 6.4808\n",
      "Epoch 693/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7330 - mse: 85.7330 - mae: 6.4254 - val_loss: 81.6064 - val_mse: 81.6064 - val_mae: 6.4517\n",
      "Epoch 694/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.6709 - mse: 85.6709 - mae: 6.4258 - val_loss: 81.8404 - val_mse: 81.8404 - val_mae: 6.5156\n",
      "Epoch 695/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.6647 - mse: 85.6647 - mae: 6.4293 - val_loss: 81.6333 - val_mse: 81.6333 - val_mae: 6.4513\n",
      "Epoch 696/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.8126 - mse: 85.8126 - mae: 6.4360 - val_loss: 81.7740 - val_mse: 81.7741 - val_mae: 6.5197\n",
      "Epoch 697/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.6757 - mse: 85.6758 - mae: 6.4300 - val_loss: 81.5841 - val_mse: 81.5841 - val_mae: 6.4015\n",
      "Epoch 698/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7318 - mse: 85.7317 - mae: 6.4312 - val_loss: 81.4053 - val_mse: 81.4053 - val_mae: 6.4388\n",
      "Epoch 699/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 85.6597 - mse: 85.6598 - mae: 6.4199 - val_loss: 82.4642 - val_mse: 82.4642 - val_mae: 6.5850\n",
      "Epoch 700/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.5573 - mse: 85.5572 - mae: 6.4212 - val_loss: 82.4400 - val_mse: 82.4400 - val_mae: 6.4018\n",
      "Epoch 701/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7860 - mse: 85.7860 - mae: 6.4223 - val_loss: 81.4448 - val_mse: 81.4448 - val_mae: 6.4995\n",
      "Epoch 702/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7496 - mse: 85.7497 - mae: 6.4266 - val_loss: 81.7071 - val_mse: 81.7071 - val_mae: 6.4899\n",
      "Epoch 703/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7474 - mse: 85.7474 - mae: 6.4252 - val_loss: 81.8449 - val_mse: 81.8449 - val_mae: 6.4248\n",
      "Epoch 704/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.7290 - mse: 85.7290 - mae: 6.4267 - val_loss: 81.4588 - val_mse: 81.4589 - val_mae: 6.4397\n",
      "Epoch 705/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7480 - mse: 85.7480 - mae: 6.4237 - val_loss: 81.5178 - val_mse: 81.5178 - val_mae: 6.5296\n",
      "Epoch 706/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7588 - mse: 85.7589 - mae: 6.4258 - val_loss: 81.6207 - val_mse: 81.6207 - val_mae: 6.4101\n",
      "Epoch 707/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7513 - mse: 85.7513 - mae: 6.4267 - val_loss: 81.5992 - val_mse: 81.5992 - val_mae: 6.4112\n",
      "Epoch 708/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7333 - mse: 85.7333 - mae: 6.4291 - val_loss: 81.5197 - val_mse: 81.5197 - val_mae: 6.3863\n",
      "Epoch 709/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6646 - mse: 85.6647 - mae: 6.4166 - val_loss: 81.3630 - val_mse: 81.3630 - val_mae: 6.4661\n",
      "Epoch 710/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.6363 - mse: 85.6364 - mae: 6.4291 - val_loss: 81.3864 - val_mse: 81.3864 - val_mae: 6.4812\n",
      "Epoch 711/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.8301 - mse: 85.8301 - mae: 6.4285 - val_loss: 81.1836 - val_mse: 81.1836 - val_mae: 6.4692\n",
      "Epoch 712/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6992 - mse: 85.6993 - mae: 6.4226 - val_loss: 82.2261 - val_mse: 82.2261 - val_mae: 6.5879\n",
      "Epoch 713/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 85.8055 - mse: 85.8055 - mae: 6.4284 - val_loss: 81.3741 - val_mse: 81.3740 - val_mae: 6.4097\n",
      "Epoch 714/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.8180 - mse: 85.8180 - mae: 6.4286 - val_loss: 81.5319 - val_mse: 81.5318 - val_mae: 6.4650\n",
      "Epoch 715/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.5668 - mse: 85.5668 - mae: 6.4160 - val_loss: 82.1981 - val_mse: 82.1981 - val_mae: 6.5485\n",
      "Epoch 716/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 85.7745 - mse: 85.7745 - mae: 6.4334 - val_loss: 81.3767 - val_mse: 81.3767 - val_mae: 6.4108\n",
      "Epoch 717/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 85.6674 - mse: 85.6674 - mae: 6.4204 - val_loss: 81.3775 - val_mse: 81.3775 - val_mae: 6.5002\n",
      "Epoch 718/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7180 - mse: 85.7180 - mae: 6.4215 - val_loss: 81.8903 - val_mse: 81.8903 - val_mae: 6.5763\n",
      "Epoch 719/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6700 - mse: 85.6700 - mae: 6.4327 - val_loss: 81.5300 - val_mse: 81.5300 - val_mae: 6.3909\n",
      "Epoch 720/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7401 - mse: 85.7401 - mae: 6.4218 - val_loss: 81.6524 - val_mse: 81.6524 - val_mae: 6.3999\n",
      "Epoch 721/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.8289 - mse: 85.8289 - mae: 6.4259 - val_loss: 82.3296 - val_mse: 82.3296 - val_mae: 6.6382\n",
      "Epoch 722/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7596 - mse: 85.7597 - mae: 6.4322 - val_loss: 81.5958 - val_mse: 81.5958 - val_mae: 6.4924\n",
      "Epoch 723/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7906 - mse: 85.7906 - mae: 6.4320 - val_loss: 81.4593 - val_mse: 81.4593 - val_mae: 6.4567\n",
      "Epoch 724/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.7459 - mse: 85.7458 - mae: 6.4334 - val_loss: 81.5011 - val_mse: 81.5011 - val_mae: 6.4095\n",
      "Epoch 725/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.7083 - mse: 85.7083 - mae: 6.4182 - val_loss: 81.6867 - val_mse: 81.6867 - val_mae: 6.5596\n",
      "Epoch 726/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.6389 - mse: 85.6389 - mae: 6.4248 - val_loss: 83.5614 - val_mse: 83.5614 - val_mae: 6.7169\n",
      "Epoch 727/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6198 - mse: 85.6198 - mae: 6.4256 - val_loss: 81.7339 - val_mse: 81.7339 - val_mae: 6.4018\n",
      "Epoch 728/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7660 - mse: 85.7660 - mae: 6.4241 - val_loss: 81.4762 - val_mse: 81.4762 - val_mae: 6.4957\n",
      "Epoch 729/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.8242 - mse: 85.8241 - mae: 6.4356 - val_loss: 81.9293 - val_mse: 81.9293 - val_mae: 6.3747\n",
      "Epoch 730/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.6847 - mse: 85.6846 - mae: 6.4212 - val_loss: 81.4859 - val_mse: 81.4858 - val_mae: 6.4164\n",
      "Epoch 731/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.5917 - mse: 85.5918 - mae: 6.4295 - val_loss: 81.3274 - val_mse: 81.3274 - val_mae: 6.4403\n",
      "Epoch 732/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.7855 - mse: 85.7855 - mae: 6.4230 - val_loss: 81.3686 - val_mse: 81.3686 - val_mae: 6.4792\n",
      "Epoch 733/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.7173 - mse: 85.7172 - mae: 6.4193 - val_loss: 82.3609 - val_mse: 82.3609 - val_mae: 6.5956\n",
      "Epoch 734/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 85.7136 - mse: 85.7135 - mae: 6.4306 - val_loss: 81.4229 - val_mse: 81.4229 - val_mae: 6.4956\n",
      "Epoch 735/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.7212 - mse: 85.7212 - mae: 6.4260 - val_loss: 81.6970 - val_mse: 81.6970 - val_mae: 6.5362\n",
      "Epoch 736/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.7033 - mse: 85.7032 - mae: 6.4268 - val_loss: 81.7613 - val_mse: 81.7614 - val_mae: 6.4140\n",
      "Epoch 737/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7970 - mse: 85.7969 - mae: 6.4225 - val_loss: 82.7037 - val_mse: 82.7037 - val_mae: 6.6394\n",
      "Epoch 738/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.8119 - mse: 85.8119 - mae: 6.4293 - val_loss: 81.2567 - val_mse: 81.2567 - val_mae: 6.4482\n",
      "Epoch 739/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.6677 - mse: 85.6677 - mae: 6.4179 - val_loss: 81.4902 - val_mse: 81.4902 - val_mae: 6.4739\n",
      "Epoch 740/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7044 - mse: 85.7044 - mae: 6.4299 - val_loss: 81.5784 - val_mse: 81.5784 - val_mae: 6.4308\n",
      "Epoch 741/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.6501 - mse: 85.6502 - mae: 6.4236 - val_loss: 81.5411 - val_mse: 81.5412 - val_mae: 6.4494\n",
      "Epoch 742/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.6346 - mse: 85.6345 - mae: 6.4223 - val_loss: 81.6963 - val_mse: 81.6963 - val_mae: 6.5016\n",
      "Epoch 743/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.7181 - mse: 85.7181 - mae: 6.4284 - val_loss: 81.1969 - val_mse: 81.1969 - val_mae: 6.4513\n",
      "Epoch 744/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.6811 - mse: 85.6810 - mae: 6.4222 - val_loss: 81.2220 - val_mse: 81.2220 - val_mae: 6.4995\n",
      "Epoch 745/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7183 - mse: 85.7183 - mae: 6.4221 - val_loss: 81.1951 - val_mse: 81.1951 - val_mae: 6.4611\n",
      "Epoch 746/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.6319 - mse: 85.6319 - mae: 6.4225 - val_loss: 81.6531 - val_mse: 81.6531 - val_mae: 6.4258\n",
      "Epoch 747/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6907 - mse: 85.6908 - mae: 6.4236 - val_loss: 81.7486 - val_mse: 81.7485 - val_mae: 6.4023\n",
      "Epoch 748/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.7261 - mse: 85.7261 - mae: 6.4194 - val_loss: 81.1995 - val_mse: 81.1995 - val_mae: 6.4653\n",
      "Epoch 749/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7045 - mse: 85.7045 - mae: 6.4170 - val_loss: 82.2259 - val_mse: 82.2258 - val_mae: 6.6379\n",
      "Epoch 750/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7070 - mse: 85.7071 - mae: 6.4268 - val_loss: 82.8755 - val_mse: 82.8755 - val_mae: 6.6268\n",
      "Epoch 751/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6892 - mse: 85.6892 - mae: 6.4253 - val_loss: 81.5038 - val_mse: 81.5038 - val_mae: 6.5196\n",
      "Epoch 752/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6857 - mse: 85.6858 - mae: 6.4315 - val_loss: 81.6853 - val_mse: 81.6852 - val_mae: 6.4289\n",
      "Epoch 753/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6627 - mse: 85.6627 - mae: 6.4227 - val_loss: 81.4440 - val_mse: 81.4440 - val_mae: 6.5611\n",
      "Epoch 754/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.7573 - mse: 85.7574 - mae: 6.4279 - val_loss: 81.6371 - val_mse: 81.6371 - val_mae: 6.4908\n",
      "Epoch 755/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6815 - mse: 85.6816 - mae: 6.4129 - val_loss: 81.4669 - val_mse: 81.4669 - val_mae: 6.4994\n",
      "Epoch 756/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7013 - mse: 85.7013 - mae: 6.4299 - val_loss: 81.5778 - val_mse: 81.5778 - val_mae: 6.4173\n",
      "Epoch 757/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.8537 - mse: 85.8537 - mae: 6.4260 - val_loss: 81.3668 - val_mse: 81.3669 - val_mae: 6.4724\n",
      "Epoch 758/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6970 - mse: 85.6971 - mae: 6.4278 - val_loss: 81.2860 - val_mse: 81.2860 - val_mae: 6.4226\n",
      "Epoch 759/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7260 - mse: 85.7261 - mae: 6.4284 - val_loss: 81.4609 - val_mse: 81.4609 - val_mae: 6.4309\n",
      "Epoch 760/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.6963 - mse: 85.6963 - mae: 6.4209 - val_loss: 82.0908 - val_mse: 82.0908 - val_mae: 6.5734\n",
      "Epoch 761/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7549 - mse: 85.7548 - mae: 6.4180 - val_loss: 81.2755 - val_mse: 81.2755 - val_mae: 6.4683\n",
      "Epoch 762/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.7125 - mse: 85.7126 - mae: 6.4285 - val_loss: 81.0799 - val_mse: 81.0799 - val_mae: 6.4175\n",
      "Epoch 763/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7337 - mse: 85.7337 - mae: 6.4212 - val_loss: 81.1479 - val_mse: 81.1479 - val_mae: 6.4496\n",
      "Epoch 764/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6428 - mse: 85.6428 - mae: 6.4177 - val_loss: 81.6864 - val_mse: 81.6864 - val_mae: 6.3894\n",
      "Epoch 765/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.8081 - mse: 85.8081 - mae: 6.4264 - val_loss: 81.4524 - val_mse: 81.4525 - val_mae: 6.4398\n",
      "Epoch 766/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7890 - mse: 85.7890 - mae: 6.4283 - val_loss: 82.0087 - val_mse: 82.0086 - val_mae: 6.5228\n",
      "Epoch 767/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.6549 - mse: 85.6550 - mae: 6.4206 - val_loss: 81.9859 - val_mse: 81.9859 - val_mae: 6.3868\n",
      "Epoch 768/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.7207 - mse: 85.7207 - mae: 6.4232 - val_loss: 81.4336 - val_mse: 81.4337 - val_mae: 6.5053\n",
      "Epoch 769/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.5968 - mse: 85.5969 - mae: 6.4155 - val_loss: 81.3780 - val_mse: 81.3780 - val_mae: 6.4622\n",
      "Epoch 770/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 85.7243 - mse: 85.7243 - mae: 6.4228 - val_loss: 81.4121 - val_mse: 81.4121 - val_mae: 6.4040\n",
      "Epoch 771/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7297 - mse: 85.7297 - mae: 6.4245 - val_loss: 81.1251 - val_mse: 81.1251 - val_mae: 6.4106\n",
      "Epoch 772/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6624 - mse: 85.6624 - mae: 6.4260 - val_loss: 81.5264 - val_mse: 81.5264 - val_mae: 6.4224\n",
      "Epoch 773/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.6430 - mse: 85.6430 - mae: 6.4214 - val_loss: 81.4780 - val_mse: 81.4780 - val_mae: 6.4974\n",
      "Epoch 774/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.7229 - mse: 85.7229 - mae: 6.4241 - val_loss: 81.2594 - val_mse: 81.2595 - val_mae: 6.4467\n",
      "Epoch 775/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.7128 - mse: 85.7127 - mae: 6.4333 - val_loss: 81.7727 - val_mse: 81.7728 - val_mae: 6.4021\n",
      "Epoch 776/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 85.7494 - mse: 85.7494 - mae: 6.4280 - val_loss: 81.2197 - val_mse: 81.2197 - val_mae: 6.4779\n",
      "Epoch 777/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.8092 - mse: 85.8091 - mae: 6.4318 - val_loss: 81.5970 - val_mse: 81.5971 - val_mae: 6.4929\n",
      "Epoch 778/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 85.6234 - mse: 85.6233 - mae: 6.4235 - val_loss: 81.5111 - val_mse: 81.5111 - val_mae: 6.4180\n",
      "Epoch 779/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6163 - mse: 85.6162 - mae: 6.4180 - val_loss: 81.6759 - val_mse: 81.6759 - val_mae: 6.4456\n",
      "Epoch 780/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6392 - mse: 85.6391 - mae: 6.4193 - val_loss: 82.0441 - val_mse: 82.0441 - val_mae: 6.5676\n",
      "Epoch 781/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6802 - mse: 85.6802 - mae: 6.4220 - val_loss: 81.2863 - val_mse: 81.2862 - val_mae: 6.5317\n",
      "Epoch 782/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 85.7392 - mse: 85.7393 - mae: 6.4276 - val_loss: 81.4404 - val_mse: 81.4404 - val_mae: 6.4821\n",
      "Epoch 783/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 3s 179us/step - loss: 85.6061 - mse: 85.6060 - mae: 6.4250 - val_loss: 82.0596 - val_mse: 82.0596 - val_mae: 6.4051\n",
      "Epoch 784/1000\n",
      "14194/14194 [==============================] - 3s 205us/step - loss: 85.6163 - mse: 85.6162 - mae: 6.4309 - val_loss: 81.6401 - val_mse: 81.6401 - val_mae: 6.4256\n",
      "Epoch 785/1000\n",
      "14194/14194 [==============================] - 3s 191us/step - loss: 85.7616 - mse: 85.7616 - mae: 6.4253 - val_loss: 81.2434 - val_mse: 81.2434 - val_mae: 6.4882\n",
      "Epoch 786/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 85.6482 - mse: 85.6482 - mae: 6.4188 - val_loss: 81.5336 - val_mse: 81.5336 - val_mae: 6.4540\n",
      "Epoch 787/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7651 - mse: 85.7651 - mae: 6.4302 - val_loss: 81.4636 - val_mse: 81.4636 - val_mae: 6.4680\n",
      "Epoch 788/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.5921 - mse: 85.5921 - mae: 6.4260 - val_loss: 81.2891 - val_mse: 81.2891 - val_mae: 6.4286\n",
      "Epoch 789/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.6546 - mse: 85.6545 - mae: 6.4149 - val_loss: 81.3902 - val_mse: 81.3903 - val_mae: 6.4460\n",
      "Epoch 790/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6280 - mse: 85.6279 - mae: 6.4162 - val_loss: 81.6166 - val_mse: 81.6167 - val_mae: 6.5238\n",
      "Epoch 791/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.6713 - mse: 85.6713 - mae: 6.4325 - val_loss: 82.3347 - val_mse: 82.3347 - val_mae: 6.4262\n",
      "Epoch 792/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.6230 - mse: 85.6229 - mae: 6.4224 - val_loss: 81.5164 - val_mse: 81.5165 - val_mae: 6.4158\n",
      "Epoch 793/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.6025 - mse: 85.6025 - mae: 6.4238 - val_loss: 82.1086 - val_mse: 82.1086 - val_mae: 6.3916\n",
      "Epoch 794/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 85.7229 - mse: 85.7230 - mae: 6.4282 - val_loss: 81.3001 - val_mse: 81.3001 - val_mae: 6.4386\n",
      "Epoch 795/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6553 - mse: 85.6553 - mae: 6.4284 - val_loss: 81.4591 - val_mse: 81.4591 - val_mae: 6.4284\n",
      "Epoch 796/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6608 - mse: 85.6607 - mae: 6.4258 - val_loss: 81.3037 - val_mse: 81.3037 - val_mae: 6.4793\n",
      "Epoch 797/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.6358 - mse: 85.6357 - mae: 6.4207 - val_loss: 81.6264 - val_mse: 81.6264 - val_mae: 6.4677\n",
      "Epoch 798/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.7768 - mse: 85.7769 - mae: 6.4309 - val_loss: 81.6082 - val_mse: 81.6082 - val_mae: 6.4932\n",
      "Epoch 799/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6959 - mse: 85.6959 - mae: 6.4281 - val_loss: 81.5471 - val_mse: 81.5471 - val_mae: 6.4136\n",
      "Epoch 800/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7577 - mse: 85.7576 - mae: 6.4216 - val_loss: 81.3137 - val_mse: 81.3137 - val_mae: 6.4620\n",
      "Epoch 801/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6593 - mse: 85.6593 - mae: 6.4268 - val_loss: 81.5588 - val_mse: 81.5588 - val_mae: 6.4526\n",
      "Epoch 802/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 85.6643 - mse: 85.6642 - mae: 6.4187 - val_loss: 81.8987 - val_mse: 81.8987 - val_mae: 6.4300\n",
      "Epoch 803/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6918 - mse: 85.6918 - mae: 6.4293 - val_loss: 81.6882 - val_mse: 81.6882 - val_mae: 6.4172\n",
      "Epoch 804/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.7611 - mse: 85.7611 - mae: 6.4218 - val_loss: 81.4751 - val_mse: 81.4751 - val_mae: 6.5117\n",
      "Epoch 805/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 85.6560 - mse: 85.6561 - mae: 6.4227 - val_loss: 81.2384 - val_mse: 81.2384 - val_mae: 6.4589\n",
      "Epoch 806/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.6152 - mse: 85.6152 - mae: 6.4217 - val_loss: 81.1586 - val_mse: 81.1586 - val_mae: 6.4662\n",
      "Epoch 807/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.6580 - mse: 85.6579 - mae: 6.4176 - val_loss: 81.8137 - val_mse: 81.8138 - val_mae: 6.5649\n",
      "Epoch 808/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6634 - mse: 85.6634 - mae: 6.4274 - val_loss: 81.7474 - val_mse: 81.7474 - val_mae: 6.5080\n",
      "Epoch 809/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 85.7264 - mse: 85.7263 - mae: 6.4176 - val_loss: 81.4545 - val_mse: 81.4545 - val_mae: 6.4969\n",
      "Epoch 810/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 85.7217 - mse: 85.7216 - mae: 6.4243 - val_loss: 81.3491 - val_mse: 81.3491 - val_mae: 6.5147\n",
      "Epoch 811/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 85.6260 - mse: 85.6260 - mae: 6.4228 - val_loss: 82.9631 - val_mse: 82.9631 - val_mae: 6.6508\n",
      "Epoch 812/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 85.6757 - mse: 85.6757 - mae: 6.4246 - val_loss: 81.5119 - val_mse: 81.5119 - val_mae: 6.4217\n",
      "Epoch 813/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 85.7237 - mse: 85.7237 - mae: 6.4267 - val_loss: 81.4497 - val_mse: 81.4497 - val_mae: 6.4227\n",
      "Epoch 814/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 85.5279 - mse: 85.5280 - mae: 6.4261 - val_loss: 81.7099 - val_mse: 81.7099 - val_mae: 6.4899\n",
      "Epoch 815/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 85.6223 - mse: 85.6222 - mae: 6.4206 - val_loss: 81.4097 - val_mse: 81.4097 - val_mae: 6.4156\n",
      "Epoch 816/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6345 - mse: 85.6346 - mae: 6.4248 - val_loss: 81.7118 - val_mse: 81.7118 - val_mae: 6.4122\n",
      "Epoch 817/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.5827 - mse: 85.5827 - mae: 6.4267 - val_loss: 82.2051 - val_mse: 82.2051 - val_mae: 6.3926\n",
      "Epoch 818/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.6510 - mse: 85.6511 - mae: 6.4283 - val_loss: 81.5194 - val_mse: 81.5194 - val_mae: 6.4211\n",
      "Epoch 819/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 85.7844 - mse: 85.7844 - mae: 6.4289 - val_loss: 82.1048 - val_mse: 82.1048 - val_mae: 6.3761\n",
      "Epoch 820/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 85.6715 - mse: 85.6715 - mae: 6.4259 - val_loss: 82.0338 - val_mse: 82.0338 - val_mae: 6.3861\n",
      "Epoch 821/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6382 - mse: 85.6382 - mae: 6.4169 - val_loss: 81.4640 - val_mse: 81.4640 - val_mae: 6.4470\n",
      "Epoch 822/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 85.5665 - mse: 85.5665 - mae: 6.4202 - val_loss: 81.6782 - val_mse: 81.6782 - val_mae: 6.5060\n",
      "Epoch 823/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 85.5698 - mse: 85.5697 - mae: 6.4234 - val_loss: 81.9044 - val_mse: 81.9044 - val_mae: 6.4633\n",
      "Epoch 824/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 85.5979 - mse: 85.5979 - mae: 6.4193 - val_loss: 81.4815 - val_mse: 81.4814 - val_mae: 6.4522\n",
      "Epoch 825/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 85.5881 - mse: 85.5881 - mae: 6.4281 - val_loss: 81.7629 - val_mse: 81.7629 - val_mae: 6.4507\n",
      "Epoch 826/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6407 - mse: 85.6407 - mae: 6.4215 - val_loss: 81.5339 - val_mse: 81.5339 - val_mae: 6.4361\n",
      "Epoch 827/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 85.7262 - mse: 85.7262 - mae: 6.4182 - val_loss: 81.4784 - val_mse: 81.4784 - val_mae: 6.4111\n",
      "Epoch 828/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.6587 - mse: 85.6587 - mae: 6.4141 - val_loss: 82.2844 - val_mse: 82.2844 - val_mae: 6.3910\n",
      "Epoch 829/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.6916 - mse: 85.6917 - mae: 6.4204 - val_loss: 81.4824 - val_mse: 81.4824 - val_mae: 6.5278\n",
      "Epoch 830/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 85.7517 - mse: 85.7517 - mae: 6.4307 - val_loss: 81.4452 - val_mse: 81.4452 - val_mae: 6.4278\n",
      "Epoch 831/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 85.6161 - mse: 85.6162 - mae: 6.4097 - val_loss: 81.3098 - val_mse: 81.3098 - val_mae: 6.5172\n",
      "Epoch 832/1000\n",
      "14194/14194 [==============================] - 10s 736us/step - loss: 85.7253 - mse: 85.7253 - mae: 6.4229 - val_loss: 81.3348 - val_mse: 81.3348 - val_mae: 6.4690\n",
      "Epoch 833/1000\n",
      "14194/14194 [==============================] - 5s 321us/step - loss: 85.6263 - mse: 85.6263 - mae: 6.4248 - val_loss: 81.7395 - val_mse: 81.7394 - val_mae: 6.4480\n",
      "Epoch 834/1000\n",
      "14194/14194 [==============================] - 3s 234us/step - loss: 85.6763 - mse: 85.6764 - mae: 6.4249 - val_loss: 81.6761 - val_mse: 81.6761 - val_mae: 6.5746\n",
      "Epoch 835/1000\n",
      "14194/14194 [==============================] - 4579s 323ms/step - loss: 85.5165 - mse: 85.5166 - mae: 6.4240 - val_loss: 81.4120 - val_mse: 81.4120 - val_mae: 6.4739\n",
      "Epoch 836/1000\n",
      "14194/14194 [==============================] - 4s 289us/step - loss: 85.7149 - mse: 85.7149 - mae: 6.4241 - val_loss: 81.4111 - val_mse: 81.4111 - val_mae: 6.4670\n",
      "Epoch 837/1000\n",
      "14194/14194 [==============================] - 3s 188us/step - loss: 85.6124 - mse: 85.6124 - mae: 6.4234 - val_loss: 81.4110 - val_mse: 81.4110 - val_mae: 6.4539\n",
      "Epoch 838/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 85.6875 - mse: 85.6876 - mae: 6.4254 - val_loss: 81.6418 - val_mse: 81.6418 - val_mae: 6.5415\n",
      "Epoch 839/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6742 - mse: 85.6742 - mae: 6.4210 - val_loss: 81.4089 - val_mse: 81.4089 - val_mae: 6.4245\n",
      "Epoch 840/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.6351 - mse: 85.6351 - mae: 6.4212 - val_loss: 81.5208 - val_mse: 81.5208 - val_mae: 6.4545\n",
      "Epoch 841/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.7192 - mse: 85.7192 - mae: 6.4251 - val_loss: 81.4666 - val_mse: 81.4666 - val_mae: 6.4179\n",
      "Epoch 842/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.7005 - mse: 85.7005 - mae: 6.4248 - val_loss: 81.6080 - val_mse: 81.6080 - val_mae: 6.4506\n",
      "Epoch 843/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 85.7149 - mse: 85.7149 - mae: 6.4216 - val_loss: 81.3950 - val_mse: 81.3950 - val_mae: 6.4810\n",
      "Epoch 844/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7251 - mse: 85.7251 - mae: 6.4245 - val_loss: 81.3034 - val_mse: 81.3034 - val_mae: 6.4741\n",
      "Epoch 845/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.6663 - mse: 85.6662 - mae: 6.4211 - val_loss: 81.5054 - val_mse: 81.5054 - val_mae: 6.4567\n",
      "Epoch 846/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.6304 - mse: 85.6304 - mae: 6.4240 - val_loss: 81.6914 - val_mse: 81.6914 - val_mae: 6.4617\n",
      "Epoch 847/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 85.6389 - mse: 85.6389 - mae: 6.4197 - val_loss: 81.5366 - val_mse: 81.5366 - val_mae: 6.4704\n",
      "Epoch 848/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 85.6156 - mse: 85.6155 - mae: 6.4260 - val_loss: 81.5778 - val_mse: 81.5778 - val_mae: 6.4143\n",
      "Epoch 849/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 85.6299 - mse: 85.6300 - mae: 6.4211 - val_loss: 81.4212 - val_mse: 81.4212 - val_mae: 6.4106\n",
      "Epoch 850/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7828 - mse: 85.7828 - mae: 6.4196 - val_loss: 81.4064 - val_mse: 81.4064 - val_mae: 6.4807\n",
      "Epoch 851/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6368 - mse: 85.6367 - mae: 6.4196 - val_loss: 81.4148 - val_mse: 81.4148 - val_mae: 6.4409\n",
      "Epoch 852/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.6870 - mse: 85.6870 - mae: 6.4255 - val_loss: 81.4438 - val_mse: 81.4438 - val_mae: 6.4679\n",
      "Epoch 853/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7784 - mse: 85.7784 - mae: 6.4273 - val_loss: 81.5119 - val_mse: 81.5119 - val_mae: 6.4563\n",
      "Epoch 854/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7216 - mse: 85.7216 - mae: 6.4244 - val_loss: 81.6782 - val_mse: 81.6782 - val_mae: 6.4063\n",
      "Epoch 855/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7527 - mse: 85.7527 - mae: 6.4272 - val_loss: 82.3223 - val_mse: 82.3224 - val_mae: 6.5768\n",
      "Epoch 856/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6120 - mse: 85.6120 - mae: 6.4250 - val_loss: 82.1336 - val_mse: 82.1336 - val_mae: 6.4043\n",
      "Epoch 857/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6018 - mse: 85.6018 - mae: 6.4180 - val_loss: 81.5534 - val_mse: 81.5533 - val_mae: 6.5414\n",
      "Epoch 858/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.5575 - mse: 85.5575 - mae: 6.4204 - val_loss: 85.1987 - val_mse: 85.1988 - val_mae: 6.7913\n",
      "Epoch 859/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8601 - mse: 85.8602 - mae: 6.4323 - val_loss: 81.2144 - val_mse: 81.2144 - val_mae: 6.4793\n",
      "Epoch 860/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7391 - mse: 85.7391 - mae: 6.4279 - val_loss: 81.6340 - val_mse: 81.6340 - val_mae: 6.4758\n",
      "Epoch 861/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6202 - mse: 85.6202 - mae: 6.4208 - val_loss: 81.9758 - val_mse: 81.9758 - val_mae: 6.5523\n",
      "Epoch 862/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.5292 - mse: 85.5292 - mae: 6.4293 - val_loss: 81.3717 - val_mse: 81.3717 - val_mae: 6.4191\n",
      "Epoch 863/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.8485 - mse: 85.8485 - mae: 6.4284 - val_loss: 81.3286 - val_mse: 81.3286 - val_mae: 6.4218\n",
      "Epoch 864/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7103 - mse: 85.7103 - mae: 6.4204 - val_loss: 81.3220 - val_mse: 81.3220 - val_mae: 6.4857\n",
      "Epoch 865/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6052 - mse: 85.6051 - mae: 6.4267 - val_loss: 81.4514 - val_mse: 81.4514 - val_mae: 6.4606\n",
      "Epoch 866/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 85.6163 - mse: 85.6163 - mae: 6.4162 - val_loss: 81.1142 - val_mse: 81.1142 - val_mae: 6.4434\n",
      "Epoch 867/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.5272 - mse: 85.5271 - mae: 6.4230 - val_loss: 81.1127 - val_mse: 81.1127 - val_mae: 6.4411\n",
      "Epoch 868/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6460 - mse: 85.6459 - mae: 6.4184 - val_loss: 81.6315 - val_mse: 81.6314 - val_mae: 6.4101\n",
      "Epoch 869/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7745 - mse: 85.7745 - mae: 6.4219 - val_loss: 81.9230 - val_mse: 81.9230 - val_mae: 6.3806\n",
      "Epoch 870/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6719 - mse: 85.6719 - mae: 6.4214 - val_loss: 82.1004 - val_mse: 82.1005 - val_mae: 6.3770\n",
      "Epoch 871/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7036 - mse: 85.7036 - mae: 6.4260 - val_loss: 81.3188 - val_mse: 81.3188 - val_mae: 6.4661\n",
      "Epoch 872/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6491 - mse: 85.6490 - mae: 6.4240 - val_loss: 81.2713 - val_mse: 81.2713 - val_mae: 6.4172\n",
      "Epoch 873/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7338 - mse: 85.7337 - mae: 6.4221 - val_loss: 81.6256 - val_mse: 81.6256 - val_mae: 6.4221\n",
      "Epoch 874/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7044 - mse: 85.7044 - mae: 6.4272 - val_loss: 81.2954 - val_mse: 81.2954 - val_mae: 6.4387\n",
      "Epoch 875/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.8003 - mse: 85.8003 - mae: 6.4270 - val_loss: 81.2977 - val_mse: 81.2977 - val_mae: 6.4939\n",
      "Epoch 876/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7321 - mse: 85.7321 - mae: 6.4254 - val_loss: 81.9122 - val_mse: 81.9123 - val_mae: 6.4017\n",
      "Epoch 877/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.6254 - mse: 85.6255 - mae: 6.4227 - val_loss: 81.7239 - val_mse: 81.7239 - val_mae: 6.5622\n",
      "Epoch 878/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6723 - mse: 85.6722 - mae: 6.4287 - val_loss: 81.4206 - val_mse: 81.4206 - val_mae: 6.4663\n",
      "Epoch 879/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.5270 - mse: 85.5270 - mae: 6.4173 - val_loss: 81.1531 - val_mse: 81.1531 - val_mae: 6.4842\n",
      "Epoch 880/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 85.7397 - mse: 85.7397 - mae: 6.4264 - val_loss: 81.3867 - val_mse: 81.3866 - val_mae: 6.4671\n",
      "Epoch 881/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6467 - mse: 85.6468 - mae: 6.4260 - val_loss: 81.4391 - val_mse: 81.4391 - val_mae: 6.4435\n",
      "Epoch 882/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7519 - mse: 85.7519 - mae: 6.4195 - val_loss: 81.4857 - val_mse: 81.4857 - val_mae: 6.5207\n",
      "Epoch 883/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.6435 - mse: 85.6435 - mae: 6.4254 - val_loss: 81.4199 - val_mse: 81.4199 - val_mae: 6.4202\n",
      "Epoch 884/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7370 - mse: 85.7370 - mae: 6.4244 - val_loss: 81.4108 - val_mse: 81.4108 - val_mae: 6.4271\n",
      "Epoch 885/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.6462 - mse: 85.6463 - mae: 6.4223 - val_loss: 81.1734 - val_mse: 81.1734 - val_mae: 6.4560\n",
      "Epoch 886/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.6225 - mse: 85.6225 - mae: 6.4157 - val_loss: 81.5093 - val_mse: 81.5093 - val_mae: 6.4764\n",
      "Epoch 887/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 85.7434 - mse: 85.7434 - mae: 6.4218 - val_loss: 81.9434 - val_mse: 81.9434 - val_mae: 6.4122\n",
      "Epoch 888/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.5652 - mse: 85.5652 - mae: 6.4169 - val_loss: 81.8944 - val_mse: 81.8944 - val_mae: 6.4066\n",
      "Epoch 889/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.7285 - mse: 85.7285 - mae: 6.4288 - val_loss: 81.2241 - val_mse: 81.2241 - val_mae: 6.4258\n",
      "Epoch 890/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.6534 - mse: 85.6534 - mae: 6.4263 - val_loss: 81.5943 - val_mse: 81.5943 - val_mae: 6.4164\n",
      "Epoch 891/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.6365 - mse: 85.6366 - mae: 6.4215 - val_loss: 81.6087 - val_mse: 81.6086 - val_mae: 6.4182\n",
      "Epoch 892/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7294 - mse: 85.7294 - mae: 6.4186 - val_loss: 81.6432 - val_mse: 81.6432 - val_mae: 6.5357\n",
      "Epoch 893/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6263 - mse: 85.6264 - mae: 6.4240 - val_loss: 81.3192 - val_mse: 81.3192 - val_mae: 6.4459\n",
      "Epoch 894/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.6503 - mse: 85.6503 - mae: 6.4185 - val_loss: 81.2732 - val_mse: 81.2731 - val_mae: 6.4608\n",
      "Epoch 895/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7544 - mse: 85.7544 - mae: 6.4230 - val_loss: 81.3781 - val_mse: 81.3781 - val_mae: 6.5051\n",
      "Epoch 896/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6818 - mse: 85.6818 - mae: 6.4301 - val_loss: 81.3928 - val_mse: 81.3929 - val_mae: 6.4415\n",
      "Epoch 897/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7139 - mse: 85.7139 - mae: 6.4282 - val_loss: 81.5017 - val_mse: 81.5017 - val_mae: 6.4092\n",
      "Epoch 898/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 85.6248 - mse: 85.6249 - mae: 6.4240 - val_loss: 81.3424 - val_mse: 81.3424 - val_mae: 6.4542\n",
      "Epoch 899/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7377 - mse: 85.7377 - mae: 6.4246 - val_loss: 81.2536 - val_mse: 81.2536 - val_mae: 6.4894\n",
      "Epoch 900/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7235 - mse: 85.7236 - mae: 6.4249 - val_loss: 81.2638 - val_mse: 81.2638 - val_mae: 6.4429\n",
      "Epoch 901/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7304 - mse: 85.7304 - mae: 6.4295 - val_loss: 81.1816 - val_mse: 81.1817 - val_mae: 6.4822\n",
      "Epoch 902/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.6741 - mse: 85.6741 - mae: 6.4236 - val_loss: 81.3483 - val_mse: 81.3483 - val_mae: 6.4922\n",
      "Epoch 903/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7060 - mse: 85.7060 - mae: 6.4266 - val_loss: 81.2817 - val_mse: 81.2817 - val_mae: 6.4195\n",
      "Epoch 904/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.6881 - mse: 85.6881 - mae: 6.4151 - val_loss: 81.2833 - val_mse: 81.2833 - val_mae: 6.4261\n",
      "Epoch 905/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.6413 - mse: 85.6413 - mae: 6.4190 - val_loss: 81.5234 - val_mse: 81.5234 - val_mae: 6.4903\n",
      "Epoch 906/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.6157 - mse: 85.6156 - mae: 6.4299 - val_loss: 81.6978 - val_mse: 81.6978 - val_mae: 6.3974\n",
      "Epoch 907/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6329 - mse: 85.6328 - mae: 6.4211 - val_loss: 81.7414 - val_mse: 81.7413 - val_mae: 6.4410\n",
      "Epoch 908/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 85.7304 - mse: 85.7304 - mae: 6.4203 - val_loss: 81.3552 - val_mse: 81.3552 - val_mae: 6.4453\n",
      "Epoch 909/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.7232 - mse: 85.7232 - mae: 6.4259 - val_loss: 81.0848 - val_mse: 81.0848 - val_mae: 6.4727\n",
      "Epoch 910/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6815 - mse: 85.6814 - mae: 6.4276 - val_loss: 81.3856 - val_mse: 81.3856 - val_mae: 6.4523\n",
      "Epoch 911/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.5657 - mse: 85.5658 - mae: 6.4230 - val_loss: 81.5716 - val_mse: 81.5715 - val_mae: 6.4464\n",
      "Epoch 912/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 85.6588 - mse: 85.6588 - mae: 6.4197 - val_loss: 81.8736 - val_mse: 81.8736 - val_mae: 6.5333\n",
      "Epoch 913/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.6883 - mse: 85.6885 - mae: 6.4261 - val_loss: 81.2187 - val_mse: 81.2188 - val_mae: 6.4094\n",
      "Epoch 914/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.6791 - mse: 85.6791 - mae: 6.4247 - val_loss: 81.7164 - val_mse: 81.7164 - val_mae: 6.4423\n",
      "Epoch 915/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 85.6557 - mse: 85.6557 - mae: 6.4196 - val_loss: 81.3811 - val_mse: 81.3812 - val_mae: 6.4022\n",
      "Epoch 916/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.5993 - mse: 85.5993 - mae: 6.4254 - val_loss: 82.2340 - val_mse: 82.2340 - val_mae: 6.3775\n",
      "Epoch 917/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 85.7923 - mse: 85.7923 - mae: 6.4199 - val_loss: 81.4766 - val_mse: 81.4766 - val_mae: 6.4772\n",
      "Epoch 918/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 85.5703 - mse: 85.5703 - mae: 6.4227 - val_loss: 81.5478 - val_mse: 81.5478 - val_mae: 6.4636\n",
      "Epoch 919/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 85.6365 - mse: 85.6365 - mae: 6.4211 - val_loss: 81.4077 - val_mse: 81.4077 - val_mae: 6.4904\n",
      "Epoch 920/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 85.6495 - mse: 85.6494 - mae: 6.4271 - val_loss: 81.3922 - val_mse: 81.3922 - val_mae: 6.4237\n",
      "Epoch 921/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.7000 - mse: 85.7000 - mae: 6.4240 - val_loss: 81.8696 - val_mse: 81.8696 - val_mae: 6.3828\n",
      "Epoch 922/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 85.7036 - mse: 85.7036 - mae: 6.4207 - val_loss: 82.0037 - val_mse: 82.0037 - val_mae: 6.3806\n",
      "Epoch 923/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 85.6919 - mse: 85.6918 - mae: 6.4237 - val_loss: 81.4297 - val_mse: 81.4297 - val_mae: 6.4536\n",
      "Epoch 924/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7806 - mse: 85.7807 - mae: 6.4294 - val_loss: 81.3825 - val_mse: 81.3825 - val_mae: 6.4052\n",
      "Epoch 925/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 85.5434 - mse: 85.5434 - mae: 6.4205 - val_loss: 81.8182 - val_mse: 81.8182 - val_mae: 6.5806\n",
      "Epoch 926/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6907 - mse: 85.6907 - mae: 6.4307 - val_loss: 82.4828 - val_mse: 82.4829 - val_mae: 6.5733\n",
      "Epoch 927/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 85.5983 - mse: 85.5983 - mae: 6.4223 - val_loss: 81.5717 - val_mse: 81.5718 - val_mae: 6.5388\n",
      "Epoch 928/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.6103 - mse: 85.6103 - mae: 6.4192 - val_loss: 81.4684 - val_mse: 81.4684 - val_mae: 6.4631\n",
      "Epoch 929/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 85.5771 - mse: 85.5769 - mae: 6.4209 - val_loss: 81.5097 - val_mse: 81.5097 - val_mae: 6.4453\n",
      "Epoch 930/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.5788 - mse: 85.5789 - mae: 6.4235 - val_loss: 81.3842 - val_mse: 81.3841 - val_mae: 6.4393\n",
      "Epoch 931/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 85.7124 - mse: 85.7123 - mae: 6.4281 - val_loss: 81.4508 - val_mse: 81.4508 - val_mae: 6.4191\n",
      "Epoch 932/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.7558 - mse: 85.7558 - mae: 6.4259 - val_loss: 81.3873 - val_mse: 81.3873 - val_mae: 6.5035\n",
      "Epoch 933/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 85.6025 - mse: 85.6025 - mae: 6.4175 - val_loss: 81.4941 - val_mse: 81.4941 - val_mae: 6.4218\n",
      "Epoch 934/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6095 - mse: 85.6095 - mae: 6.4238 - val_loss: 81.6064 - val_mse: 81.6064 - val_mae: 6.4032\n",
      "Epoch 935/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6969 - mse: 85.6967 - mae: 6.4261 - val_loss: 81.4688 - val_mse: 81.4688 - val_mae: 6.4139\n",
      "Epoch 936/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 85.6190 - mse: 85.6190 - mae: 6.4153 - val_loss: 82.7282 - val_mse: 82.7282 - val_mae: 6.6476\n",
      "Epoch 937/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 85.7766 - mse: 85.7766 - mae: 6.4448 - val_loss: 81.2211 - val_mse: 81.2211 - val_mae: 6.4673\n",
      "Epoch 938/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 85.7009 - mse: 85.7010 - mae: 6.4299 - val_loss: 81.4528 - val_mse: 81.4528 - val_mae: 6.4216\n",
      "Epoch 939/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6073 - mse: 85.6073 - mae: 6.4175 - val_loss: 81.3515 - val_mse: 81.3515 - val_mae: 6.4385\n",
      "Epoch 940/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 85.6039 - mse: 85.6039 - mae: 6.4250 - val_loss: 81.6621 - val_mse: 81.6621 - val_mae: 6.4209\n",
      "Epoch 941/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 85.6734 - mse: 85.6733 - mae: 6.4180 - val_loss: 81.3242 - val_mse: 81.3242 - val_mae: 6.4377\n",
      "Epoch 942/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.5710 - mse: 85.5710 - mae: 6.4232 - val_loss: 81.6554 - val_mse: 81.6553 - val_mae: 6.4119\n",
      "Epoch 943/1000\n",
      "14194/14194 [==============================] - 1s 76us/step - loss: 85.6798 - mse: 85.6798 - mae: 6.4212 - val_loss: 81.2790 - val_mse: 81.2790 - val_mae: 6.4970\n",
      "Epoch 944/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 85.6590 - mse: 85.6591 - mae: 6.4255 - val_loss: 81.6802 - val_mse: 81.6801 - val_mae: 6.5114\n",
      "Epoch 945/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.6458 - mse: 85.6458 - mae: 6.4284 - val_loss: 81.4866 - val_mse: 81.4866 - val_mae: 6.3982\n",
      "Epoch 946/1000\n",
      "14194/14194 [==============================] - 1s 74us/step - loss: 85.6050 - mse: 85.6050 - mae: 6.4210 - val_loss: 82.7566 - val_mse: 82.7566 - val_mae: 6.6422\n",
      "Epoch 947/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.7154 - mse: 85.7155 - mae: 6.4305 - val_loss: 81.6348 - val_mse: 81.6349 - val_mae: 6.4202\n",
      "Epoch 948/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6970 - mse: 85.6970 - mae: 6.4256 - val_loss: 81.0722 - val_mse: 81.0722 - val_mae: 6.4658\n",
      "Epoch 949/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6585 - mse: 85.6586 - mae: 6.4277 - val_loss: 81.5060 - val_mse: 81.5059 - val_mae: 6.4868\n",
      "Epoch 950/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6723 - mse: 85.6724 - mae: 6.4182 - val_loss: 81.2334 - val_mse: 81.2334 - val_mae: 6.4584\n",
      "Epoch 951/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.5239 - mse: 85.5239 - mae: 6.4235 - val_loss: 81.7558 - val_mse: 81.7558 - val_mae: 6.3798\n",
      "Epoch 952/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.6323 - mse: 85.6324 - mae: 6.4200 - val_loss: 81.3903 - val_mse: 81.3903 - val_mae: 6.4798\n",
      "Epoch 953/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.6640 - mse: 85.6639 - mae: 6.4264 - val_loss: 81.2660 - val_mse: 81.2660 - val_mae: 6.5002\n",
      "Epoch 954/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.5704 - mse: 85.5705 - mae: 6.4267 - val_loss: 81.2403 - val_mse: 81.2403 - val_mae: 6.4926\n",
      "Epoch 955/1000\n",
      "14194/14194 [==============================] - 1s 75us/step - loss: 85.6686 - mse: 85.6686 - mae: 6.4274 - val_loss: 81.9225 - val_mse: 81.9225 - val_mae: 6.5163\n",
      "Epoch 956/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6736 - mse: 85.6736 - mae: 6.4218 - val_loss: 82.1135 - val_mse: 82.1135 - val_mae: 6.5898\n",
      "Epoch 957/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6934 - mse: 85.6935 - mae: 6.4261 - val_loss: 82.0622 - val_mse: 82.0621 - val_mae: 6.3966\n",
      "Epoch 958/1000\n",
      "14194/14194 [==============================] - 1s 82us/step - loss: 85.6467 - mse: 85.6468 - mae: 6.4234 - val_loss: 81.3633 - val_mse: 81.3633 - val_mae: 6.4642\n",
      "Epoch 959/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6688 - mse: 85.6688 - mae: 6.4287 - val_loss: 81.6383 - val_mse: 81.6383 - val_mae: 6.4632\n",
      "Epoch 960/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 85.5962 - mse: 85.5962 - mae: 6.4230 - val_loss: 81.4380 - val_mse: 81.4380 - val_mae: 6.4228\n",
      "Epoch 961/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6525 - mse: 85.6525 - mae: 6.4192 - val_loss: 81.3027 - val_mse: 81.3027 - val_mae: 6.4411\n",
      "Epoch 962/1000\n",
      "14194/14194 [==============================] - 1s 76us/step - loss: 85.6002 - mse: 85.6002 - mae: 6.4182 - val_loss: 81.2818 - val_mse: 81.2818 - val_mae: 6.4944\n",
      "Epoch 963/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.6990 - mse: 85.6990 - mae: 6.4293 - val_loss: 81.7412 - val_mse: 81.7412 - val_mae: 6.3858\n",
      "Epoch 964/1000\n",
      "14194/14194 [==============================] - 1s 75us/step - loss: 85.6010 - mse: 85.6010 - mae: 6.4230 - val_loss: 82.1261 - val_mse: 82.1261 - val_mae: 6.3713\n",
      "Epoch 965/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.7666 - mse: 85.7666 - mae: 6.4162 - val_loss: 81.2181 - val_mse: 81.2181 - val_mae: 6.4425\n",
      "Epoch 966/1000\n",
      "14194/14194 [==============================] - 1s 81us/step - loss: 85.6844 - mse: 85.6843 - mae: 6.4192 - val_loss: 81.7545 - val_mse: 81.7545 - val_mae: 6.5834\n",
      "Epoch 967/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.7011 - mse: 85.7010 - mae: 6.4260 - val_loss: 81.2852 - val_mse: 81.2852 - val_mae: 6.4549\n",
      "Epoch 968/1000\n",
      "14194/14194 [==============================] - 1s 74us/step - loss: 85.7519 - mse: 85.7519 - mae: 6.4330 - val_loss: 81.5102 - val_mse: 81.5102 - val_mae: 6.5017\n",
      "Epoch 969/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.4771 - mse: 85.4772 - mae: 6.4266 - val_loss: 81.8614 - val_mse: 81.8614 - val_mae: 6.4018\n",
      "Epoch 970/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.5637 - mse: 85.5636 - mae: 6.4240 - val_loss: 81.5324 - val_mse: 81.5324 - val_mae: 6.4162\n",
      "Epoch 971/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.5956 - mse: 85.5956 - mae: 6.4197 - val_loss: 81.4007 - val_mse: 81.4008 - val_mae: 6.5264\n",
      "Epoch 972/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 85.7245 - mse: 85.7245 - mae: 6.4208 - val_loss: 81.3953 - val_mse: 81.3953 - val_mae: 6.4926\n",
      "Epoch 973/1000\n",
      "14194/14194 [==============================] - 1s 76us/step - loss: 85.7323 - mse: 85.7324 - mae: 6.4326 - val_loss: 81.2322 - val_mse: 81.2322 - val_mae: 6.4572\n",
      "Epoch 974/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.7624 - mse: 85.7623 - mae: 6.4280 - val_loss: 81.5493 - val_mse: 81.5493 - val_mae: 6.4836\n",
      "Epoch 975/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6328 - mse: 85.6328 - mae: 6.4237 - val_loss: 81.6176 - val_mse: 81.6175 - val_mae: 6.4761\n",
      "Epoch 976/1000\n",
      "14194/14194 [==============================] - 1s 76us/step - loss: 85.7454 - mse: 85.7455 - mae: 6.4231 - val_loss: 81.2752 - val_mse: 81.2752 - val_mae: 6.4689\n",
      "Epoch 977/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.5302 - mse: 85.5302 - mae: 6.4199 - val_loss: 81.4931 - val_mse: 81.4931 - val_mae: 6.5308\n",
      "Epoch 978/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6818 - mse: 85.6818 - mae: 6.4210 - val_loss: 81.2635 - val_mse: 81.2635 - val_mae: 6.5001\n",
      "Epoch 979/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6666 - mse: 85.6667 - mae: 6.4275 - val_loss: 81.4730 - val_mse: 81.4729 - val_mae: 6.4207\n",
      "Epoch 980/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.6865 - mse: 85.6865 - mae: 6.4296 - val_loss: 81.5592 - val_mse: 81.5592 - val_mae: 6.4075\n",
      "Epoch 981/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.5362 - mse: 85.5362 - mae: 6.4267 - val_loss: 81.3007 - val_mse: 81.3007 - val_mae: 6.4855\n",
      "Epoch 982/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.5067 - mse: 85.5067 - mae: 6.4256 - val_loss: 82.1912 - val_mse: 82.1912 - val_mae: 6.4104\n",
      "Epoch 983/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6674 - mse: 85.6674 - mae: 6.4228 - val_loss: 81.4685 - val_mse: 81.4685 - val_mae: 6.4738\n",
      "Epoch 984/1000\n",
      "14194/14194 [==============================] - 1s 84us/step - loss: 85.6243 - mse: 85.6243 - mae: 6.4214 - val_loss: 81.4122 - val_mse: 81.4122 - val_mae: 6.4391\n",
      "Epoch 985/1000\n",
      "14194/14194 [==============================] - 1s 81us/step - loss: 85.6601 - mse: 85.6600 - mae: 6.4228 - val_loss: 81.5430 - val_mse: 81.5430 - val_mae: 6.4215\n",
      "Epoch 986/1000\n",
      "14194/14194 [==============================] - 1s 76us/step - loss: 85.5562 - mse: 85.5561 - mae: 6.4176 - val_loss: 81.4533 - val_mse: 81.4533 - val_mae: 6.4245\n",
      "Epoch 987/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6748 - mse: 85.6748 - mae: 6.4266 - val_loss: 81.5641 - val_mse: 81.5641 - val_mae: 6.4412\n",
      "Epoch 988/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.6035 - mse: 85.6035 - mae: 6.4251 - val_loss: 81.8484 - val_mse: 81.8484 - val_mae: 6.4443\n",
      "Epoch 989/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6243 - mse: 85.6242 - mae: 6.4273 - val_loss: 81.8078 - val_mse: 81.8077 - val_mae: 6.4492\n",
      "Epoch 990/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.7188 - mse: 85.7188 - mae: 6.4209 - val_loss: 81.4854 - val_mse: 81.4853 - val_mae: 6.4832\n",
      "Epoch 991/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.7306 - mse: 85.7306 - mae: 6.4211 - val_loss: 81.5279 - val_mse: 81.5279 - val_mae: 6.4148\n",
      "Epoch 992/1000\n",
      "14194/14194 [==============================] - 1s 75us/step - loss: 85.6214 - mse: 85.6215 - mae: 6.4113 - val_loss: 82.3662 - val_mse: 82.3662 - val_mae: 6.6376\n",
      "Epoch 993/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.6725 - mse: 85.6725 - mae: 6.4264 - val_loss: 81.3772 - val_mse: 81.3772 - val_mae: 6.4786\n",
      "Epoch 994/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.7216 - mse: 85.7215 - mae: 6.4269 - val_loss: 81.3321 - val_mse: 81.3322 - val_mae: 6.4878\n",
      "Epoch 995/1000\n",
      "14194/14194 [==============================] - 1s 77us/step - loss: 85.6254 - mse: 85.6254 - mae: 6.4238 - val_loss: 81.4643 - val_mse: 81.4643 - val_mae: 6.4936\n",
      "Epoch 996/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.5352 - mse: 85.5352 - mae: 6.4187 - val_loss: 81.4632 - val_mse: 81.4632 - val_mae: 6.4780\n",
      "Epoch 997/1000\n",
      "14194/14194 [==============================] - 1s 79us/step - loss: 85.5872 - mse: 85.5872 - mae: 6.4172 - val_loss: 81.1818 - val_mse: 81.1818 - val_mae: 6.4922\n",
      "Epoch 998/1000\n",
      "14194/14194 [==============================] - 1s 80us/step - loss: 85.6391 - mse: 85.6391 - mae: 6.4165 - val_loss: 81.3027 - val_mse: 81.3027 - val_mae: 6.4329\n",
      "Epoch 999/1000\n",
      "14194/14194 [==============================] - 1s 78us/step - loss: 85.5887 - mse: 85.5887 - mae: 6.4267 - val_loss: 81.5718 - val_mse: 81.5718 - val_mae: 6.4290\n",
      "Epoch 1000/1000\n",
      "14194/14194 [==============================] - 1s 83us/step - loss: 85.7329 - mse: 85.7329 - mae: 6.4303 - val_loss: 81.6320 - val_mse: 81.6320 - val_mae: 6.4193\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt,asarray\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=1000, batch_size=10,  verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81.54566907003988, 81.545654296875, 6.389636516571045] [9.03026406 9.03026325 2.52777304]\n"
     ]
    }
   ],
   "source": [
    "error = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "# print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
    "print(error, sqrt(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelSS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxV1Z3v/c/vDHVqhgIKZFLQOIIIiEbbISYa46yJdsTHGDVGMthXTefJjdqdx6Rf127v04kx3k4btTUxaWNio0Y7rTFqHNsREBFBBVSkmKcqqqjpDL/7x9pV1HCYOVVQ5/t+vc6r9ll7OGvvfer81rD32ubuiIiIAMT6OwMiIrL3UFAQEZFOCgoiItJJQUFERDopKIiISCcFBRER6aSgILILzOxXZva/dnDZj83stN3djkhfUFAQEZFOCgoiItJJQUEGrKjZ5ntmNs/MNpvZvWY2wsyeNLNGM3vGzGq6LH+emb1rZvVm9ryZHd5l3hQzmxOt93ugtMdnnWNmc6N1XzGzSbuY56vNbLGZbTCzx81sVJRuZvZTM1tjZg3RPk2M5p1lZguivC03s/93lw6YCAoKMvBdCHweOAQ4F3gSuAkYRvj+XwtgZocADwLXA7XAE8B/mlmJmZUAfwB+AwwB/iPaLtG6U4H7gG8AQ4G7gMfNLLUzGTWzzwH/BHwZGAksBX4XzT4dODnaj8HAxcD6aN69wDfcvQqYCPxlZz5XpCsFBRno/o+7r3b35cBLwOvu/pa7twGPAlOi5S4G/svdn3b3NPBjoAz4K+A4IAnc7u5pd58JvNnlM64G7nL319096+73A23RejvjUuA+d58T5e9G4HgzGwekgSrgMMDcfaG7r4zWSwNHmFm1u2909zk7+bkinRQUZKBb3WW6Jc/7ymh6FKFkDoC754BlwOho3nLvPnrk0i7TBwDfjZqO6s2sHhgbrbczeuahiVAbGO3ufwH+Bfg5sNrM7jaz6mjRC4GzgKVm9oKZHb+TnyvSSUFBJFhB+HEHQhs+4Yd9ObASGB2lddi/y/Qy4BZ3H9zlVe7uD+5mHioIzVHLAdz9Dnc/GphAaEb6XpT+prufDwwnNHM9tJOfK9JJQUEkeAg428xONbMk8F1CE9ArwKtABrjWzBJm9iXg2C7r3gN808w+HXUIV5jZ2WZWtZN5+C1wpZlNjvoj/pHQ3PWxmR0TbT8JbAZagWzU53GpmQ2Kmr02AdndOA5S5BQURAB3fx/4CvB/gHWETulz3b3d3duBLwFXABsJ/Q+PdFl3FqFf4V+i+YujZXc2D88CPwAeJtRODgKmR7OrCcFnI6GJaT2h3wPgMuBjM9sEfDPaD5FdYnrIjoiIdFBNQUREOikoiIhIJwUFERHppKAgIiKdEv2dgd0xbNgwHzduXH9nQ0RknzJ79ux17l6bb94+HRTGjRvHrFmz+jsbIiL7FDNburV5aj4SEZFOCgoiItJJQUFERDrt030K+aTTaerq6mhtbe3vrAwYpaWljBkzhmQy2d9ZEZECG3BBoa6ujqqqKsaNG0f3QS1lV7g769evp66ujvHjx/d3dkSkwAZc81FraytDhw5VQNhDzIyhQ4eq5iVSJAZcUAAUEPYwHU+R4jEgg8L2pDM5VjW00prWsPMiIl0VZ1DI5VjT2Ep7JleQ7dfX1/Ov//qvO73eWWedRX19fQFyJCKyY4oyKBTa1oJCNrvtmskTTzzB4MGDC5UtEZHtGnBXH+0NbrjhBpYsWcLkyZNJJpNUVlYycuRI5s6dy4IFC7jgggtYtmwZra2tXHfddcyYMQPYMmxHU1MTZ555JieeeCKvvPIKo0eP5rHHHqOsrKyf90xEBrqCBQUzGwv8GtgPyAF3u/vPzOyHhEcXro0Wvcndn4jWuRG4ivCM2Wvd/andycOP/vNdFqzY1Cs9505Le5bSZJx4bOc6UY8YVc3N507Y5jK33nor8+fPZ+7cuTz//POcffbZzJ8/v/OSzvvuu48hQ4bQ0tLCMcccw4UXXsjQoUO7bWPRokU8+OCD3HPPPXz5y1/m4Ycf5itf0VMWRaSwCllTyADfdfc50QPMZ5vZ09G8n7r7j7subGZHEJ5HOwEYBTxjZoe4+z7fG3zsscd2u8b/jjvu4NFHHwVg2bJlLFq0qFdQGD9+PJMnTwbg6KOP5uOPP+6z/IpI8SpYUHD3lYSHj+PujWa2EBi9jVXOB37n7m3AR2a2GDgWeHVX87C1En1ze4bFa5oYN7SC6rLC36VbUVHROf3888/zzDPP8Oqrr1JeXs4pp5yS9x6AVCrVOR2Px2lpaSl4PkVE+qSj2czGAVOA16OkvzGzeWZ2n5nVRGmjgWVdVqtj20Fkr1VVVUVjY2PeeQ0NDdTU1FBeXs57773Ha6+91se5ExHZuoIHBTOrBB4Grnf3TcCdwEHAZEJN4icdi+ZZ3fNsb4aZzTKzWWvXrs2zyg7kaZfW2nFDhw7lhBNOYOLEiXzve9/rNu+MM84gk8kwadIkfvCDH3DccccVODciIjvO3Hv97u65jZslgT8CT7n7bXnmjwP+6O4To05m3P2fonlPAT909602H02bNs17PmRn4cKFHH744dvMV0t7hkVrmjhgaAWD+qD5aCDYkeMqIvsGM5vt7tPyzStYTcHC2Aj3Agu7BgQzG9llsS8C86Ppx4HpZpYys/HAwcAbhcqfiIj0Vsirj04ALgPeMbO5UdpNwCVmNpnQNPQx8A0Ad3/XzB4CFhCuXLpmIFx5JCKyLynk1Ucvk7/5/oltrHMLcEuh8iQiItumYS5ERKSTgoKIiHQq0qCg5wOIiORTpEGhQ+Eux90ZlZWVAKxYsYKLLroo7zKnnHIKPS+/7en222+nubm5872G4haRnVXkQWHvMmrUKGbOnLnL6/cMChqKW0R2VnEHhQJVFL7//e93e57CD3/4Q370ox9x6qmnMnXqVI488kgee+yxXut9/PHHTJw4EYCWlhamT5/OpEmTuPjii7uNffStb32LadOmMWHCBG6++WYgDLK3YsUKPvvZz/LZz34WCENxr1u3DoDbbruNiRMnMnHiRG6//fbOzzv88MO5+uqrmTBhAqeffrrGWBIpcgP7eQpP3gCr3umVXOLOge1ZSpMxiO1kXNzvSDjz1m0uMn36dK6//nq+/e1vA/DQQw/xpz/9ie985ztUV1ezbt06jjvuOM4777ytPv/4zjvvpLy8nHnz5jFv3jymTp3aOe+WW25hyJAhZLNZTj31VObNm8e1117LbbfdxnPPPcewYcO6bWv27Nn88pe/5PXXX8fd+fSnP81nPvMZampqNES3iHRT3DWFApkyZQpr1qxhxYoVvP3229TU1DBy5EhuuukmJk2axGmnncby5ctZvXr1Vrfx4osvdv44T5o0iUmTJnXOe+ihh5g6dSpTpkzh3XffZcGCBdvMz8svv8wXv/hFKioqqKys5Etf+hIvvfQSoCG6RaS7gV1T2EqJvj2d5cPVjRwwpJxB5SUF+eiLLrqImTNnsmrVKqZPn84DDzzA2rVrmT17NslkknHjxuUdMrurfLWIjz76iB//+Me8+eab1NTUcMUVV2x3O9sa30pDdItIV0VZU+iLC1KnT5/O7373O2bOnMlFF11EQ0MDw4cPJ5lM8txzz7F06dJtrn/yySfzwAMPADB//nzmzZsHwKZNm6ioqGDQoEGsXr2aJ598snOdrQ3ZffLJJ/OHP/yB5uZmNm/ezKOPPspJJ520B/dWRAaKgV1T2I5CXpA6YcIEGhsbGT16NCNHjuTSSy/l3HPPZdq0aUyePJnDDjtsm+t/61vf4sorr2TSpElMnjyZY489FoCjjjqKKVOmMGHCBA488EBOOOGEznVmzJjBmWeeyciRI3nuuec606dOncoVV1zRuY2vf/3rTJkyRU1FItJLQYfOLrRdHTq7NZ3lg9WN7D+knMEFaj4aaDR0tsjA0S9DZ4uIyL5HQUFERDoNyKCwLzeJ7Y10PEWKx4ALCqWlpaxfv14/ZHuIu7N+/XpKS0v7Oysi0gcG3NVHY8aMoa6ujrVr1251mXQ2x+pNbaTXl1BeEu/D3O2bSktLGTNmTH9nQ0T6wIALCslkkvHjx29zmcVrGrn631/kjkumcN7ho/ooZyIie78B13y0Y/Q8BRGRfIo0KATqdxAR6a5gQcHMxprZc2a20MzeNbProvR/NrP3zGyemT1qZoOj9HFm1mJmc6PXLwqXt0JtWURk31bImkIG+K67Hw4cB1xjZkcATwMT3X0S8AFwY5d1lrj75Oj1zQLmTURE8ihYUHD3le4+J5puBBYCo939z+6eiRZ7Dejzy1pUURARya9P+hTMbBwwBXi9x6yvAU92eT/ezN4ysxfMLO8wnmY2w8xmmdmsbV12KiIiO6/gQcHMKoGHgevdfVOX9L8jNDE9ECWtBPZ39ynA3wK/NbPqnttz97vdfZq7T6utrd2tvKmfWUSku4IGBTNLEgLCA+7+SJf0y4FzgEs9ugTI3dvcfX00PRtYAhxSoHwVYrMiIvu8Ql59ZMC9wEJ3v61L+hnA94Hz3L25S3qtmcWj6QOBg4EPC5U/AC/oExVERPY9hbyj+QTgMuAdM5sbpd0E3AGkgKejEvtr0ZVGJwP/YGYZIAt80903FCJjqieIiORXsKDg7i+T//f3ia0s/zChqanPqE9BRKS7oryjWV0KIiL5FWVQEBGR/Io6KKj5SESku6IMCqauZhGRvIoyKHRQRUFEpLuiDArqaBYRya8og0IHPU9BRKS7og4KIiLSXVEHBdUTRES6K8qgoD4FEZH8ijIoiIhIfsUdFNR+JCLSTVEGBT1PQUQkv6IMCh30PAURke6KMiioniAikl9RBoUOundNRKS7ogwK6lIQEcmvKIOCiIjkV9RBQa1HIiLdFSwomNlYM3vOzBaa2btmdl2UPsTMnjazRdHfmijdzOwOM1tsZvPMbGrB8qauZhGRvApZU8gA33X3w4HjgGvM7AjgBuBZdz8YeDZ6D3AmcHD0mgHcWcC8AepoFhHpqWBBwd1XuvucaLoRWAiMBs4H7o8Wux+4IJo+H/i1B68Bg81sZCHypo5mEZH8+qRPwczGAVOA14ER7r4SQuAAhkeLjQaWdVmtLkrrua0ZZjbLzGatXbt2t/Klm9dERLoreFAws0rgYeB6d9+0rUXzpPX61Xb3u919mrtPq62t3bU87dJaIiIDX0GDgpklCQHhAXd/JEpe3dEsFP1dE6XXAWO7rD4GWFHI/KlPQUSku0JefWTAvcBCd7+ty6zHgcuj6cuBx7qkfzW6Cuk4oKGjmUlERPpGooDbPgG4DHjHzOZGaTcBtwIPmdlVwCfAX0fzngDOAhYDzcCVBcuZ2o9ERPIqWFBw95fZ+s/vqXmWd+CaQuUnH7UeiYh0V5R3NOvmNRGR/IoyKHRST7OISDdFGRR085qISH5FGRQ6qJ4gItJdUQYFVRRERPIryqAgIiL5FXVQUD+ziEh3RRkUTD3NIiJ5FWVQ6OCqKoiIdFOUQUH1BBGR/IoyKHRQPUFEpLuiDArqUhARya8og4KIiORX1EFB/cwiIt0VZVDQKKkiIvkVZVDooIqCiEh3xRkUVFEQEcmrOINCRDeviYh0V5RBQZekiojkV7CgYGb3mdkaM5vfJe33ZjY3en1sZnOj9HFm1tJl3i8KlS8REdm6RAG3/SvgX4BfdyS4+8Ud02b2E6Chy/JL3H1yAfPTSRUFEZH8ChYU3P1FMxuXb56FYUq/DHyuUJ8vIiI7r7/6FE4CVrv7oi5p483sLTN7wcxO6otMqJ9ZRKS7QjYfbcslwINd3q8E9nf39WZ2NPAHM5vg7pt6rmhmM4AZAPvvv/8ufbiepyAikt8O1RTM7Dozq7bgXjObY2an78oHmlkC+BLw+440d29z9/XR9GxgCXBIvvXd/W53n+bu02pra3clC1u2pdvXRES62dHmo69FpfbTgVrgSuDWXfzM04D33L2uI8HMas0sHk0fCBwMfLiL298u1RNERPLb0aDQ8Tt6FvBLd3+b7fy2mtmDwKvAoWZWZ2ZXRbOm073pCOBkYJ6ZvQ3MBL7p7ht2MG+7TH0KIiLd7Wifwmwz+zMwHrjRzKqA3LZWcPdLtpJ+RZ60h4GHdzAvu01dCiIi+e1oULgKmAx86O7NZjaE0IQkIiIDyI42Hx0PvO/u9Wb2FeDv6X7j2T5JrUciIt3taFC4E2g2s6OA/wkspcudyvsaPU9BRCS/HQ0KGQ9Dip4P/MzdfwZUFS5bfUMdzSIi3e1on0Kjmd0IXAacFF0+mixctgpLHc0iIvntaE3hYqCNcL/CKmA08M8Fy1Uf0c1rIiLd7VBQiALBA8AgMzsHaHX3fbZPQURE8tvRYS6+DLwB/DVhdNPXzeyiQmasL6hPQUSkux3tU/g74Bh3XwNhWArgGcLdxyIiMkDsaJ9CrCMgRNbvxLp7HXU0i4jkt6M1hT+Z2VNsGbPoYuCJwmRJRET6yw4FBXf/npldCJxAGAjvbnd/tKA5KyDdvCYikt8OP2Snrwet6wuunmYRkW62GRTMrJH8QwQZ4O5eXZBcFZj6FERE8ttmUHD3fX4oi21RRUFEpLt99gqi3aGKgohIfkUZFEREJL+iDgpqPRIR6a4og4Kpp1lEJK+iDAod1NEsItJdwYKCmd1nZmvMbH6XtB+a2XIzmxu9zuoy70YzW2xm75vZFwqVL1BHs4jI1hSypvAr4Iw86T9198nR6wkAMzsCmA5MiNb51+hBPgWl5ymIiHRXsKDg7i8CG3Zw8fOB37l7m7t/BCwGji1U3tSlICKSX3/0KfyNmc2LmpdqorTRwLIuy9RFab2Y2Qwzm2Vms9auXVvovIqIFJW+Dgp3AgcBk4GVwE+i9Hxl97xtO+5+t7tPc/dptbW1u5UZdTSLiHTXp0HB3Ve7e9bdc8A9bGkiqgPGdll0DLCiUPnQJakiIvn1aVAws5Fd3n4R6Lgy6XFgupmlzGw8cDDh8Z8FpYqCiEh3Ozx09s4ysweBU4BhZlYH3AycYmaTCb/HHwPfAHD3d83sIWABkAGucfdsofImIiL5FSwouPsleZLv3cbytwC3FCo/W/nQPv04EZG9XdHe0axuBRGR3oo2KID6FEREeiraoKCKgohIb0UbFEREpLeiDgrqZxYR6a5og4JuYBMR6a1ogwJolFQRkZ6KNiioniAi0lvRBgVQn4KISE9FGxTUpSAi0lvRBgUREemtqIOCWo9ERLor2qBg6moWEemlaIMCqKNZRKSn4g0KqiiIiPRSvEEB3bwmItJT0QYFVRRERHor2qAA6PIjEZEeijsoiIhINwULCmZ2n5mtMbP5XdL+2czeM7N5ZvaomQ2O0seZWYuZzY1evyhUvrbkpdCfICKy7ylkTeFXwBk90p4GJrr7JOAD4MYu85a4++To9c0C5quTWo9ERLorWFBw9xeBDT3S/uzumejta8CYQn3+9ujmNRGR3vqzT+FrwJNd3o83s7fM7AUzO2lrK5nZDDObZWaz1q5du1sZcN29JiLSTb8EBTP7OyADPBAlrQT2d/cpwN8CvzWz6nzruvvd7j7N3afV1tbuRh52eVURkQGrz4OCmV0OnANc6lFR3d3b3H19ND0bWAIcUui8qKIgItJdnwYFMzsD+D5wnrs3d0mvNbN4NH0gcDDwYUHzUsiNi4jsoxKF2rCZPQicAgwzszrgZsLVRingaQvtN69FVxqdDPyDmWWALPBNd9+Qd8MiIlIwBQsK7n5JnuR7t7Lsw8DDhcrL1qj1SESku6K9o9nU0ywi0kvRBgVQR7OISE9FGxRUTxAR6a1ogwLoeQoiIj0Vb1BQVUFEpJfiDQoiItJLUQcFdTSLiHRXtEFBrUciIr0VbVAQEZHeijYo6OY1EZHeijYogJ6nICLSU9EGBVUURER6K9qgABoQT0Skp6INCqooiIj0VrRBQUREeivqoKB+ZhGR7oo2KOiSVBGR3oo2KIBGSRUR6alog4LqCSIivRU0KJjZfWa2xszmd0kbYmZPm9mi6G9NlG5mdoeZLTazeWY2tZB5A/UpiIj0VOiawq+AM3qk3QA86+4HA89G7wHOBA6OXjOAOwuZMXUpiIj0VtCg4O4vAht6JJ8P3B9N3w9c0CX91x68Bgw2s5GFzJ+IiHTXH30KI9x9JUD0d3iUPhpY1mW5uiitGzObYWazzGzW2rVrdysjaj0SEelub+pozteg0+t3293vdvdp7j6ttrZ2D3+ciEhx64+gsLqjWSj6uyZKrwPGdlluDLCiUJlIJWI0t2UKtXkRkX1SfwSFx4HLo+nLgce6pH81ugrpOKCho5mpEA4ZUcl7qxoLtXkRkX1SoS9JfRB4FTjUzOrM7CrgVuDzZrYI+Hz0HuAJ4ENgMXAP8O1C5m3i6EEsWtNEazpbyI8REdmnJAq5cXe/ZCuzTs2zrAPXFDI/XU0cUYbl0ry3qpHJYwf31ceKiOzV9qaO5r7zyeuc/ugkjost5J3lDf2dGxGRvUZxBoXqkRjOxPIGnl6wur9zIyKy1yjOoFA1CizOKfu18tKitSyvb+nvHImI7BWKMyjEE1A9miMr6nGHmbPq+jtHIiJ7heIMCgCDx1LRspLPHlrLXS8uYfEaXZ4qIlLEQeEAWLOQfzrvYMqScWb8ejZzl9X3d65ERPpVQS9J3asdcR68/Vv22zibn02fwhW/fIMLfv7fHDKikmPGDWHskHJGVKc4cvQgRg0uoywZJ+cQj2l4DBEZuIo3KOx/XPhbN4sTTzmN/77hc8ycXcdrH67nkTnLadnKTW2H7VfFmJpyJoyqZmNzO8eMG0IybgypSLFfdSnDqkooLynewyoi+zbzffhJM9OmTfNZs2bt+gbuPw8+egG+8y4MGtOZnMs5Szc08+zC1bS0Z/lo/WY+WN1IKhGnqjTBh2s388mG5q1utqo0QcyMhpY01aUJRteUU5KIMbSihJJ4jBHVKYZVpihJxKgpL6GqNEFVaZLK0gSVqQTVpQnSOactnWVQWZKKVIKSeIxNrWkGlSUBPWNaRHadmc1292n55hV3kfYz3w9B4VfnwP+YA7HQxRKLGeNTjXz9+NGQSOVdtTWd5cO1m2lJZzAz5izdyJrGNsygLZ1jw+Z2mtuzDK0o4YM1jaQSMRataaShOc2m1p0fiC8ZN9LZEMBLEjGSMSNmRs6dqtIkibixuS1DRSpBPGa4Q015EsyIGzS3Z4nHjJGDSknEYrRlsrSks1SmwlcglYjTms4ycnApmazT1JahqjRJazrLiOpS3J3Vm1pJxmOkkjGyOcfMKE/GaWzNMLg8SXs2R3lJnKrSJBs2t/PSonUcO66GccMqKE3GiZuxelMriXiMkkSMkriRcyhNxnAP+xWPGS3tWbLulHQuFyMWM9Y3tZNKxGjL5NjclmF4dYpszqkuTWIGmZyzvqmdRMzI5JxE3CgviVORSpDJOhUlcdqzOVrTOSpSccqScRyob05TmUrQ2JqmuixJKhFjbWMbOXdKk2H9ZDxGImYk4zFa0lk2NreTyTqjBpeSSsRxd1Y0tFJTniSViFOaDPlsastQUZKguT1DaTJOImYk4jHcnUzOWVHfQmkyDsCwypLOY9eeyREzI+tONpcjGY8xvKqUze0ZWtqzVJUmSEf7sq6pLRQmypK0Z3JAeIhUVSpJOpejLBmnuT1LWUmc+uZ2hlWmaG7P0tyeIWbhu1JWEieTzZGIx0glYsTMGFJRQlNbJvruNjF6cBmrN7USjxmDy5M0tmaIGVSXJmnL5CgridPSniURD9/NTS1pBpeXsLG5nZw7IweVsXFzO/GYhe9DzIjHjGTcaG7PEjOjvrmdRCx8xza1pBlamaKxNU1ze5ZRg8sAaM/kGFyeJGZGU1uGxtZw/sI+OC3pLDXR51ZGBar6ljQAg8qSZHPO5rYMiXg4n22ZHG3pLDkP/9fxmDEs+txNrRlGVKfIefjcqtIEbekcJYkYreks6WyO8lQCIwzpnIgZm1rTVKWSrGtqY9TgMmJG5/euNBmObTqbwx3KS+K0RefaCd/foZUlbG4LxzFuRln0/WjP5mjLhG20tudoTmcYOahsp39Ltqe4awoA95wKy2fBASdCexMcfTkMOwR+dXaYf9g5cPG/935U26YVUDkCYvGd/kh3py2To6ElzbINzZSVxNnclqWpLU1ja4amtgwbmtrJ5Jya8iSbWjNsbstQmUqwoqGFZDxGMh4jnc3RngmvFQ0t0T9QhkTMqCpNUN+cpr65nXTWGVaVoj76IUtnc7Rnc9RWptjcHprJNm5up6wkzsbmdpLxGJWpBOsa22jLhGXNIGZGRUk8b1AriYeA2p7N7fTx2FtcHn+KHyXv55DW+2kn2d/ZkT0gHjNiRmeBalvOj73MX3JTaaR8j+YhZpDbzZ/ZrtuIx4xszvnChBHcdVnewv52baumoKDQ2gC/nQ6fvLL1ZUZNgWNnwKblsHwOHPnXMPNKOOenMHJymN8RNNwH3LM+G1vTVJQkMAvNVtmcEzM6Sz51G1sYNbgMAxpa0jSns5Ql42zY3MbIQWWs2tTaWRqet6yeKfvXdH7JO0r4BrSkszS0hFLf4LJQ+uwIYOmMU1YSo745TTrrrN/cxqeGV+IO6SgQpbOh5F2ZSrCmsZXhVaXEYkYqEWo2HTWoVDJGY2uG+uZ2yksSJONGSTzGMQ8dTbK9nufOeYll6Wo2bG6nqjTJuKHlmEE2B22ZLCvrWxlendrSRFiWpLE1zbDKFAtXbmJEdSmlyRiJWIymtkwUZONkov/q1nSO9U1tDKtMkYiH0m51aZLm9gxNbdlQwyP887dHQTnn0NSaobwkTnsmx6DyJCXxGMvrWzCDUYPKyOacilScprZQgm1LZ3Fgw+Z2GlszfGp4JSXxGBua2wEYXpWiNZ0jGZXsnfDjs7k9y6aWNLmcUx4dy1GDQi0hZkZtVSrUGBMxEnFjw+Z2NrVkyLl3HpfVm1o7S+2TxgwiZsYnG5qJWagZpBIxMrkw353OErgTamfZnJNz2NyW6SxtL9/YQjrntLSH2nkybpSXhJpx3Iz2bI5ELOzH6k2tDCkvoS2TI+vOsMoUm9sy1G1spqaihGEVofUDIisAAA66SURBVAVgc3uGtkyOg3IfcdEb03l70Oe4Z8QPqK1KMWpQGcOqSvhoXTObWtIMr06xsj7UBmOxUIJPJWKks05bJht9j40la5s4qLaSmMGaxjYGlSUpTcYoTcZpiGosHRes1DenGVJRQjYXCmulyTjLN7aw/9ByVtS3UFWapL65nYpUgqrSBPHoO5dKxjj7yFEcMap6l/6nFRS2J9MOH/wJ6t6A9/4LNny489s45mr48DlYvxi+8RJk2mDZa3D833QPEq0NMPt+OOqSMP3Kz+Csn0CiJP92m9bA49fCuT+DqhHQsBzWLoTxp0DbJigfshP72Qa5DJRU9J6XTcPS/4YDT9n+dtYvgaqRULJnS1T97tYDoLUernkDag/t79zsmKY1kG6GmnF7drutDfDJa3DIF/bM9po3wCt3wCk3bf273p+Wvgq/PAPGHANff6a/c1Nw6lPYnkRJuET1iPPg9P+1Jb1xVXitfR9Kq2HBY7DkL3D4uZBphbf+fcuyb96zZfquk7ZM//nvweLgPa5meuMeaPgkTM/5NXz9WWiog5d+AqvmhfRJF8OKt2DdB/BEEs76Mfz0iO7bueppeO6WkO+PXoT9j4d3H4XJl4ZmsceugRP/Fo6aDn/4dki7uR7WLAxBbL8joa0JPn4ZXvt5aCobMSH86C/9byiphIcuhwvvgY1LYeinwj/PYefAl38T+mHaGmH1gnCMhh8egsa8h0KAqT0UkmWAwer5oWZlFprdNq8Px6UyeiJrWxM880M4/JxwH8mQ8d33dfEzsG5ROP7Vo0OtLOoHYtEzMO7EENAH7w+pyi3ruW85jkdNh3QLNK/vdnFBN2/9JhzPbAbWvQ/lw0JA7rq9dAvULw3721UuC9n2ELyHfarHvBy8+M8w4YIQmEsHQaoKWjZCshxiifDjmaqE958M+9myMaSXD4H5j8CgsTD2mC3b/JdjQiD7YZ6BHds3wz+OCgWT46+BDR/B2GPh45fCdpLlMKjXE29h/sPw5x+EmvF3FsDGj2Hl3PB9rBgWzvcnr8PBp21Zp3FV+A5N+GI4t9kMZFogURqO9Z9uhHcfgf0mwbCDQyHsoM/B6KPDsr+/FI75Ohz8+fznpKd0CzSt7h0M3/svePx/wLVzIV4CydKQvmkFVI/a+vbWvR/+2m7euuUejk9pdTj+Hz4Ph50dvg8v/QS+8I8h4D5zM5z5/4fluq6br5Uh0xb6Nt3Db8PIo3Yvj9uhmsLuSreGf4bSQdC0Cla9A0tfCf94h50TflizGWjfi+6YTpSFf9iC6Ohy2454SfjxBBh2aPhHaVrVfZkT/xbq3oTyoeEHfc2C7vOHT4BDz4A5v4HNa7rPO+AEGP8ZmPtA+PHOZ8plIRBn06HWs+jPW8/v/n8FZTXw/n/1njf+5BCQt6ZyRPgRXp7nuzrxIpg/MwTndYu3fl6GHAQblmx5f8EvQvBf9FR4/5nvw7t/gGOvDvl8/p9CrXVnTP1q+JH+z+t2bPmxx4XCxZhpsOTZkJYog1N/AIuf3ZK2LWf8b/jgyfDjCXDGrfDmv4Wg/9ELodl28AGw8PFQI1r1TqhlL58dvhNDDoKJF4Zafs14mP3LrX/WxItCf2GyNBR2WushWQGVtTDza1uWu/BeqJsVCgfjPxMKLRO+CPWfwBt3wZADYcpXIZ6EERNDQWTRn0Pwfv4fwzZGTYUVc8L0qTfDsz8K04PGQkP0KPrSwSEIvvMfYfmWDSEAH3o2HHF++Ly174XvxzFfh1n3gefghOth6EHh/+Kws3fsXPWg5qP+5B5Kj7l0+BGMp0JJaslz4YfsU6fB2w/CppVQcwBU7Qeb14XS9Yq5oWReUhFKFvsdGb74408O65uFUknLxlDiWPpq+EHItIUS4OoFoTScLIVV8yHbFvI09tPQuik0Q3UEiGQFpDf3zn/pIEhVh3/SZa8V5hgly0OQaNUd5SI77PBzQ81+FygoSGFsrbqby21p1km3hGahVGWYLqkMJapsO+BQvyzMqx4Val3J0tAUUVYDLfVhubKasG4iFZpbmlaH9KqRW64Ca9kQahJjjgkl/7Ka0NRXUhmaTCwWmjGWvhJKw02rQoWmZUOo5lePDsG35gDAQu1v+GGhKax0UKhRlA8NeR+8f2hOqRwR2vSzbaGZpOOzY/HQvNi4OjTBHPIFePPeUOIcvH8I/NWjQ+2i5oAQkOf9PpQaEylobw6f9ea/hdJo+VA49EyIJWHZ6+EYrJwbmiUTqVDibG+EQ8+CRU+HJp7Na8M+N60J+WqtD6XUkgr44Knwd8j4sO47M0PhYeXcsOzwI0KQLhscjkNZTchv2eBwbJrXhabEWDykparDuUi3hOZAi8GHL4TzP3hsaDJqbYDqkaGk31IfSttNq2DNe2F7I48KNYKGulCzW78kXNBR/0lo+jMLNe/m9dC4Mhz7mnGw7I2wH9Wjw7mtGAoVw0OhqmZ8KPBk2sI5rl8ajl1bY/hetG0K56OtMeRx+azQpLliTjjPo6eGmlD1qLBP7qEAkywNBbGSypCXRGk4RyveCsdi7DFh2UQq5HPjR+HcbfwofF9T1eG4zflN+GEfNSVsp7Uh7Pfh58Kad8N0SUXIW0VtqOWsfCscs8rh4fjs4kUtCgoiItJpW0GheAfEExGRXhQURESkU59fkmpmhwK/75J0IPD/AYOBq4G1UfpN7v5EH2dPRKSo9XlQcPf3gckAZhYHlgOPAlcCP3X3H/d1nkREJOjv5qNTgSXuvpULyUVEpC/1d1CYDjzY5f3fmNk8M7vPzGryrWBmM8xslpnNWrt2bb5FRERkF/VbUDCzEuA84D+ipDuBgwhNSyuBn+Rbz93vdvdp7j6ttra2T/IqIlIs+rOmcCYwx91XA7j7anfPunsOuAc4th/zJiJSlPpzQLxL6NJ0ZGYj3X1l9PaLwPztbWD27NnrzGx3+iOGAet2Y/19TbHtL2ifi4X2eeccsLUZ/XJHs5mVA8uAA929IUr7DaHpyIGPgW90CRKFysesrd3VNxAV2/6C9rlYaJ/3nH6pKbh7MzC0R9pl/ZEXERHZor+vPhIRkb1IsQeFu/s7A32s2PYXtM/FQvu8h+zTo6SKiMieVew1BRER6UJBQUREOhVlUDCzM8zsfTNbbGY39Hd+9hQzG2tmz5nZQjN718yui9KHmNnTZrYo+lsTpZuZ3REdh3lmNrV/92DXmFnczN4ysz9G78eb2evR/v4+unseM0tF7xdH88f1Z753h5kNNrOZZvZedL6PL4Lz/J3oez3fzB40s9KBdq6jIX7WmNn8Lmk7fV7N7PJo+UVmdvnO5KHogkI0MuvPCXdUHwFcYmZH9G+u9pgM8F13Pxw4Drgm2rcbgGfd/WDg2eg9hGNwcPSaQRhqZF90HbCwy/v/TRhx92BgI3BVlH4VsNHdPwX8NFpuX/Uz4E/ufhhwFGH/B+x5NrPRwLXANHefCMQJY6cNtHP9K+CMHmk7dV7NbAhwM/BpwsgQN29tLLm83L2oXsDxwFNd3t8I3Njf+SrQvj4GfB54HxgZpY0E3o+m7wIu6bJ853L7ygsYE/2jfA74I2CEuzwTPc838BRwfDSdiJaz/t6HXdjnauCjnnkf4Od5NOGG1yHRufsj8IWBeK6BccD8XT2vhNEi7uqS3m257b2KrqbAli9Xh7oobUCJqstTgNeBER7dHR79HR4tNhCOxe3A/wRy0fuhQL27Z6L3Xfepc3+j+Q30uIlyH3Eg4WFUv4yazf7NzCoYwOfZ3ZcDPwY+IQyY2QDMZuCfa9j587pb57sYg4LlSRtQ1+WaWSXwMHC9u2/a1qJ50vaZY2Fm5wBr3H121+Q8i/oOzNuXJICpwJ3uPgXYzJYmhXz2+f2Omj/OB8YDo4AKQvNJTwPtXG/L1vZxt/a9GINCHTC2y/sxwIp+ysseZ2ZJQkB4wN0fiZJXm9nIaP5IYE2Uvq8fixOA88zsY+B3hCak24HBZtYxhEvXferc32j+IGBDX2Z4D6kD6tz99ej9TEKQGKjnGeA04CN3X+vuaeAR4K8Y+Ocadv687tb5Lsag8CZwcHTVQgmhs+rxfs7THmFmBtwLLHT327rMehzouALhckJfQ0f6V6OrGI4DGrzAgxDuSe5+o7uPcfdxhPP4F3e/FHgOuCharOf+dhyHi6Ll97nSo7uvApZZeN45hCcYLmCAnufIJ8BxZlYefc879nlAn+vIzp7Xp4DTzawmqmGdHqXtmP7uVOmnjpyzgA+AJcDf9Xd+9uB+nUioJs4D5kavswhtqc8Ci6K/Q6LljXAl1hLgHcKVHf2+H7u476cAf4ymDwTeABYTHuKUitJLo/eLo/kH9ne+d2N/JwOzonP9B6BmoJ9n4EfAe4Rh9X8DpAbauSY8TmAlkCaU+K/alfMKfC3a98XAlTuTBw1zISIinYqx+UhERLZCQUFERDopKIiISCcFBRER6aSgICIinRQURPqJmZ3SMbKryN5CQUFERDopKIhsh5l9xczeMLO5ZnZX9PyGJjP7iZnNMbNnzaw2Wnaymb0WjW//aJex7z9lZs+Y2dvROgdFm6/s8lyEB6K7dUX6jYKCyDaY2eHAxcAJ7j4ZyAKXEgZkm+PuU4EXCOPXA/wa+L67TyLcZdqR/gDwc3c/ijBmT8cwE1OA6wnP9jiQMJ6TSL9JbH8RkaJ2KnA08GZUiC8jDEiWA34fLfPvwCNmNggY7O4vROn3A/9hZlXAaHd/FMDdWwGi7b3h7nXR+7mEsfRfLvxuieSnoCCybQbc7+43dks0+0GP5bY1Xsy2moTaukxn0f+k9DM1H4ls27PARWY2HDqfl3sA4X+nY3TO/wd42d0bgI1mdlKUfhnwgodnWtSZ2QXRNlJmVt6neyGyg1QqEdkGd19gZn8P/NnMYoTRK68hPNhmgpnNJjzV6+JolcuBX0Q/+h8CV0bplwF3mdk/RNv46z7cDZEdplFSRXaBmTW5e2V/50NkT1PzkYiIdFJNQUREOqmmICIinRQURESkk4KCiIh0UlAQEZFOCgoiItLp/wJUVL310ICiCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# d = defaultdict(LabelEncoder)\n",
    "\n",
    "# labeled_df = X_df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# labeled_df\n",
    "# labeled_df.apply(lambda x: d[x.name].inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.expand_dims(X_test_scaled[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: [[33.86278]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted price: {model.predict(test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
