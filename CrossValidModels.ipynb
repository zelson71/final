{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Resources/uswhites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  price  rating      region          subregion  \\\n",
       "0     14.1   50.0      94  California             Sonoma   \n",
       "1     12.6   30.0      94      Oregon  Willamette Valley   \n",
       "2     13.8   60.0      94  California        Napa-Sonoma   \n",
       "3     13.8   35.0      94  Washington    Columbia Valley   \n",
       "4     14.7   65.0      94  California             Sonoma   \n",
       "\n",
       "                     varietal  vintage  \n",
       "0                  Chardonnay     2016  \n",
       "1                    Riesling     2016  \n",
       "2                  Chardonnay     2016  \n",
       "3  Bordeaux-style White Blend     2017  \n",
       "4                  Chardonnay     2016  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['ID', 'category', 'country', 'description', 'designation', 'subsubregion', 'title', 'url', 'winery'], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol      float64\n",
       "rating         int64\n",
       "region        object\n",
       "subregion     object\n",
       "varietal      object\n",
       "vintage        int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.DataFrame(df)\n",
    "X_df = df.drop(\"price\", axis=1)\n",
    "X_df['subregion'] = X_df['subregion'].astype(str)\n",
    "X_df['vintage'] = X_df['vintage'].astype(int)\n",
    "y = df[\"price\"]\n",
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23653</td>\n",
       "      <td>147</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23654</td>\n",
       "      <td>147</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23655</td>\n",
       "      <td>159</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23656</td>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>138</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23657</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23658 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       alcohol  rating  region  subregion  varietal  vintage\n",
       "0          130      14       2         42        17       22\n",
       "1           68      14      17         56        87       22\n",
       "2          118      14       2         28        17       22\n",
       "3          118      14      24          7        10       23\n",
       "4          152      14       2         42        17       22\n",
       "...        ...     ...     ...        ...       ...      ...\n",
       "23653      147      11       2          3        17       11\n",
       "23654      147      12       2          3        17       11\n",
       "23655      159      12       2          3        17       11\n",
       "23656      176      12       2         42       138       10\n",
       "23657      139      12       2          2        17       11\n",
       "\n",
       "[23658 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = X_df.apply(LabelEncoder().fit_transform)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130.  14.   2.  42.  17.  22.]\n",
      " [ 68.  14.  17.  56.  87.  22.]\n",
      " [118.  14.   2.  28.  17.  22.]\n",
      " ...\n",
      " [159.  12.   2.   3.  17.  11.]\n",
      " [176.  12.   2.  42. 138.  10.]\n",
      " [139.  12.   2.   2.  17.  11.]]\n",
      "[50. 30. 60. ... 29. 24. 55.]\n"
     ]
    }
   ],
   "source": [
    "X = X.values.astype(\"float32\")\n",
    "print(X)\n",
    "y = y.values.astype(\"float32\")\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim=input_dims, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 3s - loss: 137.7555\n",
      "Epoch 2/100\n",
      "21292/21292 - 3s - loss: 99.3115\n",
      "Epoch 3/100\n",
      "21292/21292 - 3s - loss: 92.3624\n",
      "Epoch 4/100\n",
      "21292/21292 - 3s - loss: 91.2501\n",
      "Epoch 5/100\n",
      "21292/21292 - 3s - loss: 91.0060\n",
      "Epoch 6/100\n",
      "21292/21292 - 3s - loss: 90.8542\n",
      "Epoch 7/100\n",
      "21292/21292 - 3s - loss: 90.7134\n",
      "Epoch 8/100\n",
      "21292/21292 - 3s - loss: 90.7637\n",
      "Epoch 9/100\n",
      "21292/21292 - 3s - loss: 90.7203\n",
      "Epoch 10/100\n",
      "21292/21292 - 3s - loss: 90.6528\n",
      "Epoch 11/100\n",
      "21292/21292 - 3s - loss: 90.6461\n",
      "Epoch 12/100\n",
      "21292/21292 - 3s - loss: 90.5157\n",
      "Epoch 13/100\n",
      "21292/21292 - 3s - loss: 90.6351\n",
      "Epoch 14/100\n",
      "21292/21292 - 3s - loss: 90.4576\n",
      "Epoch 15/100\n",
      "21292/21292 - 3s - loss: 90.5049\n",
      "Epoch 16/100\n",
      "21292/21292 - 3s - loss: 90.5095\n",
      "Epoch 17/100\n",
      "21292/21292 - 3s - loss: 90.3657\n",
      "Epoch 18/100\n",
      "21292/21292 - 3s - loss: 90.3134\n",
      "Epoch 19/100\n",
      "21292/21292 - 3s - loss: 90.4348\n",
      "Epoch 20/100\n",
      "21292/21292 - 3s - loss: 90.4044\n",
      "Epoch 21/100\n",
      "21292/21292 - 3s - loss: 90.2419\n",
      "Epoch 22/100\n",
      "21292/21292 - 3s - loss: 90.4636\n",
      "Epoch 23/100\n",
      "21292/21292 - 3s - loss: 90.4444\n",
      "Epoch 24/100\n",
      "21292/21292 - 4s - loss: 90.4715\n",
      "Epoch 25/100\n",
      "21292/21292 - 3s - loss: 90.4961\n",
      "Epoch 26/100\n",
      "21292/21292 - 3s - loss: 90.4190\n",
      "Epoch 27/100\n",
      "21292/21292 - 3s - loss: 90.3161\n",
      "Epoch 28/100\n",
      "21292/21292 - 3s - loss: 90.3773\n",
      "Epoch 29/100\n",
      "21292/21292 - 3s - loss: 90.3449\n",
      "Epoch 30/100\n",
      "21292/21292 - 3s - loss: 90.3351\n",
      "Epoch 31/100\n",
      "21292/21292 - 3s - loss: 90.2626\n",
      "Epoch 32/100\n",
      "21292/21292 - 3s - loss: 90.3727\n",
      "Epoch 33/100\n",
      "21292/21292 - 3s - loss: 90.2815\n",
      "Epoch 34/100\n",
      "21292/21292 - 3s - loss: 90.2332\n",
      "Epoch 35/100\n",
      "21292/21292 - 4s - loss: 90.3669\n",
      "Epoch 36/100\n",
      "21292/21292 - 3s - loss: 90.3433\n",
      "Epoch 37/100\n",
      "21292/21292 - 3s - loss: 90.3948\n",
      "Epoch 38/100\n",
      "21292/21292 - 4s - loss: 90.3284\n",
      "Epoch 39/100\n",
      "21292/21292 - 4s - loss: 90.3495\n",
      "Epoch 40/100\n",
      "21292/21292 - 4s - loss: 90.1977\n",
      "Epoch 41/100\n",
      "21292/21292 - 4s - loss: 90.2382\n",
      "Epoch 42/100\n",
      "21292/21292 - 4s - loss: 90.3020\n",
      "Epoch 43/100\n",
      "21292/21292 - 4s - loss: 90.1675\n",
      "Epoch 44/100\n",
      "21292/21292 - 4s - loss: 90.1613\n",
      "Epoch 45/100\n",
      "21292/21292 - 4s - loss: 90.2296\n",
      "Epoch 46/100\n",
      "21292/21292 - 4s - loss: 90.2308\n",
      "Epoch 47/100\n",
      "21292/21292 - 4s - loss: 90.1710\n",
      "Epoch 48/100\n",
      "21292/21292 - 4s - loss: 90.1637\n",
      "Epoch 49/100\n",
      "21292/21292 - 4s - loss: 90.4004\n",
      "Epoch 50/100\n",
      "21292/21292 - 4s - loss: 90.2059\n",
      "Epoch 51/100\n",
      "21292/21292 - 4s - loss: 90.2493\n",
      "Epoch 52/100\n",
      "21292/21292 - 5s - loss: 90.2277\n",
      "Epoch 53/100\n",
      "21292/21292 - 4s - loss: 90.2184\n",
      "Epoch 54/100\n",
      "21292/21292 - 4s - loss: 89.9453\n",
      "Epoch 55/100\n",
      "21292/21292 - 4s - loss: 90.2730\n",
      "Epoch 56/100\n",
      "21292/21292 - 4s - loss: 90.1704\n",
      "Epoch 57/100\n",
      "21292/21292 - 4s - loss: 90.1076\n",
      "Epoch 58/100\n",
      "21292/21292 - 4s - loss: 90.2043\n",
      "Epoch 59/100\n",
      "21292/21292 - 4s - loss: 90.1732\n",
      "Epoch 60/100\n",
      "21292/21292 - 4s - loss: 90.0845\n",
      "Epoch 61/100\n",
      "21292/21292 - 3s - loss: 90.1924\n",
      "Epoch 62/100\n",
      "21292/21292 - 4s - loss: 90.1016\n",
      "Epoch 63/100\n",
      "21292/21292 - 4s - loss: 90.2891\n",
      "Epoch 64/100\n",
      "21292/21292 - 4s - loss: 90.1717\n",
      "Epoch 65/100\n",
      "21292/21292 - 3s - loss: 90.1571\n",
      "Epoch 66/100\n",
      "21292/21292 - 4s - loss: 90.0984\n",
      "Epoch 67/100\n",
      "21292/21292 - 4s - loss: 90.1121\n",
      "Epoch 68/100\n",
      "21292/21292 - 4s - loss: 90.1392\n",
      "Epoch 69/100\n",
      "21292/21292 - 4s - loss: 90.1615\n",
      "Epoch 70/100\n",
      "21292/21292 - 4s - loss: 90.0071\n",
      "Epoch 71/100\n",
      "21292/21292 - 3s - loss: 90.1845\n",
      "Epoch 72/100\n",
      "21292/21292 - 4s - loss: 90.0938\n",
      "Epoch 73/100\n",
      "21292/21292 - 4s - loss: 90.1071\n",
      "Epoch 74/100\n",
      "21292/21292 - 4s - loss: 90.1493\n",
      "Epoch 75/100\n",
      "21292/21292 - 4s - loss: 90.1847\n",
      "Epoch 76/100\n",
      "21292/21292 - 3s - loss: 90.1049\n",
      "Epoch 77/100\n",
      "21292/21292 - 3s - loss: 90.1714\n",
      "Epoch 78/100\n",
      "21292/21292 - 3s - loss: 90.1013\n",
      "Epoch 79/100\n",
      "21292/21292 - 4s - loss: 90.2232\n",
      "Epoch 80/100\n",
      "21292/21292 - 3s - loss: 90.0898\n",
      "Epoch 81/100\n",
      "21292/21292 - 4s - loss: 90.0684\n",
      "Epoch 82/100\n",
      "21292/21292 - 3s - loss: 90.1581\n",
      "Epoch 83/100\n",
      "21292/21292 - 3s - loss: 90.0174\n",
      "Epoch 84/100\n",
      "21292/21292 - 4s - loss: 90.0141\n",
      "Epoch 85/100\n",
      "21292/21292 - 4s - loss: 90.0528\n",
      "Epoch 86/100\n",
      "21292/21292 - 4s - loss: 89.9812\n",
      "Epoch 87/100\n",
      "21292/21292 - 4s - loss: 90.1364\n",
      "Epoch 88/100\n",
      "21292/21292 - 4s - loss: 90.1259\n",
      "Epoch 89/100\n",
      "21292/21292 - 3s - loss: 90.0950\n",
      "Epoch 90/100\n",
      "21292/21292 - 3s - loss: 89.9839\n",
      "Epoch 91/100\n",
      "21292/21292 - 3s - loss: 89.9150\n",
      "Epoch 92/100\n",
      "21292/21292 - 4s - loss: 89.9837\n",
      "Epoch 93/100\n",
      "21292/21292 - 3s - loss: 90.0330\n",
      "Epoch 94/100\n",
      "21292/21292 - 3s - loss: 89.9898\n",
      "Epoch 95/100\n",
      "21292/21292 - 3s - loss: 90.0089\n",
      "Epoch 96/100\n",
      "21292/21292 - 3s - loss: 90.1502\n",
      "Epoch 97/100\n",
      "21292/21292 - 5s - loss: 90.0449\n",
      "Epoch 98/100\n",
      "21292/21292 - 3s - loss: 90.0542\n",
      "Epoch 99/100\n",
      "21292/21292 - 3s - loss: 90.0164\n",
      "Epoch 100/100\n",
      "21292/21292 - 3s - loss: 90.0397\n",
      "2366/2366 - 0s - loss: 126.1239\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 4s - loss: 138.9029\n",
      "Epoch 2/100\n",
      "21292/21292 - 3s - loss: 109.7271\n",
      "Epoch 3/100\n",
      "21292/21292 - 3s - loss: 105.7784\n",
      "Epoch 4/100\n",
      "21292/21292 - 4s - loss: 99.1584\n",
      "Epoch 5/100\n",
      "21292/21292 - 4s - loss: 97.5094\n",
      "Epoch 6/100\n",
      "21292/21292 - 4s - loss: 97.0223\n",
      "Epoch 7/100\n",
      "21292/21292 - 4s - loss: 96.8676\n",
      "Epoch 8/100\n",
      "21292/21292 - 3s - loss: 96.8792\n",
      "Epoch 9/100\n",
      "21292/21292 - 4s - loss: 96.7466\n",
      "Epoch 10/100\n",
      "21292/21292 - 3s - loss: 96.3993\n",
      "Epoch 11/100\n",
      "21292/21292 - 4s - loss: 96.1967\n",
      "Epoch 12/100\n",
      "21292/21292 - 3s - loss: 95.8246\n",
      "Epoch 13/100\n",
      "21292/21292 - 5s - loss: 95.8416\n",
      "Epoch 14/100\n",
      "21292/21292 - 4s - loss: 95.9744\n",
      "Epoch 15/100\n",
      "21292/21292 - 4s - loss: 95.5221\n",
      "Epoch 16/100\n",
      "21292/21292 - 4s - loss: 95.5434\n",
      "Epoch 17/100\n",
      "21292/21292 - 4s - loss: 95.3003\n",
      "Epoch 18/100\n",
      "21292/21292 - 4s - loss: 95.2930\n",
      "Epoch 19/100\n",
      "21292/21292 - 4s - loss: 95.2312\n",
      "Epoch 20/100\n",
      "21292/21292 - 5s - loss: 94.8853\n",
      "Epoch 21/100\n",
      "21292/21292 - 4s - loss: 94.9288\n",
      "Epoch 22/100\n",
      "21292/21292 - 4s - loss: 94.7347\n",
      "Epoch 23/100\n",
      "21292/21292 - 4s - loss: 94.7143\n",
      "Epoch 24/100\n",
      "21292/21292 - 4s - loss: 94.7780\n",
      "Epoch 25/100\n",
      "21292/21292 - 4s - loss: 94.6877\n",
      "Epoch 26/100\n",
      "21292/21292 - 4s - loss: 94.8091\n",
      "Epoch 27/100\n",
      "21292/21292 - 4s - loss: 94.6304\n",
      "Epoch 28/100\n",
      "21292/21292 - 4s - loss: 94.5327\n",
      "Epoch 29/100\n",
      "21292/21292 - 4s - loss: 94.7473\n",
      "Epoch 30/100\n",
      "21292/21292 - 4s - loss: 94.6719\n",
      "Epoch 31/100\n",
      "21292/21292 - 4s - loss: 94.7722\n",
      "Epoch 32/100\n",
      "21292/21292 - 4s - loss: 94.5109\n",
      "Epoch 33/100\n",
      "21292/21292 - 4s - loss: 94.4588\n",
      "Epoch 34/100\n",
      "21292/21292 - 4s - loss: 94.6036\n",
      "Epoch 35/100\n",
      "21292/21292 - 4s - loss: 94.4791\n",
      "Epoch 36/100\n",
      "21292/21292 - 4s - loss: 94.5132\n",
      "Epoch 37/100\n",
      "21292/21292 - 4s - loss: 94.5544\n",
      "Epoch 38/100\n",
      "21292/21292 - 4s - loss: 94.5417\n",
      "Epoch 39/100\n",
      "21292/21292 - 5s - loss: 94.3717\n",
      "Epoch 40/100\n",
      "21292/21292 - 4s - loss: 94.5147\n",
      "Epoch 41/100\n",
      "21292/21292 - 5s - loss: 94.3854\n",
      "Epoch 42/100\n",
      "21292/21292 - 4s - loss: 94.4281\n",
      "Epoch 43/100\n",
      "21292/21292 - 4s - loss: 94.2940\n",
      "Epoch 44/100\n",
      "21292/21292 - 4s - loss: 94.3733\n",
      "Epoch 45/100\n",
      "21292/21292 - 4s - loss: 94.5722\n",
      "Epoch 46/100\n",
      "21292/21292 - 4s - loss: 94.2050\n",
      "Epoch 47/100\n",
      "21292/21292 - 4s - loss: 94.2047\n",
      "Epoch 48/100\n",
      "21292/21292 - 4s - loss: 94.3191\n",
      "Epoch 49/100\n",
      "21292/21292 - 4s - loss: 94.1594\n",
      "Epoch 50/100\n",
      "21292/21292 - 4s - loss: 94.3029\n",
      "Epoch 51/100\n",
      "21292/21292 - 4s - loss: 94.2821\n",
      "Epoch 52/100\n",
      "21292/21292 - 4s - loss: 94.2645\n",
      "Epoch 53/100\n",
      "21292/21292 - 4s - loss: 94.1959\n",
      "Epoch 54/100\n",
      "21292/21292 - 4s - loss: 94.3618\n",
      "Epoch 55/100\n",
      "21292/21292 - 4s - loss: 94.1039\n",
      "Epoch 56/100\n",
      "21292/21292 - 3s - loss: 94.1957\n",
      "Epoch 57/100\n",
      "21292/21292 - 3s - loss: 94.1304\n",
      "Epoch 58/100\n",
      "21292/21292 - 3s - loss: 94.3838\n",
      "Epoch 59/100\n",
      "21292/21292 - 4s - loss: 94.0382\n",
      "Epoch 60/100\n",
      "21292/21292 - 4s - loss: 94.2133\n",
      "Epoch 61/100\n",
      "21292/21292 - 4s - loss: 94.0268\n",
      "Epoch 62/100\n",
      "21292/21292 - 4s - loss: 93.9179\n",
      "Epoch 63/100\n",
      "21292/21292 - 4s - loss: 93.8424\n",
      "Epoch 64/100\n",
      "21292/21292 - 4s - loss: 93.7898\n",
      "Epoch 65/100\n",
      "21292/21292 - 4s - loss: 93.9251\n",
      "Epoch 66/100\n",
      "21292/21292 - 4s - loss: 93.8991\n",
      "Epoch 67/100\n",
      "21292/21292 - 4s - loss: 93.8486\n",
      "Epoch 68/100\n",
      "21292/21292 - 4s - loss: 93.7858\n",
      "Epoch 69/100\n",
      "21292/21292 - 4s - loss: 93.6103\n",
      "Epoch 70/100\n",
      "21292/21292 - 4s - loss: 93.6307\n",
      "Epoch 71/100\n",
      "21292/21292 - 4s - loss: 93.6288\n",
      "Epoch 72/100\n",
      "21292/21292 - 4s - loss: 93.5928\n",
      "Epoch 73/100\n",
      "21292/21292 - 4s - loss: 93.5720\n",
      "Epoch 74/100\n",
      "21292/21292 - 4s - loss: 93.7044\n",
      "Epoch 75/100\n",
      "21292/21292 - 4s - loss: 93.5558\n",
      "Epoch 76/100\n",
      "21292/21292 - 3s - loss: 93.4039\n",
      "Epoch 77/100\n",
      "21292/21292 - 4s - loss: 93.5717\n",
      "Epoch 78/100\n",
      "21292/21292 - 4s - loss: 93.6689\n",
      "Epoch 79/100\n",
      "21292/21292 - 4s - loss: 93.3737\n",
      "Epoch 80/100\n",
      "21292/21292 - 4s - loss: 93.4961\n",
      "Epoch 81/100\n",
      "21292/21292 - 4s - loss: 93.5842\n",
      "Epoch 82/100\n",
      "21292/21292 - 4s - loss: 93.4695\n",
      "Epoch 83/100\n",
      "21292/21292 - 4s - loss: 93.5229\n",
      "Epoch 84/100\n",
      "21292/21292 - 3s - loss: 93.5976\n",
      "Epoch 85/100\n",
      "21292/21292 - 4s - loss: 93.5094\n",
      "Epoch 86/100\n",
      "21292/21292 - 4s - loss: 93.5138\n",
      "Epoch 87/100\n",
      "21292/21292 - 4s - loss: 93.3330\n",
      "Epoch 88/100\n",
      "21292/21292 - 4s - loss: 93.3412\n",
      "Epoch 89/100\n",
      "21292/21292 - 4s - loss: 93.3655\n",
      "Epoch 90/100\n",
      "21292/21292 - 4s - loss: 93.4242\n",
      "Epoch 91/100\n",
      "21292/21292 - 4s - loss: 93.4034\n",
      "Epoch 92/100\n",
      "21292/21292 - 4s - loss: 93.5067\n",
      "Epoch 93/100\n",
      "21292/21292 - 4s - loss: 93.4557\n",
      "Epoch 94/100\n",
      "21292/21292 - 4s - loss: 93.4266\n",
      "Epoch 95/100\n",
      "21292/21292 - 4s - loss: 93.3927\n",
      "Epoch 96/100\n",
      "21292/21292 - 4s - loss: 93.3121\n",
      "Epoch 97/100\n",
      "21292/21292 - 4s - loss: 93.1380\n",
      "Epoch 98/100\n",
      "21292/21292 - 4s - loss: 93.2431\n",
      "Epoch 99/100\n",
      "21292/21292 - 4s - loss: 93.2580\n",
      "Epoch 100/100\n",
      "21292/21292 - 4s - loss: 93.1782\n",
      "2366/2366 - 0s - loss: 71.3032\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 4s - loss: 138.7544\n",
      "Epoch 2/100\n",
      "21292/21292 - 4s - loss: 106.3270\n",
      "Epoch 3/100\n",
      "21292/21292 - 4s - loss: 103.2927\n",
      "Epoch 4/100\n",
      "21292/21292 - 4s - loss: 102.5683\n",
      "Epoch 5/100\n",
      "21292/21292 - 4s - loss: 96.3893\n",
      "Epoch 6/100\n",
      "21292/21292 - 4s - loss: 93.9156\n",
      "Epoch 7/100\n",
      "21292/21292 - 4s - loss: 93.1060\n",
      "Epoch 8/100\n",
      "21292/21292 - 4s - loss: 92.8110\n",
      "Epoch 9/100\n",
      "21292/21292 - 4s - loss: 92.8201\n",
      "Epoch 10/100\n",
      "21292/21292 - 4s - loss: 92.6819\n",
      "Epoch 11/100\n",
      "21292/21292 - 4s - loss: 92.6524\n",
      "Epoch 12/100\n",
      "21292/21292 - 4s - loss: 92.5120\n",
      "Epoch 13/100\n",
      "21292/21292 - 4s - loss: 92.5032\n",
      "Epoch 14/100\n",
      "21292/21292 - 4s - loss: 92.3234\n",
      "Epoch 15/100\n",
      "21292/21292 - 4s - loss: 92.3859\n",
      "Epoch 16/100\n",
      "21292/21292 - 4s - loss: 92.3152\n",
      "Epoch 17/100\n",
      "21292/21292 - 4s - loss: 92.4153\n",
      "Epoch 18/100\n",
      "21292/21292 - 4s - loss: 92.2887\n",
      "Epoch 19/100\n",
      "21292/21292 - 4s - loss: 92.3347\n",
      "Epoch 20/100\n",
      "21292/21292 - 4s - loss: 92.3291\n",
      "Epoch 21/100\n",
      "21292/21292 - 4s - loss: 92.3144\n",
      "Epoch 22/100\n",
      "21292/21292 - 4s - loss: 92.2674\n",
      "Epoch 23/100\n",
      "21292/21292 - 4s - loss: 92.4564\n",
      "Epoch 24/100\n",
      "21292/21292 - 4s - loss: 92.1963\n",
      "Epoch 25/100\n",
      "21292/21292 - 4s - loss: 92.3442\n",
      "Epoch 26/100\n",
      "21292/21292 - 4s - loss: 92.2876\n",
      "Epoch 27/100\n",
      "21292/21292 - 4s - loss: 92.2639\n",
      "Epoch 28/100\n",
      "21292/21292 - 4s - loss: 92.2280\n",
      "Epoch 29/100\n",
      "21292/21292 - 4s - loss: 92.2707\n",
      "Epoch 30/100\n",
      "21292/21292 - 4s - loss: 92.2441\n",
      "Epoch 31/100\n",
      "21292/21292 - 4s - loss: 92.2680\n",
      "Epoch 32/100\n",
      "21292/21292 - 4s - loss: 92.1105\n",
      "Epoch 33/100\n",
      "21292/21292 - 4s - loss: 92.1473\n",
      "Epoch 34/100\n",
      "21292/21292 - 4s - loss: 92.1785\n",
      "Epoch 35/100\n",
      "21292/21292 - 4s - loss: 92.1669\n",
      "Epoch 36/100\n",
      "21292/21292 - 4s - loss: 91.9567\n",
      "Epoch 37/100\n",
      "21292/21292 - 4s - loss: 92.1353\n",
      "Epoch 38/100\n",
      "21292/21292 - 4s - loss: 92.1584\n",
      "Epoch 39/100\n",
      "21292/21292 - 4s - loss: 92.1909\n",
      "Epoch 40/100\n",
      "21292/21292 - 4s - loss: 92.1628\n",
      "Epoch 41/100\n",
      "21292/21292 - 4s - loss: 92.1984\n",
      "Epoch 42/100\n",
      "21292/21292 - 4s - loss: 91.9983\n",
      "Epoch 43/100\n",
      "21292/21292 - 4s - loss: 92.1306\n",
      "Epoch 44/100\n",
      "21292/21292 - 4s - loss: 91.9796\n",
      "Epoch 45/100\n",
      "21292/21292 - 4s - loss: 92.1906\n",
      "Epoch 46/100\n",
      "21292/21292 - 4s - loss: 92.1517\n",
      "Epoch 47/100\n",
      "21292/21292 - 4s - loss: 92.0998\n",
      "Epoch 48/100\n",
      "21292/21292 - 4s - loss: 92.0869\n",
      "Epoch 49/100\n",
      "21292/21292 - 5s - loss: 92.2208\n",
      "Epoch 50/100\n",
      "21292/21292 - 4s - loss: 92.1171\n",
      "Epoch 51/100\n",
      "21292/21292 - 7s - loss: 92.1437\n",
      "Epoch 52/100\n",
      "21292/21292 - 5s - loss: 92.1141\n",
      "Epoch 53/100\n",
      "21292/21292 - 4s - loss: 91.9275\n",
      "Epoch 54/100\n",
      "21292/21292 - 6s - loss: 92.2026\n",
      "Epoch 55/100\n",
      "21292/21292 - 5s - loss: 92.0663\n",
      "Epoch 56/100\n",
      "21292/21292 - 5s - loss: 92.0784\n",
      "Epoch 57/100\n",
      "21292/21292 - 5s - loss: 92.0731\n",
      "Epoch 58/100\n",
      "21292/21292 - 5s - loss: 92.1053\n",
      "Epoch 59/100\n",
      "21292/21292 - 5s - loss: 92.0161\n",
      "Epoch 60/100\n",
      "21292/21292 - 6s - loss: 92.0907\n",
      "Epoch 61/100\n",
      "21292/21292 - 6s - loss: 92.0953\n",
      "Epoch 62/100\n",
      "21292/21292 - 5s - loss: 92.0458\n",
      "Epoch 63/100\n",
      "21292/21292 - 4s - loss: 91.9909\n",
      "Epoch 64/100\n",
      "21292/21292 - 5s - loss: 92.0408\n",
      "Epoch 65/100\n",
      "21292/21292 - 5s - loss: 92.0056\n",
      "Epoch 66/100\n",
      "21292/21292 - 5s - loss: 91.8826\n",
      "Epoch 67/100\n",
      "21292/21292 - 5s - loss: 92.0544\n",
      "Epoch 68/100\n",
      "21292/21292 - 5s - loss: 91.9729\n",
      "Epoch 69/100\n",
      "21292/21292 - 5s - loss: 91.9412\n",
      "Epoch 70/100\n",
      "21292/21292 - 5s - loss: 91.8906\n",
      "Epoch 71/100\n",
      "21292/21292 - 5s - loss: 91.8868\n",
      "Epoch 72/100\n",
      "21292/21292 - 5s - loss: 92.0353\n",
      "Epoch 73/100\n",
      "21292/21292 - 6s - loss: 91.9665\n",
      "Epoch 74/100\n",
      "21292/21292 - 4s - loss: 91.9184\n",
      "Epoch 75/100\n",
      "21292/21292 - 6s - loss: 91.9936\n",
      "Epoch 76/100\n",
      "21292/21292 - 4s - loss: 91.9959\n",
      "Epoch 77/100\n",
      "21292/21292 - 5s - loss: 91.9540\n",
      "Epoch 78/100\n",
      "21292/21292 - 5s - loss: 91.8406\n",
      "Epoch 79/100\n",
      "21292/21292 - 5s - loss: 91.8280\n",
      "Epoch 80/100\n",
      "21292/21292 - 5s - loss: 91.9648\n",
      "Epoch 81/100\n",
      "21292/21292 - 5s - loss: 92.0787\n",
      "Epoch 82/100\n",
      "21292/21292 - 5s - loss: 91.9315\n",
      "Epoch 83/100\n",
      "21292/21292 - 5s - loss: 91.8449\n",
      "Epoch 84/100\n",
      "21292/21292 - 5s - loss: 91.9342\n",
      "Epoch 85/100\n",
      "21292/21292 - 5s - loss: 91.8466\n",
      "Epoch 86/100\n",
      "21292/21292 - 5s - loss: 91.6950\n",
      "Epoch 87/100\n",
      "21292/21292 - 5s - loss: 91.8741\n",
      "Epoch 88/100\n",
      "21292/21292 - 5s - loss: 91.7910\n",
      "Epoch 89/100\n",
      "21292/21292 - 5s - loss: 91.8562\n",
      "Epoch 90/100\n",
      "21292/21292 - 5s - loss: 91.7785\n",
      "Epoch 91/100\n",
      "21292/21292 - 5s - loss: 91.9447\n",
      "Epoch 92/100\n",
      "21292/21292 - 4s - loss: 91.7138\n",
      "Epoch 93/100\n",
      "21292/21292 - 5s - loss: 91.7071\n",
      "Epoch 94/100\n",
      "21292/21292 - 5s - loss: 91.8203\n",
      "Epoch 95/100\n",
      "21292/21292 - 5s - loss: 91.8846\n",
      "Epoch 96/100\n",
      "21292/21292 - 4s - loss: 91.7979\n",
      "Epoch 97/100\n",
      "21292/21292 - 5s - loss: 91.7937\n",
      "Epoch 98/100\n",
      "21292/21292 - 5s - loss: 91.7213\n",
      "Epoch 99/100\n",
      "21292/21292 - 5s - loss: 91.7662\n",
      "Epoch 100/100\n",
      "21292/21292 - 5s - loss: 91.7870\n",
      "2366/2366 - 0s - loss: 111.4087\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 6s - loss: 135.8542\n",
      "Epoch 2/100\n",
      "21292/21292 - 5s - loss: 105.6318\n",
      "Epoch 3/100\n",
      "21292/21292 - 5s - loss: 104.4794\n",
      "Epoch 4/100\n",
      "21292/21292 - 5s - loss: 104.6385\n",
      "Epoch 5/100\n",
      "21292/21292 - 5s - loss: 99.7064\n",
      "Epoch 6/100\n",
      "21292/21292 - 5s - loss: 96.2885\n",
      "Epoch 7/100\n",
      "21292/21292 - 5s - loss: 95.1115\n",
      "Epoch 8/100\n",
      "21292/21292 - 5s - loss: 94.7310\n",
      "Epoch 9/100\n",
      "21292/21292 - 5s - loss: 94.4142\n",
      "Epoch 10/100\n",
      "21292/21292 - 5s - loss: 94.1389\n",
      "Epoch 11/100\n",
      "21292/21292 - 5s - loss: 94.0220\n",
      "Epoch 12/100\n",
      "21292/21292 - 5s - loss: 93.8668\n",
      "Epoch 13/100\n",
      "21292/21292 - 5s - loss: 93.7787\n",
      "Epoch 14/100\n",
      "21292/21292 - 5s - loss: 93.1688\n",
      "Epoch 15/100\n",
      "21292/21292 - 4s - loss: 92.7446\n",
      "Epoch 16/100\n",
      "21292/21292 - 5s - loss: 92.5209\n",
      "Epoch 17/100\n",
      "21292/21292 - 5s - loss: 92.3131\n",
      "Epoch 18/100\n",
      "21292/21292 - 5s - loss: 92.3565\n",
      "Epoch 19/100\n",
      "21292/21292 - 5s - loss: 92.0663\n",
      "Epoch 20/100\n",
      "21292/21292 - 5s - loss: 92.0703\n",
      "Epoch 21/100\n",
      "21292/21292 - 5s - loss: 91.9880\n",
      "Epoch 22/100\n",
      "21292/21292 - 5s - loss: 91.8468\n",
      "Epoch 23/100\n",
      "21292/21292 - 5s - loss: 91.9523\n",
      "Epoch 24/100\n",
      "21292/21292 - 5s - loss: 92.0346\n",
      "Epoch 25/100\n",
      "21292/21292 - 5s - loss: 91.8396\n",
      "Epoch 26/100\n",
      "21292/21292 - 4s - loss: 91.7868\n",
      "Epoch 27/100\n",
      "21292/21292 - 5s - loss: 91.8667\n",
      "Epoch 28/100\n",
      "21292/21292 - 5s - loss: 92.0962\n",
      "Epoch 29/100\n",
      "21292/21292 - 5s - loss: 91.8595\n",
      "Epoch 30/100\n",
      "21292/21292 - 5s - loss: 91.8090\n",
      "Epoch 31/100\n",
      "21292/21292 - 5s - loss: 91.7176\n",
      "Epoch 32/100\n",
      "21292/21292 - 5s - loss: 91.7521\n",
      "Epoch 33/100\n",
      "21292/21292 - 5s - loss: 91.6044\n",
      "Epoch 34/100\n",
      "21292/21292 - 5s - loss: 91.6061\n",
      "Epoch 35/100\n",
      "21292/21292 - 5s - loss: 91.5922\n",
      "Epoch 36/100\n",
      "21292/21292 - 5s - loss: 91.5242\n",
      "Epoch 37/100\n",
      "21292/21292 - 5s - loss: 91.6350\n",
      "Epoch 38/100\n",
      "21292/21292 - 5s - loss: 91.6383\n",
      "Epoch 39/100\n",
      "21292/21292 - 5s - loss: 91.4960\n",
      "Epoch 40/100\n",
      "21292/21292 - 5s - loss: 91.6474\n",
      "Epoch 41/100\n",
      "21292/21292 - 5s - loss: 91.4515\n",
      "Epoch 42/100\n",
      "21292/21292 - 5s - loss: 91.3116\n",
      "Epoch 43/100\n",
      "21292/21292 - 5s - loss: 91.4698\n",
      "Epoch 44/100\n",
      "21292/21292 - 5s - loss: 91.5544\n",
      "Epoch 45/100\n",
      "21292/21292 - 5s - loss: 91.4443\n",
      "Epoch 46/100\n",
      "21292/21292 - 5s - loss: 91.5650\n",
      "Epoch 47/100\n",
      "21292/21292 - 5s - loss: 91.4525\n",
      "Epoch 48/100\n",
      "21292/21292 - 5s - loss: 91.4781\n",
      "Epoch 49/100\n",
      "21292/21292 - 5s - loss: 91.4745\n",
      "Epoch 50/100\n",
      "21292/21292 - 4s - loss: 91.3848\n",
      "Epoch 51/100\n",
      "21292/21292 - 5s - loss: 91.4583\n",
      "Epoch 52/100\n",
      "21292/21292 - 5s - loss: 91.4653\n",
      "Epoch 53/100\n",
      "21292/21292 - 5s - loss: 91.3103\n",
      "Epoch 54/100\n",
      "21292/21292 - 5s - loss: 91.3943\n",
      "Epoch 55/100\n",
      "21292/21292 - 5s - loss: 91.3597\n",
      "Epoch 56/100\n",
      "21292/21292 - 5s - loss: 91.4114\n",
      "Epoch 57/100\n",
      "21292/21292 - 5s - loss: 91.4055\n",
      "Epoch 58/100\n",
      "21292/21292 - 4s - loss: 91.3175\n",
      "Epoch 59/100\n",
      "21292/21292 - 5s - loss: 91.1445\n",
      "Epoch 60/100\n",
      "21292/21292 - 5s - loss: 91.3861\n",
      "Epoch 61/100\n",
      "21292/21292 - 5s - loss: 91.4640\n",
      "Epoch 62/100\n",
      "21292/21292 - 5s - loss: 91.3300\n",
      "Epoch 63/100\n",
      "21292/21292 - 5s - loss: 91.3647\n",
      "Epoch 64/100\n",
      "21292/21292 - 5s - loss: 91.3197\n",
      "Epoch 65/100\n",
      "21292/21292 - 5s - loss: 91.3764\n",
      "Epoch 66/100\n",
      "21292/21292 - 5s - loss: 91.3292\n",
      "Epoch 67/100\n",
      "21292/21292 - 5s - loss: 91.3487\n",
      "Epoch 68/100\n",
      "21292/21292 - 5s - loss: 91.3486\n",
      "Epoch 69/100\n",
      "21292/21292 - 5s - loss: 91.2658\n",
      "Epoch 70/100\n",
      "21292/21292 - 5s - loss: 91.2088\n",
      "Epoch 71/100\n",
      "21292/21292 - 5s - loss: 91.1793\n",
      "Epoch 72/100\n",
      "21292/21292 - 5s - loss: 91.3637\n",
      "Epoch 73/100\n",
      "21292/21292 - 5s - loss: 91.2406\n",
      "Epoch 74/100\n",
      "21292/21292 - 5s - loss: 91.2265\n",
      "Epoch 75/100\n",
      "21292/21292 - 5s - loss: 91.3429\n",
      "Epoch 76/100\n",
      "21292/21292 - 5s - loss: 91.1495\n",
      "Epoch 77/100\n",
      "21292/21292 - 5s - loss: 91.2218\n",
      "Epoch 78/100\n",
      "21292/21292 - 5s - loss: 91.2051\n",
      "Epoch 79/100\n",
      "21292/21292 - 5s - loss: 91.0752\n",
      "Epoch 80/100\n",
      "21292/21292 - 5s - loss: 91.1743\n",
      "Epoch 81/100\n",
      "21292/21292 - 5s - loss: 90.8434\n",
      "Epoch 82/100\n",
      "21292/21292 - 5s - loss: 91.1287\n",
      "Epoch 83/100\n",
      "21292/21292 - 5s - loss: 91.1679\n",
      "Epoch 84/100\n",
      "21292/21292 - 4s - loss: 90.9628\n",
      "Epoch 85/100\n",
      "21292/21292 - 5s - loss: 91.2301\n",
      "Epoch 86/100\n",
      "21292/21292 - 5s - loss: 91.2359\n",
      "Epoch 87/100\n",
      "21292/21292 - 5s - loss: 91.1709\n",
      "Epoch 88/100\n",
      "21292/21292 - 5s - loss: 91.1823\n",
      "Epoch 89/100\n",
      "21292/21292 - 5s - loss: 91.1607\n",
      "Epoch 90/100\n",
      "21292/21292 - 5s - loss: 91.0417\n",
      "Epoch 91/100\n",
      "21292/21292 - 5s - loss: 91.0915\n",
      "Epoch 92/100\n",
      "21292/21292 - 5s - loss: 91.1009\n",
      "Epoch 93/100\n",
      "21292/21292 - 5s - loss: 91.0004\n",
      "Epoch 94/100\n",
      "21292/21292 - 6s - loss: 91.0033\n",
      "Epoch 95/100\n",
      "21292/21292 - 6s - loss: 90.9710\n",
      "Epoch 96/100\n",
      "21292/21292 - 5s - loss: 90.9783\n",
      "Epoch 97/100\n",
      "21292/21292 - 5s - loss: 90.9288\n",
      "Epoch 98/100\n",
      "21292/21292 - 5s - loss: 90.9149\n",
      "Epoch 99/100\n",
      "21292/21292 - 5s - loss: 91.0503\n",
      "Epoch 100/100\n",
      "21292/21292 - 5s - loss: 90.8904\n",
      "2366/2366 - 0s - loss: 98.6506\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 5s - loss: 698.9275\n",
      "Epoch 2/100\n",
      "21292/21292 - 5s - loss: 534.2397\n",
      "Epoch 3/100\n",
      "21292/21292 - 5s - loss: 403.8854\n",
      "Epoch 4/100\n",
      "21292/21292 - 5s - loss: 306.7132\n",
      "Epoch 5/100\n",
      "21292/21292 - 5s - loss: 240.7248\n",
      "Epoch 6/100\n",
      "21292/21292 - 5s - loss: 202.1141\n",
      "Epoch 7/100\n",
      "21292/21292 - 5s - loss: 184.3113\n",
      "Epoch 8/100\n",
      "21292/21292 - 5s - loss: 178.0173\n",
      "Epoch 9/100\n",
      "21292/21292 - 5s - loss: 176.2669\n",
      "Epoch 10/100\n",
      "21292/21292 - 5s - loss: 175.8290\n",
      "Epoch 11/100\n",
      "21292/21292 - 5s - loss: 175.7188\n",
      "Epoch 12/100\n",
      "21292/21292 - 5s - loss: 175.6935\n",
      "Epoch 13/100\n",
      "21292/21292 - 6s - loss: 175.6886\n",
      "Epoch 14/100\n",
      "21292/21292 - 7s - loss: 175.6868\n",
      "Epoch 15/100\n",
      "21292/21292 - 6s - loss: 175.6862\n",
      "Epoch 16/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 17/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 18/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 19/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 20/100\n",
      "21292/21292 - 6s - loss: 175.6849\n",
      "Epoch 21/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 22/100\n",
      "21292/21292 - 6s - loss: 175.6857\n",
      "Epoch 23/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 24/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 25/100\n",
      "21292/21292 - 6s - loss: 175.6848\n",
      "Epoch 26/100\n",
      "21292/21292 - 7s - loss: 175.6865\n",
      "Epoch 27/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 28/100\n",
      "21292/21292 - 6s - loss: 175.6861\n",
      "Epoch 29/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 30/100\n",
      "21292/21292 - 6s - loss: 175.6851\n",
      "Epoch 31/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 32/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 33/100\n",
      "21292/21292 - 6s - loss: 175.6843\n",
      "Epoch 34/100\n",
      "21292/21292 - 6s - loss: 175.6855\n",
      "Epoch 35/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 36/100\n",
      "21292/21292 - 6s - loss: 175.6846\n",
      "Epoch 37/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 38/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 39/100\n",
      "21292/21292 - 7s - loss: 175.6849\n",
      "Epoch 40/100\n",
      "21292/21292 - 6s - loss: 175.6861\n",
      "Epoch 41/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 42/100\n",
      "21292/21292 - 6s - loss: 175.6863\n",
      "Epoch 43/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 44/100\n",
      "21292/21292 - 6s - loss: 175.6857\n",
      "Epoch 45/100\n",
      "21292/21292 - 6s - loss: 175.6863\n",
      "Epoch 46/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 47/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 48/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 49/100\n",
      "21292/21292 - 6s - loss: 175.6863\n",
      "Epoch 50/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 51/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 52/100\n",
      "21292/21292 - 6s - loss: 175.6851\n",
      "Epoch 53/100\n",
      "21292/21292 - 6s - loss: 175.6840\n",
      "Epoch 54/100\n",
      "21292/21292 - 6s - loss: 175.6855\n",
      "Epoch 55/100\n",
      "21292/21292 - 7s - loss: 175.6856\n",
      "Epoch 56/100\n",
      "21292/21292 - 7s - loss: 175.6850\n",
      "Epoch 57/100\n",
      "21292/21292 - 7s - loss: 175.6851\n",
      "Epoch 58/100\n",
      "21292/21292 - 7s - loss: 175.6866\n",
      "Epoch 59/100\n",
      "21292/21292 - 7s - loss: 175.6864\n",
      "Epoch 60/100\n",
      "21292/21292 - 6s - loss: 175.6857\n",
      "Epoch 61/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 62/100\n",
      "21292/21292 - 6s - loss: 175.6861\n",
      "Epoch 63/100\n",
      "21292/21292 - 5s - loss: 175.6854\n",
      "Epoch 64/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 65/100\n",
      "21292/21292 - 6s - loss: 175.6861\n",
      "Epoch 66/100\n",
      "21292/21292 - 6s - loss: 175.6864\n",
      "Epoch 67/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 68/100\n",
      "21292/21292 - 7s - loss: 175.6858\n",
      "Epoch 69/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 70/100\n",
      "21292/21292 - 6s - loss: 175.6847\n",
      "Epoch 71/100\n",
      "21292/21292 - 6s - loss: 175.6851\n",
      "Epoch 72/100\n",
      "21292/21292 - 6s - loss: 175.6851\n",
      "Epoch 73/100\n",
      "21292/21292 - 6s - loss: 175.6854\n",
      "Epoch 74/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 75/100\n",
      "21292/21292 - 6s - loss: 175.6853\n",
      "Epoch 76/100\n",
      "21292/21292 - 6s - loss: 175.6846\n",
      "Epoch 77/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 78/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 79/100\n",
      "21292/21292 - 6s - loss: 175.6837\n",
      "Epoch 80/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "Epoch 81/100\n",
      "21292/21292 - 6s - loss: 175.6852\n",
      "Epoch 82/100\n",
      "21292/21292 - 6s - loss: 175.6859\n",
      "Epoch 83/100\n",
      "21292/21292 - 6s - loss: 175.6857\n",
      "Epoch 84/100\n",
      "21292/21292 - 6s - loss: 175.6850\n",
      "Epoch 85/100\n",
      "21292/21292 - 6s - loss: 175.6855\n",
      "Epoch 86/100\n",
      "21292/21292 - 6s - loss: 175.6839\n",
      "Epoch 87/100\n",
      "21292/21292 - 6s - loss: 175.6860\n",
      "Epoch 88/100\n",
      "21292/21292 - 6s - loss: 175.6864\n",
      "Epoch 89/100\n",
      "21292/21292 - 6s - loss: 175.6840\n",
      "Epoch 90/100\n",
      "21292/21292 - 6s - loss: 175.6845\n",
      "Epoch 91/100\n",
      "21292/21292 - 6s - loss: 175.6855\n",
      "Epoch 92/100\n",
      "21292/21292 - 6s - loss: 175.6859\n",
      "Epoch 93/100\n",
      "21292/21292 - 6s - loss: 175.6848\n",
      "Epoch 94/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 95/100\n",
      "21292/21292 - 5s - loss: 175.6845\n",
      "Epoch 96/100\n",
      "21292/21292 - 6s - loss: 175.6866\n",
      "Epoch 97/100\n",
      "21292/21292 - 5s - loss: 175.6860\n",
      "Epoch 98/100\n",
      "21292/21292 - 6s - loss: 175.6858\n",
      "Epoch 99/100\n",
      "21292/21292 - 6s - loss: 175.6857\n",
      "Epoch 100/100\n",
      "21292/21292 - 6s - loss: 175.6856\n",
      "2366/2366 - 0s - loss: 168.9266\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 6s - loss: 144.9017\n",
      "Epoch 2/100\n",
      "21292/21292 - 6s - loss: 110.0451\n",
      "Epoch 3/100\n",
      "21292/21292 - 5s - loss: 100.3570\n",
      "Epoch 4/100\n",
      "21292/21292 - 7s - loss: 96.8560\n",
      "Epoch 5/100\n",
      "21292/21292 - 6s - loss: 96.0144\n",
      "Epoch 6/100\n",
      "21292/21292 - 6s - loss: 95.8168\n",
      "Epoch 7/100\n",
      "21292/21292 - 6s - loss: 95.6948\n",
      "Epoch 8/100\n",
      "21292/21292 - 6s - loss: 95.6545\n",
      "Epoch 9/100\n",
      "21292/21292 - 6s - loss: 95.5620\n",
      "Epoch 10/100\n",
      "21292/21292 - 6s - loss: 95.5042\n",
      "Epoch 11/100\n",
      "21292/21292 - 6s - loss: 95.4123\n",
      "Epoch 12/100\n",
      "21292/21292 - 6s - loss: 95.4787\n",
      "Epoch 13/100\n",
      "21292/21292 - 6s - loss: 95.2627\n",
      "Epoch 14/100\n",
      "21292/21292 - 6s - loss: 95.2706\n",
      "Epoch 15/100\n",
      "21292/21292 - 6s - loss: 95.4801\n",
      "Epoch 16/100\n",
      "21292/21292 - 6s - loss: 95.3563\n",
      "Epoch 17/100\n",
      "21292/21292 - 6s - loss: 95.1075\n",
      "Epoch 18/100\n",
      "21292/21292 - 6s - loss: 95.3108\n",
      "Epoch 19/100\n",
      "21292/21292 - 5s - loss: 95.2866\n",
      "Epoch 20/100\n",
      "21292/21292 - 6s - loss: 95.2668\n",
      "Epoch 21/100\n",
      "21292/21292 - 6s - loss: 95.1889\n",
      "Epoch 22/100\n",
      "21292/21292 - 6s - loss: 95.1510\n",
      "Epoch 23/100\n",
      "21292/21292 - 6s - loss: 95.0893\n",
      "Epoch 24/100\n",
      "21292/21292 - 6s - loss: 95.1709\n",
      "Epoch 25/100\n",
      "21292/21292 - 6s - loss: 95.0958\n",
      "Epoch 26/100\n",
      "21292/21292 - 6s - loss: 95.0601\n",
      "Epoch 27/100\n",
      "21292/21292 - 6s - loss: 95.1365\n",
      "Epoch 28/100\n",
      "21292/21292 - 5s - loss: 95.2395\n",
      "Epoch 29/100\n",
      "21292/21292 - 6s - loss: 95.1043\n",
      "Epoch 30/100\n",
      "21292/21292 - 6s - loss: 95.0410\n",
      "Epoch 31/100\n",
      "21292/21292 - 6s - loss: 95.0765\n",
      "Epoch 32/100\n",
      "21292/21292 - 6s - loss: 95.0669\n",
      "Epoch 33/100\n",
      "21292/21292 - 6s - loss: 95.0980\n",
      "Epoch 34/100\n",
      "21292/21292 - 6s - loss: 94.9809\n",
      "Epoch 35/100\n",
      "21292/21292 - 6s - loss: 95.0056\n",
      "Epoch 36/100\n",
      "21292/21292 - 6s - loss: 95.0545\n",
      "Epoch 37/100\n",
      "21292/21292 - 6s - loss: 94.8841\n",
      "Epoch 38/100\n",
      "21292/21292 - 6s - loss: 95.0569\n",
      "Epoch 39/100\n",
      "21292/21292 - 6s - loss: 94.9235\n",
      "Epoch 40/100\n",
      "21292/21292 - 6s - loss: 95.1018\n",
      "Epoch 41/100\n",
      "21292/21292 - 6s - loss: 94.7953\n",
      "Epoch 42/100\n",
      "21292/21292 - 6s - loss: 94.9823\n",
      "Epoch 43/100\n",
      "21292/21292 - 6s - loss: 94.9558\n",
      "Epoch 44/100\n",
      "21292/21292 - 6s - loss: 94.9563\n",
      "Epoch 45/100\n",
      "21292/21292 - 6s - loss: 95.0809\n",
      "Epoch 46/100\n",
      "21292/21292 - 6s - loss: 95.0272\n",
      "Epoch 47/100\n",
      "21292/21292 - 6s - loss: 94.9702\n",
      "Epoch 48/100\n",
      "21292/21292 - 6s - loss: 94.9416\n",
      "Epoch 49/100\n",
      "21292/21292 - 6s - loss: 94.9182\n",
      "Epoch 50/100\n",
      "21292/21292 - 6s - loss: 94.9478\n",
      "Epoch 51/100\n",
      "21292/21292 - 6s - loss: 94.8139\n",
      "Epoch 52/100\n",
      "21292/21292 - 6s - loss: 95.0721\n",
      "Epoch 53/100\n",
      "21292/21292 - 6s - loss: 94.8127\n",
      "Epoch 54/100\n",
      "21292/21292 - 6s - loss: 94.8350\n",
      "Epoch 55/100\n",
      "21292/21292 - 6s - loss: 94.7117\n",
      "Epoch 56/100\n",
      "21292/21292 - 6s - loss: 94.9421\n",
      "Epoch 57/100\n",
      "21292/21292 - 6s - loss: 94.7172\n",
      "Epoch 58/100\n",
      "21292/21292 - 6s - loss: 94.6975\n",
      "Epoch 59/100\n",
      "21292/21292 - 6s - loss: 94.7763\n",
      "Epoch 60/100\n",
      "21292/21292 - 6s - loss: 94.7782\n",
      "Epoch 61/100\n",
      "21292/21292 - 6s - loss: 94.7542\n",
      "Epoch 62/100\n",
      "21292/21292 - 6s - loss: 94.7460\n",
      "Epoch 63/100\n",
      "21292/21292 - 6s - loss: 94.6384\n",
      "Epoch 64/100\n",
      "21292/21292 - 6s - loss: 94.7341\n",
      "Epoch 65/100\n",
      "21292/21292 - 6s - loss: 94.8343\n",
      "Epoch 66/100\n",
      "21292/21292 - 6s - loss: 94.8462\n",
      "Epoch 67/100\n",
      "21292/21292 - 6s - loss: 94.5722\n",
      "Epoch 68/100\n",
      "21292/21292 - 6s - loss: 94.8678\n",
      "Epoch 69/100\n",
      "21292/21292 - 6s - loss: 94.8467\n",
      "Epoch 70/100\n",
      "21292/21292 - 6s - loss: 94.7333\n",
      "Epoch 71/100\n",
      "21292/21292 - 6s - loss: 94.7900\n",
      "Epoch 72/100\n",
      "21292/21292 - 6s - loss: 94.7439\n",
      "Epoch 73/100\n",
      "21292/21292 - 6s - loss: 94.5885\n",
      "Epoch 74/100\n",
      "21292/21292 - 6s - loss: 94.7024\n",
      "Epoch 75/100\n",
      "21292/21292 - 6s - loss: 94.6539\n",
      "Epoch 76/100\n",
      "21292/21292 - 6s - loss: 94.6278\n",
      "Epoch 77/100\n",
      "21292/21292 - 6s - loss: 94.6566\n",
      "Epoch 78/100\n",
      "21292/21292 - 6s - loss: 94.6343\n",
      "Epoch 79/100\n",
      "21292/21292 - 6s - loss: 94.5726\n",
      "Epoch 80/100\n",
      "21292/21292 - 6s - loss: 94.7703\n",
      "Epoch 81/100\n",
      "21292/21292 - 6s - loss: 94.5192\n",
      "Epoch 82/100\n",
      "21292/21292 - 6s - loss: 94.6708\n",
      "Epoch 83/100\n",
      "21292/21292 - 6s - loss: 94.4880\n",
      "Epoch 84/100\n",
      "21292/21292 - 6s - loss: 94.6983\n",
      "Epoch 85/100\n",
      "21292/21292 - 6s - loss: 94.6418\n",
      "Epoch 86/100\n",
      "21292/21292 - 6s - loss: 94.5476\n",
      "Epoch 87/100\n",
      "21292/21292 - 6s - loss: 94.5973\n",
      "Epoch 88/100\n",
      "21292/21292 - 6s - loss: 94.7378\n",
      "Epoch 89/100\n",
      "21292/21292 - 8s - loss: 94.4745\n",
      "Epoch 90/100\n",
      "21292/21292 - 6s - loss: 94.5137\n",
      "Epoch 91/100\n",
      "21292/21292 - 6s - loss: 94.6389\n",
      "Epoch 92/100\n",
      "21292/21292 - 6s - loss: 94.6517\n",
      "Epoch 93/100\n",
      "21292/21292 - 6s - loss: 94.5424\n",
      "Epoch 94/100\n",
      "21292/21292 - 7s - loss: 94.7392\n",
      "Epoch 95/100\n",
      "21292/21292 - 6s - loss: 94.5832\n",
      "Epoch 96/100\n",
      "21292/21292 - 6s - loss: 94.4192\n",
      "Epoch 97/100\n",
      "21292/21292 - 6s - loss: 94.5098\n",
      "Epoch 98/100\n",
      "21292/21292 - 6s - loss: 94.5142\n",
      "Epoch 99/100\n",
      "21292/21292 - 6s - loss: 94.5429\n",
      "Epoch 100/100\n",
      "21292/21292 - 7s - loss: 94.6159\n",
      "2366/2366 - 1s - loss: 87.1580\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 7s - loss: 132.8461\n",
      "Epoch 2/100\n",
      "21292/21292 - 6s - loss: 106.6416\n",
      "Epoch 3/100\n",
      "21292/21292 - 6s - loss: 99.6094\n",
      "Epoch 4/100\n",
      "21292/21292 - 6s - loss: 97.2371\n",
      "Epoch 5/100\n",
      "21292/21292 - 6s - loss: 96.9636\n",
      "Epoch 6/100\n",
      "21292/21292 - 6s - loss: 96.6718\n",
      "Epoch 7/100\n",
      "21292/21292 - 6s - loss: 96.4389\n",
      "Epoch 8/100\n",
      "21292/21292 - 6s - loss: 96.4227\n",
      "Epoch 9/100\n",
      "21292/21292 - 6s - loss: 96.3487\n",
      "Epoch 10/100\n",
      "21292/21292 - 6s - loss: 96.0736\n",
      "Epoch 11/100\n",
      "21292/21292 - 6s - loss: 95.5584\n",
      "Epoch 12/100\n",
      "21292/21292 - 6s - loss: 95.5325\n",
      "Epoch 13/100\n",
      "21292/21292 - 6s - loss: 95.3613\n",
      "Epoch 14/100\n",
      "21292/21292 - 6s - loss: 95.2599\n",
      "Epoch 15/100\n",
      "21292/21292 - 6s - loss: 95.3441\n",
      "Epoch 16/100\n",
      "21292/21292 - 6s - loss: 95.1299\n",
      "Epoch 17/100\n",
      "21292/21292 - 7s - loss: 94.7611\n",
      "Epoch 18/100\n",
      "21292/21292 - 6s - loss: 94.9266\n",
      "Epoch 19/100\n",
      "21292/21292 - 6s - loss: 94.7571\n",
      "Epoch 20/100\n",
      "21292/21292 - 6s - loss: 94.6452\n",
      "Epoch 21/100\n",
      "21292/21292 - 6s - loss: 94.3283\n",
      "Epoch 22/100\n",
      "21292/21292 - 7s - loss: 94.3795\n",
      "Epoch 23/100\n",
      "21292/21292 - 6s - loss: 94.3216\n",
      "Epoch 24/100\n",
      "21292/21292 - 6s - loss: 94.1489\n",
      "Epoch 25/100\n",
      "21292/21292 - 6s - loss: 94.2975\n",
      "Epoch 26/100\n",
      "21292/21292 - 6s - loss: 94.2399\n",
      "Epoch 27/100\n",
      "21292/21292 - 6s - loss: 94.1065\n",
      "Epoch 28/100\n",
      "21292/21292 - 6s - loss: 94.1869\n",
      "Epoch 29/100\n",
      "21292/21292 - 6s - loss: 94.2403\n",
      "Epoch 30/100\n",
      "21292/21292 - 6s - loss: 94.2001\n",
      "Epoch 31/100\n",
      "21292/21292 - 6s - loss: 94.0221\n",
      "Epoch 32/100\n",
      "21292/21292 - 6s - loss: 94.1022\n",
      "Epoch 33/100\n",
      "21292/21292 - 6s - loss: 94.1163\n",
      "Epoch 34/100\n",
      "21292/21292 - 6s - loss: 94.0497\n",
      "Epoch 35/100\n",
      "21292/21292 - 6s - loss: 94.1944\n",
      "Epoch 36/100\n",
      "21292/21292 - 6s - loss: 94.1083\n",
      "Epoch 37/100\n",
      "21292/21292 - 6s - loss: 94.2216\n",
      "Epoch 38/100\n",
      "21292/21292 - 6s - loss: 94.0086\n",
      "Epoch 39/100\n",
      "21292/21292 - 6s - loss: 93.8602\n",
      "Epoch 40/100\n",
      "21292/21292 - 6s - loss: 93.9851\n",
      "Epoch 41/100\n",
      "21292/21292 - 6s - loss: 94.1594\n",
      "Epoch 42/100\n",
      "21292/21292 - 6s - loss: 93.9681\n",
      "Epoch 43/100\n",
      "21292/21292 - 6s - loss: 94.1301\n",
      "Epoch 44/100\n",
      "21292/21292 - 6s - loss: 93.7250\n",
      "Epoch 45/100\n",
      "21292/21292 - 6s - loss: 93.8104\n",
      "Epoch 46/100\n",
      "21292/21292 - 6s - loss: 93.7510\n",
      "Epoch 47/100\n",
      "21292/21292 - 6s - loss: 93.9562\n",
      "Epoch 48/100\n",
      "21292/21292 - 6s - loss: 93.8344\n",
      "Epoch 49/100\n",
      "21292/21292 - 6s - loss: 93.8084\n",
      "Epoch 50/100\n",
      "21292/21292 - 6s - loss: 93.8266\n",
      "Epoch 51/100\n",
      "21292/21292 - 6s - loss: 93.6318\n",
      "Epoch 52/100\n",
      "21292/21292 - 6s - loss: 93.5372\n",
      "Epoch 53/100\n",
      "21292/21292 - 6s - loss: 93.5741\n",
      "Epoch 54/100\n",
      "21292/21292 - 6s - loss: 93.4024\n",
      "Epoch 55/100\n",
      "21292/21292 - 6s - loss: 93.3962\n",
      "Epoch 56/100\n",
      "21292/21292 - 6s - loss: 93.4410\n",
      "Epoch 57/100\n",
      "21292/21292 - 6s - loss: 93.1882\n",
      "Epoch 58/100\n",
      "21292/21292 - 6s - loss: 93.2139\n",
      "Epoch 59/100\n",
      "21292/21292 - 6s - loss: 93.3642\n",
      "Epoch 60/100\n",
      "21292/21292 - 6s - loss: 93.1269\n",
      "Epoch 61/100\n",
      "21292/21292 - 6s - loss: 92.9823\n",
      "Epoch 62/100\n",
      "21292/21292 - 6s - loss: 93.1863\n",
      "Epoch 63/100\n",
      "21292/21292 - 6s - loss: 93.1886\n",
      "Epoch 64/100\n",
      "21292/21292 - 6s - loss: 93.1593\n",
      "Epoch 65/100\n",
      "21292/21292 - 6s - loss: 93.0127\n",
      "Epoch 66/100\n",
      "21292/21292 - 6s - loss: 93.0598\n",
      "Epoch 67/100\n",
      "21292/21292 - 6s - loss: 93.0520\n",
      "Epoch 68/100\n",
      "21292/21292 - 6s - loss: 93.0437\n",
      "Epoch 69/100\n",
      "21292/21292 - 7s - loss: 93.2612\n",
      "Epoch 70/100\n",
      "21292/21292 - 6s - loss: 93.2308\n",
      "Epoch 71/100\n",
      "21292/21292 - 6s - loss: 92.9328\n",
      "Epoch 72/100\n",
      "21292/21292 - 6s - loss: 93.0558\n",
      "Epoch 73/100\n",
      "21292/21292 - 6s - loss: 92.7996\n",
      "Epoch 74/100\n",
      "21292/21292 - 6s - loss: 92.9102\n",
      "Epoch 75/100\n",
      "21292/21292 - 6s - loss: 93.0456\n",
      "Epoch 76/100\n",
      "21292/21292 - 6s - loss: 92.9429\n",
      "Epoch 77/100\n",
      "21292/21292 - 6s - loss: 92.7464\n",
      "Epoch 78/100\n",
      "21292/21292 - 6s - loss: 92.8075\n",
      "Epoch 79/100\n",
      "21292/21292 - 7s - loss: 92.6921\n",
      "Epoch 80/100\n",
      "21292/21292 - 6s - loss: 92.6946\n",
      "Epoch 81/100\n",
      "21292/21292 - 6s - loss: 92.7431\n",
      "Epoch 82/100\n",
      "21292/21292 - 6s - loss: 92.7274\n",
      "Epoch 83/100\n",
      "21292/21292 - 7s - loss: 92.7355\n",
      "Epoch 84/100\n",
      "21292/21292 - 6s - loss: 92.6582\n",
      "Epoch 85/100\n",
      "21292/21292 - 7s - loss: 92.6621\n",
      "Epoch 86/100\n",
      "21292/21292 - 8s - loss: 92.4857\n",
      "Epoch 87/100\n",
      "21292/21292 - 8s - loss: 92.3917\n",
      "Epoch 88/100\n",
      "21292/21292 - 8s - loss: 92.4864\n",
      "Epoch 89/100\n",
      "21292/21292 - 9s - loss: 92.6124\n",
      "Epoch 90/100\n",
      "21292/21292 - 7s - loss: 92.4175\n",
      "Epoch 91/100\n",
      "21292/21292 - 6s - loss: 92.6823\n",
      "Epoch 92/100\n",
      "21292/21292 - 7s - loss: 92.3507\n",
      "Epoch 93/100\n",
      "21292/21292 - 6s - loss: 92.4401\n",
      "Epoch 94/100\n",
      "21292/21292 - 6s - loss: 92.2546\n",
      "Epoch 95/100\n",
      "21292/21292 - 7s - loss: 92.5197\n",
      "Epoch 96/100\n",
      "21292/21292 - 6s - loss: 92.3982\n",
      "Epoch 97/100\n",
      "21292/21292 - 6s - loss: 92.4355\n",
      "Epoch 98/100\n",
      "21292/21292 - 6s - loss: 92.3629\n",
      "Epoch 99/100\n",
      "21292/21292 - 6s - loss: 92.2780\n",
      "Epoch 100/100\n",
      "21292/21292 - 6s - loss: 92.4330\n",
      "2366/2366 - 0s - loss: 73.5199\n",
      "Train on 21292 samples\n",
      "Epoch 1/100\n",
      "21292/21292 - 7s - loss: 135.5118\n",
      "Epoch 2/100\n",
      "21292/21292 - 6s - loss: 101.2773\n",
      "Epoch 3/100\n",
      "21292/21292 - 6s - loss: 97.3845\n",
      "Epoch 4/100\n",
      "21292/21292 - 6s - loss: 96.6236\n",
      "Epoch 5/100\n",
      "21292/21292 - 7s - loss: 96.1045\n",
      "Epoch 6/100\n",
      "21292/21292 - 6s - loss: 96.1686\n",
      "Epoch 7/100\n",
      "21292/21292 - 6s - loss: 95.8048\n",
      "Epoch 8/100\n",
      "21292/21292 - 6s - loss: 96.0126\n",
      "Epoch 9/100\n",
      "21292/21292 - 7s - loss: 95.7598\n",
      "Epoch 10/100\n",
      "21292/21292 - 8s - loss: 95.7223\n",
      "Epoch 11/100\n",
      "21292/21292 - 9s - loss: 95.8599\n",
      "Epoch 12/100\n",
      "21292/21292 - 8s - loss: 95.4997\n",
      "Epoch 13/100\n",
      "21292/21292 - 7s - loss: 95.5842\n",
      "Epoch 14/100\n",
      "21292/21292 - 6s - loss: 95.7035\n",
      "Epoch 15/100\n",
      "21292/21292 - 6s - loss: 95.6119\n",
      "Epoch 16/100\n",
      "21292/21292 - 6s - loss: 95.4319\n",
      "Epoch 17/100\n",
      "21292/21292 - 6s - loss: 95.4717\n",
      "Epoch 18/100\n",
      "21292/21292 - 6s - loss: 95.5525\n",
      "Epoch 19/100\n",
      "21292/21292 - 6s - loss: 95.3199\n",
      "Epoch 20/100\n",
      "21292/21292 - 6s - loss: 95.3049\n",
      "Epoch 21/100\n",
      "21292/21292 - 6s - loss: 95.2117\n",
      "Epoch 22/100\n",
      "21292/21292 - 6s - loss: 95.2830\n",
      "Epoch 23/100\n",
      "21292/21292 - 6s - loss: 95.5663\n",
      "Epoch 24/100\n",
      "21292/21292 - 6s - loss: 95.2155\n",
      "Epoch 25/100\n",
      "21292/21292 - 6s - loss: 95.3449\n",
      "Epoch 26/100\n",
      "21292/21292 - 6s - loss: 95.4022\n",
      "Epoch 27/100\n",
      "21292/21292 - 7s - loss: 95.3887\n",
      "Epoch 28/100\n",
      "21292/21292 - 7s - loss: 95.3138\n",
      "Epoch 29/100\n",
      "21292/21292 - 7s - loss: 95.2275\n",
      "Epoch 30/100\n",
      "21292/21292 - 6s - loss: 95.2851\n",
      "Epoch 31/100\n",
      "21292/21292 - 6s - loss: 95.2430\n",
      "Epoch 32/100\n",
      "21292/21292 - 9s - loss: 95.1991\n",
      "Epoch 33/100\n",
      "21292/21292 - 7s - loss: 95.2786\n",
      "Epoch 34/100\n",
      "21292/21292 - 7s - loss: 95.3584\n",
      "Epoch 35/100\n",
      "21292/21292 - 9s - loss: 95.2105\n",
      "Epoch 36/100\n",
      "21292/21292 - 9s - loss: 95.1213\n",
      "Epoch 37/100\n",
      "21292/21292 - 9s - loss: 95.2214\n",
      "Epoch 38/100\n",
      "21292/21292 - 8s - loss: 95.3534\n",
      "Epoch 39/100\n",
      "21292/21292 - 6s - loss: 95.1567\n",
      "Epoch 40/100\n",
      "21292/21292 - 6s - loss: 95.2083\n",
      "Epoch 41/100\n",
      "21292/21292 - 6s - loss: 94.9799\n",
      "Epoch 42/100\n",
      "21292/21292 - 6s - loss: 95.0856\n",
      "Epoch 43/100\n",
      "21292/21292 - 6s - loss: 95.3524\n",
      "Epoch 44/100\n",
      "21292/21292 - 6s - loss: 94.9775\n",
      "Epoch 45/100\n",
      "21292/21292 - 6s - loss: 95.0344\n",
      "Epoch 46/100\n",
      "21292/21292 - 6s - loss: 95.1281\n",
      "Epoch 47/100\n",
      "21292/21292 - 7s - loss: 95.0213\n",
      "Epoch 48/100\n",
      "21292/21292 - 6s - loss: 95.1175\n",
      "Epoch 49/100\n",
      "21292/21292 - 6s - loss: 95.1236\n",
      "Epoch 50/100\n",
      "21292/21292 - 6s - loss: 95.1409\n",
      "Epoch 51/100\n",
      "21292/21292 - 6s - loss: 94.9611\n",
      "Epoch 52/100\n",
      "21292/21292 - 6s - loss: 95.1064\n",
      "Epoch 53/100\n",
      "21292/21292 - 6s - loss: 95.2539\n",
      "Epoch 54/100\n",
      "21292/21292 - 6s - loss: 95.1150\n",
      "Epoch 55/100\n",
      "21292/21292 - 6s - loss: 95.0715\n",
      "Epoch 56/100\n",
      "21292/21292 - 6s - loss: 94.9270\n",
      "Epoch 57/100\n",
      "21292/21292 - 6s - loss: 94.8316\n",
      "Epoch 58/100\n",
      "21292/21292 - 6s - loss: 95.0477\n",
      "Epoch 59/100\n",
      "21292/21292 - 6s - loss: 94.9257\n",
      "Epoch 60/100\n",
      "21292/21292 - 6s - loss: 95.0483\n",
      "Epoch 61/100\n",
      "21292/21292 - 6s - loss: 94.9947\n",
      "Epoch 62/100\n",
      "21292/21292 - 6s - loss: 94.7625\n",
      "Epoch 63/100\n",
      "21292/21292 - 6s - loss: 94.5598\n",
      "Epoch 64/100\n",
      "21292/21292 - 6s - loss: 94.6015\n",
      "Epoch 65/100\n",
      "21292/21292 - 6s - loss: 94.2721\n",
      "Epoch 66/100\n",
      "21292/21292 - 6s - loss: 94.0114\n",
      "Epoch 67/100\n",
      "21292/21292 - 6s - loss: 93.7469\n",
      "Epoch 68/100\n",
      "21292/21292 - 6s - loss: 93.4919\n",
      "Epoch 69/100\n",
      "21292/21292 - 6s - loss: 93.4501\n",
      "Epoch 70/100\n",
      "21292/21292 - 6s - loss: 93.3409\n",
      "Epoch 71/100\n",
      "21292/21292 - 6s - loss: 93.2509\n",
      "Epoch 72/100\n",
      "21292/21292 - 6s - loss: 93.3025\n",
      "Epoch 73/100\n",
      "21292/21292 - 6s - loss: 93.0206\n",
      "Epoch 74/100\n",
      "21292/21292 - 6s - loss: 93.2213\n",
      "Epoch 75/100\n",
      "21292/21292 - 6s - loss: 93.0476\n",
      "Epoch 76/100\n",
      "21292/21292 - 6s - loss: 92.8588\n",
      "Epoch 77/100\n",
      "21292/21292 - 6s - loss: 92.9428\n",
      "Epoch 78/100\n",
      "21292/21292 - 6s - loss: 92.9883\n",
      "Epoch 79/100\n",
      "21292/21292 - 6s - loss: 92.5625\n",
      "Epoch 80/100\n",
      "21292/21292 - 6s - loss: 92.9735\n",
      "Epoch 81/100\n",
      "21292/21292 - 6s - loss: 92.8842\n",
      "Epoch 82/100\n",
      "21292/21292 - 6s - loss: 92.7815\n",
      "Epoch 83/100\n",
      "21292/21292 - 8s - loss: 92.6842\n",
      "Epoch 84/100\n",
      "21292/21292 - 10s - loss: 92.8083\n",
      "Epoch 85/100\n",
      "21292/21292 - 6s - loss: 92.7502\n",
      "Epoch 86/100\n",
      "21292/21292 - 6s - loss: 92.7204\n",
      "Epoch 87/100\n",
      "21292/21292 - 6s - loss: 92.5726\n",
      "Epoch 88/100\n",
      "21292/21292 - 6s - loss: 92.6988\n",
      "Epoch 89/100\n",
      "21292/21292 - 6s - loss: 92.5450\n",
      "Epoch 90/100\n",
      "21292/21292 - 6s - loss: 92.6499\n",
      "Epoch 91/100\n",
      "21292/21292 - 6s - loss: 92.5896\n",
      "Epoch 92/100\n",
      "21292/21292 - 6s - loss: 92.5415\n",
      "Epoch 93/100\n",
      "21292/21292 - 6s - loss: 92.4650\n",
      "Epoch 94/100\n",
      "21292/21292 - 6s - loss: 92.5425\n",
      "Epoch 95/100\n",
      "21292/21292 - 6s - loss: 92.3680\n",
      "Epoch 96/100\n",
      "21292/21292 - 6s - loss: 92.4526\n",
      "Epoch 97/100\n",
      "21292/21292 - 7s - loss: 92.5267\n",
      "Epoch 98/100\n",
      "21292/21292 - 6s - loss: 92.4272\n",
      "Epoch 99/100\n",
      "21292/21292 - 6s - loss: 92.3521\n",
      "Epoch 100/100\n",
      "21292/21292 - 6s - loss: 92.4488\n",
      "2366/2366 - 0s - loss: 84.4558\n",
      "Train on 21293 samples\n",
      "Epoch 1/100\n",
      "21293/21293 - 7s - loss: 137.3066\n",
      "Epoch 2/100\n",
      "21293/21293 - 6s - loss: 106.8808\n",
      "Epoch 3/100\n",
      "21293/21293 - 6s - loss: 103.9678\n",
      "Epoch 4/100\n",
      "21293/21293 - 6s - loss: 103.5648\n",
      "Epoch 5/100\n",
      "21293/21293 - 6s - loss: 103.5084\n",
      "Epoch 6/100\n",
      "21293/21293 - 6s - loss: 101.7060\n",
      "Epoch 7/100\n",
      "21293/21293 - 6s - loss: 95.0392\n",
      "Epoch 8/100\n",
      "21293/21293 - 6s - loss: 93.9694\n",
      "Epoch 9/100\n",
      "21293/21293 - 6s - loss: 93.6656\n",
      "Epoch 10/100\n",
      "21293/21293 - 6s - loss: 93.5037\n",
      "Epoch 11/100\n",
      "21293/21293 - 6s - loss: 93.3148\n",
      "Epoch 12/100\n",
      "21293/21293 - 6s - loss: 93.4061\n",
      "Epoch 13/100\n",
      "21293/21293 - 6s - loss: 93.4373\n",
      "Epoch 14/100\n",
      "21293/21293 - 6s - loss: 93.0845\n",
      "Epoch 15/100\n",
      "21293/21293 - 6s - loss: 93.1874\n",
      "Epoch 16/100\n",
      "21293/21293 - 6s - loss: 93.2566\n",
      "Epoch 17/100\n",
      "21293/21293 - 6s - loss: 93.1473\n",
      "Epoch 18/100\n",
      "21293/21293 - 6s - loss: 93.2248\n",
      "Epoch 19/100\n",
      "21293/21293 - 6s - loss: 93.1174\n",
      "Epoch 20/100\n",
      "21293/21293 - 6s - loss: 93.1797\n",
      "Epoch 21/100\n",
      "21293/21293 - 6s - loss: 93.0596\n",
      "Epoch 22/100\n",
      "21293/21293 - 6s - loss: 93.0646\n",
      "Epoch 23/100\n",
      "21293/21293 - 6s - loss: 93.0777\n",
      "Epoch 24/100\n",
      "21293/21293 - 6s - loss: 93.0284\n",
      "Epoch 25/100\n",
      "21293/21293 - 6s - loss: 93.0849\n",
      "Epoch 26/100\n",
      "21293/21293 - 6s - loss: 93.1373\n",
      "Epoch 27/100\n",
      "21293/21293 - 6s - loss: 93.0523\n",
      "Epoch 28/100\n",
      "21293/21293 - 6s - loss: 92.9661\n",
      "Epoch 29/100\n",
      "21293/21293 - 6s - loss: 93.0084\n",
      "Epoch 30/100\n",
      "21293/21293 - 6s - loss: 92.8898\n",
      "Epoch 31/100\n",
      "21293/21293 - 6s - loss: 93.0819\n",
      "Epoch 32/100\n",
      "21293/21293 - 6s - loss: 92.9792\n",
      "Epoch 33/100\n",
      "21293/21293 - 6s - loss: 92.9597\n",
      "Epoch 34/100\n",
      "21293/21293 - 6s - loss: 92.9998\n",
      "Epoch 35/100\n",
      "21293/21293 - 6s - loss: 92.9330\n",
      "Epoch 36/100\n",
      "21293/21293 - 6s - loss: 92.8202\n",
      "Epoch 37/100\n",
      "21293/21293 - 6s - loss: 92.8865\n",
      "Epoch 38/100\n",
      "21293/21293 - 6s - loss: 92.7355\n",
      "Epoch 39/100\n",
      "21293/21293 - 6s - loss: 92.9624\n",
      "Epoch 40/100\n",
      "21293/21293 - 6s - loss: 92.9153\n",
      "Epoch 41/100\n",
      "21293/21293 - 6s - loss: 92.8762\n",
      "Epoch 42/100\n",
      "21293/21293 - 7s - loss: 92.9559\n",
      "Epoch 43/100\n",
      "21293/21293 - 9s - loss: 92.7735\n",
      "Epoch 44/100\n",
      "21293/21293 - 8s - loss: 92.8607\n",
      "Epoch 45/100\n",
      "21293/21293 - 8s - loss: 93.0217\n",
      "Epoch 46/100\n",
      "21293/21293 - 8s - loss: 92.9095\n",
      "Epoch 47/100\n",
      "21293/21293 - 6s - loss: 92.6594\n",
      "Epoch 48/100\n",
      "21293/21293 - 6s - loss: 92.8281\n",
      "Epoch 49/100\n",
      "21293/21293 - 6s - loss: 92.8730\n",
      "Epoch 50/100\n",
      "21293/21293 - 6s - loss: 92.8840\n",
      "Epoch 51/100\n",
      "21293/21293 - 6s - loss: 92.7610\n",
      "Epoch 52/100\n",
      "21293/21293 - 6s - loss: 92.8349\n",
      "Epoch 53/100\n",
      "21293/21293 - 6s - loss: 92.7442\n",
      "Epoch 54/100\n",
      "21293/21293 - 6s - loss: 92.7619\n",
      "Epoch 55/100\n",
      "21293/21293 - 6s - loss: 92.7092\n",
      "Epoch 56/100\n",
      "21293/21293 - 5s - loss: 92.7376\n",
      "Epoch 57/100\n",
      "21293/21293 - 5s - loss: 92.7342\n",
      "Epoch 58/100\n",
      "21293/21293 - 6s - loss: 92.7650\n",
      "Epoch 59/100\n",
      "21293/21293 - 5s - loss: 92.9010\n",
      "Epoch 60/100\n",
      "21293/21293 - 6s - loss: 92.8557\n",
      "Epoch 61/100\n",
      "21293/21293 - 6s - loss: 92.7753\n",
      "Epoch 62/100\n",
      "21293/21293 - 6s - loss: 92.7588\n",
      "Epoch 63/100\n",
      "21293/21293 - 6s - loss: 92.6970\n",
      "Epoch 64/100\n",
      "21293/21293 - 6s - loss: 92.6491\n",
      "Epoch 65/100\n",
      "21293/21293 - 6s - loss: 92.8087\n",
      "Epoch 66/100\n",
      "21293/21293 - 6s - loss: 92.6989\n",
      "Epoch 67/100\n",
      "21293/21293 - 6s - loss: 92.6462\n",
      "Epoch 68/100\n",
      "21293/21293 - 8s - loss: 92.7704\n",
      "Epoch 69/100\n",
      "21293/21293 - 8s - loss: 92.5884\n",
      "Epoch 70/100\n",
      "21293/21293 - 8s - loss: 92.6725\n",
      "Epoch 71/100\n",
      "21293/21293 - 8s - loss: 92.6521\n",
      "Epoch 72/100\n",
      "21293/21293 - 8s - loss: 92.7658\n",
      "Epoch 73/100\n",
      "21293/21293 - 8s - loss: 92.6760\n",
      "Epoch 74/100\n",
      "21293/21293 - 8s - loss: 92.6809\n",
      "Epoch 75/100\n",
      "21293/21293 - 8s - loss: 92.5932\n",
      "Epoch 76/100\n",
      "21293/21293 - 6s - loss: 92.6591\n",
      "Epoch 77/100\n",
      "21293/21293 - 6s - loss: 92.4935\n",
      "Epoch 78/100\n",
      "21293/21293 - 6s - loss: 92.5246\n",
      "Epoch 79/100\n",
      "21293/21293 - 6s - loss: 92.7032\n",
      "Epoch 80/100\n",
      "21293/21293 - 7s - loss: 92.5637\n",
      "Epoch 81/100\n",
      "21293/21293 - 6s - loss: 92.5885\n",
      "Epoch 82/100\n",
      "21293/21293 - 7s - loss: 92.5731\n",
      "Epoch 83/100\n",
      "21293/21293 - 6s - loss: 92.5980\n",
      "Epoch 84/100\n",
      "21293/21293 - 6s - loss: 92.6310\n",
      "Epoch 85/100\n",
      "21293/21293 - 6s - loss: 92.6604\n",
      "Epoch 86/100\n",
      "21293/21293 - 6s - loss: 92.5946\n",
      "Epoch 87/100\n",
      "21293/21293 - 6s - loss: 92.6164\n",
      "Epoch 88/100\n",
      "21293/21293 - 6s - loss: 92.4328\n",
      "Epoch 89/100\n",
      "21293/21293 - 6s - loss: 92.6436\n",
      "Epoch 90/100\n",
      "21293/21293 - 6s - loss: 92.5319\n",
      "Epoch 91/100\n",
      "21293/21293 - 6s - loss: 92.3495\n",
      "Epoch 92/100\n",
      "21293/21293 - 6s - loss: 92.5542\n",
      "Epoch 93/100\n",
      "21293/21293 - 7s - loss: 92.5702\n",
      "Epoch 94/100\n",
      "21293/21293 - 8s - loss: 92.4459\n",
      "Epoch 95/100\n",
      "21293/21293 - 8s - loss: 92.5181\n",
      "Epoch 96/100\n",
      "21293/21293 - 9s - loss: 92.4709\n",
      "Epoch 97/100\n",
      "21293/21293 - 9s - loss: 92.5600\n",
      "Epoch 98/100\n",
      "21293/21293 - 8s - loss: 92.5437\n",
      "Epoch 99/100\n",
      "21293/21293 - 8s - loss: 92.3015\n",
      "Epoch 100/100\n",
      "21293/21293 - 8s - loss: 92.4164\n",
      "2365/2365 - 1s - loss: 105.0668\n",
      "Train on 21293 samples\n",
      "Epoch 1/100\n",
      "21293/21293 - 9s - loss: 153.1459\n",
      "Epoch 2/100\n",
      "21293/21293 - 8s - loss: 116.0966\n",
      "Epoch 3/100\n",
      "21293/21293 - 8s - loss: 109.5369\n",
      "Epoch 4/100\n",
      "21293/21293 - 8s - loss: 107.4331\n",
      "Epoch 5/100\n",
      "21293/21293 - 7s - loss: 106.6987\n",
      "Epoch 6/100\n",
      "21293/21293 - 6s - loss: 106.6275\n",
      "Epoch 7/100\n",
      "21293/21293 - 6s - loss: 106.4645\n",
      "Epoch 8/100\n",
      "21293/21293 - 6s - loss: 106.4434\n",
      "Epoch 9/100\n",
      "21293/21293 - 6s - loss: 106.6238\n",
      "Epoch 10/100\n",
      "21293/21293 - 6s - loss: 106.3910\n",
      "Epoch 11/100\n",
      "21293/21293 - 6s - loss: 106.5870\n",
      "Epoch 12/100\n",
      "21293/21293 - 6s - loss: 106.4402\n",
      "Epoch 13/100\n",
      "21293/21293 - 6s - loss: 106.4943\n",
      "Epoch 14/100\n",
      "21293/21293 - 6s - loss: 106.3467\n",
      "Epoch 15/100\n",
      "21293/21293 - 6s - loss: 106.4173\n",
      "Epoch 16/100\n",
      "21293/21293 - 6s - loss: 106.4220\n",
      "Epoch 17/100\n",
      "21293/21293 - 6s - loss: 106.5174\n",
      "Epoch 18/100\n",
      "21293/21293 - 6s - loss: 106.5632\n",
      "Epoch 19/100\n",
      "21293/21293 - 6s - loss: 106.4194\n",
      "Epoch 20/100\n",
      "21293/21293 - 6s - loss: 106.2940\n",
      "Epoch 21/100\n",
      "21293/21293 - 6s - loss: 106.5405\n",
      "Epoch 22/100\n",
      "21293/21293 - 6s - loss: 106.3510\n",
      "Epoch 23/100\n",
      "21293/21293 - 6s - loss: 106.3675\n",
      "Epoch 24/100\n",
      "21293/21293 - 6s - loss: 106.3386\n",
      "Epoch 25/100\n",
      "21293/21293 - 6s - loss: 106.4707\n",
      "Epoch 26/100\n",
      "21293/21293 - 6s - loss: 106.4105\n",
      "Epoch 27/100\n",
      "21293/21293 - 6s - loss: 106.4015\n",
      "Epoch 28/100\n",
      "21293/21293 - 6s - loss: 106.4317\n",
      "Epoch 29/100\n",
      "21293/21293 - 6s - loss: 106.4705\n",
      "Epoch 30/100\n",
      "21293/21293 - 6s - loss: 106.3379\n",
      "Epoch 31/100\n",
      "21293/21293 - 6s - loss: 106.4191\n",
      "Epoch 32/100\n",
      "21293/21293 - 6s - loss: 106.3257\n",
      "Epoch 33/100\n",
      "21293/21293 - 6s - loss: 106.3796\n",
      "Epoch 34/100\n",
      "21293/21293 - 6s - loss: 106.3677\n",
      "Epoch 35/100\n",
      "21293/21293 - 6s - loss: 106.5028\n",
      "Epoch 36/100\n",
      "21293/21293 - 6s - loss: 106.4080\n",
      "Epoch 37/100\n",
      "21293/21293 - 6s - loss: 106.4325\n",
      "Epoch 38/100\n",
      "21293/21293 - 6s - loss: 106.3758\n",
      "Epoch 39/100\n",
      "21293/21293 - 6s - loss: 106.5090\n",
      "Epoch 40/100\n",
      "21293/21293 - 6s - loss: 106.2753\n",
      "Epoch 41/100\n",
      "21293/21293 - 6s - loss: 106.4278\n",
      "Epoch 42/100\n",
      "21293/21293 - 6s - loss: 106.4035\n",
      "Epoch 43/100\n",
      "21293/21293 - 6s - loss: 106.4148\n",
      "Epoch 44/100\n",
      "21293/21293 - 6s - loss: 106.4007\n",
      "Epoch 45/100\n",
      "21293/21293 - 6s - loss: 106.4002\n",
      "Epoch 46/100\n",
      "21293/21293 - 6s - loss: 106.3991\n",
      "Epoch 47/100\n",
      "21293/21293 - 6s - loss: 106.4231\n",
      "Epoch 48/100\n",
      "21293/21293 - 6s - loss: 106.3536\n",
      "Epoch 49/100\n",
      "21293/21293 - 6s - loss: 106.3925\n",
      "Epoch 50/100\n",
      "21293/21293 - 6s - loss: 106.3071\n",
      "Epoch 51/100\n",
      "21293/21293 - 6s - loss: 106.3784\n",
      "Epoch 52/100\n",
      "21293/21293 - 6s - loss: 106.3359\n",
      "Epoch 53/100\n",
      "21293/21293 - 6s - loss: 106.3902\n",
      "Epoch 54/100\n",
      "21293/21293 - 6s - loss: 106.4192\n",
      "Epoch 55/100\n",
      "21293/21293 - 6s - loss: 106.3759\n",
      "Epoch 56/100\n",
      "21293/21293 - 6s - loss: 106.3330\n",
      "Epoch 57/100\n",
      "21293/21293 - 6s - loss: 106.3933\n",
      "Epoch 58/100\n",
      "21293/21293 - 6s - loss: 106.3251\n",
      "Epoch 59/100\n",
      "21293/21293 - 6s - loss: 106.3530\n",
      "Epoch 60/100\n",
      "21293/21293 - 6s - loss: 106.2406\n",
      "Epoch 61/100\n",
      "21293/21293 - 6s - loss: 106.4285\n",
      "Epoch 62/100\n",
      "21293/21293 - 6s - loss: 106.3402\n",
      "Epoch 63/100\n",
      "21293/21293 - 6s - loss: 106.3173\n",
      "Epoch 64/100\n",
      "21293/21293 - 6s - loss: 106.3346\n",
      "Epoch 65/100\n",
      "21293/21293 - 6s - loss: 106.2399\n",
      "Epoch 66/100\n",
      "21293/21293 - 6s - loss: 106.3523\n",
      "Epoch 67/100\n",
      "21293/21293 - 6s - loss: 106.4010\n",
      "Epoch 68/100\n",
      "21293/21293 - 6s - loss: 106.3526\n",
      "Epoch 69/100\n",
      "21293/21293 - 6s - loss: 106.3896\n",
      "Epoch 70/100\n",
      "21293/21293 - 6s - loss: 106.3567\n",
      "Epoch 71/100\n",
      "21293/21293 - 6s - loss: 106.3513\n",
      "Epoch 72/100\n",
      "21293/21293 - 6s - loss: 106.3801\n",
      "Epoch 73/100\n",
      "21293/21293 - 6s - loss: 106.5128\n",
      "Epoch 74/100\n",
      "21293/21293 - 6s - loss: 106.3716\n",
      "Epoch 75/100\n",
      "21293/21293 - 6s - loss: 106.3260\n",
      "Epoch 76/100\n",
      "21293/21293 - 6s - loss: 106.3551\n",
      "Epoch 77/100\n",
      "21293/21293 - 6s - loss: 106.2826\n",
      "Epoch 78/100\n",
      "21293/21293 - 6s - loss: 106.3844\n",
      "Epoch 79/100\n",
      "21293/21293 - 6s - loss: 106.3708\n",
      "Epoch 80/100\n",
      "21293/21293 - 6s - loss: 106.3245\n",
      "Epoch 81/100\n",
      "21293/21293 - 7s - loss: 106.3496\n",
      "Epoch 82/100\n",
      "21293/21293 - 6s - loss: 106.4128\n",
      "Epoch 83/100\n",
      "21293/21293 - 6s - loss: 106.4014\n",
      "Epoch 84/100\n",
      "21293/21293 - 6s - loss: 106.3542\n",
      "Epoch 85/100\n",
      "21293/21293 - 6s - loss: 106.4271\n",
      "Epoch 86/100\n",
      "21293/21293 - 6s - loss: 106.2463\n",
      "Epoch 87/100\n",
      "21293/21293 - 6s - loss: 106.3350\n",
      "Epoch 88/100\n",
      "21293/21293 - 6s - loss: 106.3144\n",
      "Epoch 89/100\n",
      "21293/21293 - 6s - loss: 106.3064\n",
      "Epoch 90/100\n",
      "21293/21293 - 6s - loss: 106.3285\n",
      "Epoch 91/100\n",
      "21293/21293 - 6s - loss: 106.3203\n",
      "Epoch 92/100\n",
      "21293/21293 - 6s - loss: 106.3185\n",
      "Epoch 93/100\n",
      "21293/21293 - 6s - loss: 106.3117\n",
      "Epoch 94/100\n",
      "21293/21293 - 6s - loss: 106.3824\n",
      "Epoch 95/100\n",
      "21293/21293 - 6s - loss: 106.3943\n",
      "Epoch 96/100\n",
      "21293/21293 - 6s - loss: 106.3597\n",
      "Epoch 97/100\n",
      "21293/21293 - 6s - loss: 106.4046\n",
      "Epoch 98/100\n",
      "21293/21293 - 6s - loss: 106.3321\n",
      "Epoch 99/100\n",
      "21293/21293 - 6s - loss: 106.3518\n",
      "Epoch 100/100\n",
      "21293/21293 - 6s - loss: 106.4175\n",
      "2365/2365 - 0s - loss: 88.1388\n",
      "Results: -101.48 (27.64) MSE\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=2)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 266us/sample - loss: 248.5340\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 4s 230us/sample - loss: 96.9351\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 4s 229us/sample - loss: 96.0822\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 232us/sample - loss: 95.2967\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 5s 243us/sample - loss: 94.2356\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 5s 238us/sample - loss: 93.4514\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 4s 213us/sample - loss: 92.6871\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 5s 238us/sample - loss: 92.1080\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 5s 239us/sample - loss: 91.6306\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 4s 227us/sample - loss: 91.2106\n",
      "4732/4732 [==============================] - 1s 160us/sample - loss: 100.9467\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 258us/sample - loss: 259.5500\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 5s 263us/sample - loss: 97.2891\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 4s 217us/sample - loss: 95.4052\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 216us/sample - loss: 94.0091\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 4s 238us/sample - loss: 92.8913\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 4s 236us/sample - loss: 92.0109\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 4s 212us/sample - loss: 91.4543\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 4s 212us/sample - loss: 90.9582\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 4s 217us/sample - loss: 90.6268\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 4s 229us/sample - loss: 90.3394\n",
      "4732/4732 [==============================] - 1s 158us/sample - loss: 108.3196\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 241us/sample - loss: 264.3101\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 4s 213us/sample - loss: 101.8763\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 4s 218us/sample - loss: 99.0876\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 218us/sample - loss: 97.0493\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 4s 219us/sample - loss: 95.5070\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 4s 218us/sample - loss: 94.5707\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 4s 215us/sample - loss: 93.8885\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 4s 218us/sample - loss: 93.2589\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 4s 220us/sample - loss: 92.8426\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 4s 221us/sample - loss: 92.3698\n",
      "4732/4732 [==============================] - 1s 143us/sample - loss: 89.0039\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 5s 250us/sample - loss: 280.6038\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 4s 211us/sample - loss: 103.4202\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 4s 212us/sample - loss: 101.9990\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 4s 213us/sample - loss: 100.7746\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 4s 213us/sample - loss: 99.5212\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 4s 215us/sample - loss: 98.2125\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 4s 215us/sample - loss: 97.0571\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 4s 220us/sample - loss: 95.9590\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 4s 219us/sample - loss: 95.2238\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 5s 253us/sample - loss: 94.5897\n",
      "4731/4731 [==============================] - 1s 168us/sample - loss: 79.6669\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 4s 219us/sample - loss: 260.4723\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 3s 165us/sample - loss: 100.4469\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 3s 167us/sample - loss: 99.4028\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 3s 169us/sample - loss: 98.1698\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 3s 166us/sample - loss: 96.8173\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 3s 174us/sample - loss: 95.6006\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 3s 168us/sample - loss: 94.6617\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 3s 166us/sample - loss: 93.9612\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 3s 166us/sample - loss: 93.4797\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 3s 167us/sample - loss: 93.1049\n",
      "4731/4731 [==============================] - 1s 109us/sample - loss: 91.4446\n",
      "Standardized: -93.88 (9.90) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=input_dims, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 4s 204us/sample - loss: 154.0473\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 4s 185us/sample - loss: 90.8883\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 4s 204us/sample - loss: 89.5462\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 193us/sample - loss: 88.6632\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 4s 199us/sample - loss: 88.2036\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 4s 198us/sample - loss: 87.8964\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 4s 199us/sample - loss: 87.6687\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 4s 198us/sample - loss: 87.3833\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 4s 213us/sample - loss: 87.1959\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 3s 184us/sample - loss: 87.1860\n",
      "4732/4732 [==============================] - 1s 126us/sample - loss: 99.4227\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 4s 219us/sample - loss: 149.2943\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 4s 202us/sample - loss: 88.7926\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 4s 192us/sample - loss: 86.9422\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 199us/sample - loss: 86.1752\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 4s 209us/sample - loss: 85.8710\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 4s 207us/sample - loss: 85.5509\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 4s 204us/sample - loss: 85.4299\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 4s 222us/sample - loss: 85.2218\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 4s 237us/sample - loss: 85.2237\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 5s 245us/sample - loss: 85.0159\n",
      "4732/4732 [==============================] - 1s 156us/sample - loss: 102.0551\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 6s 303us/sample - loss: 159.1764\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 5s 255us/sample - loss: 93.8858\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 5s 253us/sample - loss: 91.7534\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 5s 254us/sample - loss: 90.8156\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 5s 274us/sample - loss: 90.3827\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 6s 310us/sample - loss: 89.9501\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 5s 250us/sample - loss: 89.7492\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 5s 245us/sample - loss: 89.5578\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 5s 279us/sample - loss: 89.3338\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 5s 271us/sample - loss: 89.2157\n",
      "4732/4732 [==============================] - 1s 262us/sample - loss: 84.7662\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 6s 336us/sample - loss: 154.9076\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 6s 316us/sample - loss: 96.6142\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 5s 281us/sample - loss: 94.2220\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 5s 262us/sample - loss: 92.5484\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 5s 246us/sample - loss: 92.0333\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 5s 242us/sample - loss: 91.6916\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 5s 273us/sample - loss: 91.2194\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 5s 256us/sample - loss: 91.1776\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 5s 238us/sample - loss: 90.9302\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 4s 235us/sample - loss: 90.7850\n",
      "4731/4731 [==============================] - 1s 176us/sample - loss: 77.1522\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 5s 276us/sample - loss: 153.8927\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 4s 234us/sample - loss: 95.1932\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 5s 243us/sample - loss: 93.2511\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 4s 237us/sample - loss: 92.0272\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 5s 241us/sample - loss: 90.9974\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 5s 245us/sample - loss: 90.2468\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 4s 229us/sample - loss: 89.7304\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 4s 236us/sample - loss: 89.3461\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 5s 239us/sample - loss: 89.0814\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 4s 231us/sample - loss: 88.9516\n",
      "4731/4731 [==============================] - 1s 170us/sample - loss: 84.7267\n",
      "Larger: -89.62 (9.53) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=input_dims, kernel_initializer='random_uniform', activation='relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 279us/sample - loss: 187.3545\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 5s 248us/sample - loss: 107.8215\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 5s 249us/sample - loss: 104.3796\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 5s 263us/sample - loss: 103.0333\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 5s 275us/sample - loss: 102.8310\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 5s 254us/sample - loss: 102.7672\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 5s 253us/sample - loss: 102.8015\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 5s 244us/sample - loss: 102.8085\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 5s 270us/sample - loss: 102.8369\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 5s 263us/sample - loss: 102.7604\n",
      "4732/4732 [==============================] - 1s 168us/sample - loss: 113.3233\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 281us/sample - loss: 179.4851\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 5s 256us/sample - loss: 107.3454\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 5s 243us/sample - loss: 103.5550\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 5s 261us/sample - loss: 101.8892\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 5s 266us/sample - loss: 101.2580\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 5s 277us/sample - loss: 100.8583\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 5s 252us/sample - loss: 100.7616\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 5s 258us/sample - loss: 100.8168\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 5s 249us/sample - loss: 100.8080\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 5s 246us/sample - loss: 100.8303\n",
      "4732/4732 [==============================] - 1s 179us/sample - loss: 118.6677\n",
      "Train on 18926 samples\n",
      "Epoch 1/10\n",
      "18926/18926 [==============================] - 5s 280us/sample - loss: 186.4383\n",
      "Epoch 2/10\n",
      "18926/18926 [==============================] - 4s 237us/sample - loss: 111.7069\n",
      "Epoch 3/10\n",
      "18926/18926 [==============================] - 5s 241us/sample - loss: 107.8919\n",
      "Epoch 4/10\n",
      "18926/18926 [==============================] - 4s 237us/sample - loss: 106.1810\n",
      "Epoch 5/10\n",
      "18926/18926 [==============================] - 5s 240us/sample - loss: 105.4365\n",
      "Epoch 6/10\n",
      "18926/18926 [==============================] - 5s 271us/sample - loss: 105.1550\n",
      "Epoch 7/10\n",
      "18926/18926 [==============================] - 5s 272us/sample - loss: 105.1383\n",
      "Epoch 8/10\n",
      "18926/18926 [==============================] - 5s 257us/sample - loss: 105.1556\n",
      "Epoch 9/10\n",
      "18926/18926 [==============================] - 5s 268us/sample - loss: 105.0669\n",
      "Epoch 10/10\n",
      "18926/18926 [==============================] - 5s 251us/sample - loss: 105.1124\n",
      "4732/4732 [==============================] - 1s 165us/sample - loss: 101.3172\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 5s 288us/sample - loss: 177.4246\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 5s 272us/sample - loss: 111.5928\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 5s 254us/sample - loss: 106.3665\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 5s 252us/sample - loss: 103.2876\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 5s 257us/sample - loss: 101.2966\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 5s 250us/sample - loss: 99.8917\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 5s 238us/sample - loss: 98.9686\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 5s 239us/sample - loss: 98.2009\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 5s 249us/sample - loss: 97.6348\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 5s 246us/sample - loss: 97.1849\n",
      "4731/4731 [==============================] - 1s 177us/sample - loss: 84.2822\n",
      "Train on 18927 samples\n",
      "Epoch 1/10\n",
      "18927/18927 [==============================] - 5s 276us/sample - loss: 173.5081\n",
      "Epoch 2/10\n",
      "18927/18927 [==============================] - 5s 254us/sample - loss: 109.1475\n",
      "Epoch 3/10\n",
      "18927/18927 [==============================] - 5s 258us/sample - loss: 106.2115\n",
      "Epoch 4/10\n",
      "18927/18927 [==============================] - 5s 245us/sample - loss: 105.1638\n",
      "Epoch 5/10\n",
      "18927/18927 [==============================] - 5s 245us/sample - loss: 105.0790\n",
      "Epoch 6/10\n",
      "18927/18927 [==============================] - 5s 238us/sample - loss: 104.8606\n",
      "Epoch 7/10\n",
      "18927/18927 [==============================] - 5s 266us/sample - loss: 104.9770\n",
      "Epoch 8/10\n",
      "18927/18927 [==============================] - 5s 250us/sample - loss: 104.9535\n",
      "Epoch 9/10\n",
      "18927/18927 [==============================] - 5s 244us/sample - loss: 104.9778\n",
      "Epoch 10/10\n",
      "18927/18927 [==============================] - 4s 236us/sample - loss: 104.9852\n",
      "4731/4731 [==============================] - 1s 159us/sample - loss: 102.0379\n",
      "Larger_2: -103.93 (11.85) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', MinMaxScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model_2, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(\"Larger_2: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
