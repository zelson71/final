{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>subsubregion</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14.1</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This white has an expert level of intensity an...</td>\n",
       "      <td>Dutton Ranch Walker Hill Vineyard</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Green Valley</td>\n",
       "      <td>Dutton-Goldfield 2016 Dutton Ranch Walker Hill...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/dutton-go...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Dutton-Goldfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>12.6</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>Stunning aromatics combine spicy citrus, tangy...</td>\n",
       "      <td>Maresh Vineyard Old Vine</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Dundee Hills</td>\n",
       "      <td>Harper Voit 2016 Maresh Vineyard Old Vine Ries...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/harper-vo...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "      <td>Harper Voit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a structured and remarkable wine, burs...</td>\n",
       "      <td>Year of the Monkey Single Vineyard</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Carneros</td>\n",
       "      <td>Donum 2016 Year of the Monkey Single Vineyard ...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/donum-201...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Donum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>13.8</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>The wine's aromas are arresting in notes of le...</td>\n",
       "      <td>Chaleur Blanc</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>DeLille 2017 Chaleur Blanc White (Columbia Val...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/delille-2...</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "      <td>DeLille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>14.7</td>\n",
       "      <td>White</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a grainy, structured and textured whit...</td>\n",
       "      <td>Lewis MacGregor Estate Vineyard</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Russian River Valley</td>\n",
       "      <td>Williams Selyem 2016 Lewis MacGregor Estate Vi...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/williams-...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "      <td>Williams Selyem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  alcohol category country  \\\n",
       "0   4     14.1    White      US   \n",
       "1  39     12.6    White      US   \n",
       "2  53     13.8    White      US   \n",
       "3  56     13.8    White      US   \n",
       "4  74     14.7    White      US   \n",
       "\n",
       "                                         description  \\\n",
       "0  This white has an expert level of intensity an...   \n",
       "1  Stunning aromatics combine spicy citrus, tangy...   \n",
       "2  This is a structured and remarkable wine, burs...   \n",
       "3  The wine's aromas are arresting in notes of le...   \n",
       "4  This is a grainy, structured and textured whit...   \n",
       "\n",
       "                          designation  price  rating      region  \\\n",
       "0   Dutton Ranch Walker Hill Vineyard   50.0      94  California   \n",
       "1            Maresh Vineyard Old Vine   30.0      94      Oregon   \n",
       "2  Year of the Monkey Single Vineyard   60.0      94  California   \n",
       "3                       Chaleur Blanc   35.0      94  Washington   \n",
       "4     Lewis MacGregor Estate Vineyard   65.0      94  California   \n",
       "\n",
       "           subregion          subsubregion  \\\n",
       "0             Sonoma          Green Valley   \n",
       "1  Willamette Valley          Dundee Hills   \n",
       "2        Napa-Sonoma              Carneros   \n",
       "3    Columbia Valley  Columbia Valley (WA)   \n",
       "4             Sonoma  Russian River Valley   \n",
       "\n",
       "                                               title  \\\n",
       "0  Dutton-Goldfield 2016 Dutton Ranch Walker Hill...   \n",
       "1  Harper Voit 2016 Maresh Vineyard Old Vine Ries...   \n",
       "2  Donum 2016 Year of the Monkey Single Vineyard ...   \n",
       "3  DeLille 2017 Chaleur Blanc White (Columbia Val...   \n",
       "4  Williams Selyem 2016 Lewis MacGregor Estate Vi...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.winemag.com/buying-guide/dutton-go...   \n",
       "1  https://www.winemag.com/buying-guide/harper-vo...   \n",
       "2  https://www.winemag.com/buying-guide/donum-201...   \n",
       "3  https://www.winemag.com/buying-guide/delille-2...   \n",
       "4  https://www.winemag.com/buying-guide/williams-...   \n",
       "\n",
       "                     varietal  vintage            winery  \n",
       "0                  Chardonnay     2016  Dutton-Goldfield  \n",
       "1                    Riesling     2016       Harper Voit  \n",
       "2                  Chardonnay     2016             Donum  \n",
       "3  Bordeaux-style White Blend     2017           DeLille  \n",
       "4                  Chardonnay     2016   Williams Selyem  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Resources/uswhites.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa-Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  price  rating      region          subregion  \\\n",
       "0     14.1   50.0      94  California             Sonoma   \n",
       "1     12.6   30.0      94      Oregon  Willamette Valley   \n",
       "2     13.8   60.0      94  California        Napa-Sonoma   \n",
       "3     13.8   35.0      94  Washington    Columbia Valley   \n",
       "4     14.7   65.0      94  California             Sonoma   \n",
       "\n",
       "                     varietal  vintage  \n",
       "0                  Chardonnay     2016  \n",
       "1                    Riesling     2016  \n",
       "2                  Chardonnay     2016  \n",
       "3  Bordeaux-style White Blend     2017  \n",
       "4                  Chardonnay     2016  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['ID', 'category', 'country', 'description', 'designation', 'subsubregion', 'title', 'url', 'winery'], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(df)\n",
    "X_df['subregion'] = X_df['subregion'].astype(str)\n",
    "X_df['vintage'] = X_df['vintage'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23653</td>\n",
       "      <td>147</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23654</td>\n",
       "      <td>147</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23655</td>\n",
       "      <td>159</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23656</td>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>138</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23657</td>\n",
       "      <td>139</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23658 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       alcohol  rating  region  subregion  varietal  vintage\n",
       "0          130      14       2         42        17       22\n",
       "1           68      14      17         56        87       22\n",
       "2          118      14       2         28        17       22\n",
       "3          118      14      24          7        10       23\n",
       "4          152      14       2         42        17       22\n",
       "...        ...     ...     ...        ...       ...      ...\n",
       "23653      147      11       2          3        17       11\n",
       "23654      147      12       2          3        17       11\n",
       "23655      159      12       2          3        17       11\n",
       "23656      176      12       2         42       138       10\n",
       "23657      139      12       2          2        17       11\n",
       "\n",
       "[23658 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= X.apply(LabelEncoder().fit_transform)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130.  14.   2.  42.  17.  22.]\n",
      " [ 68.  14.  17.  56.  87.  22.]\n",
      " [118.  14.   2.  28.  17.  22.]\n",
      " ...\n",
      " [159.  12.   2.   3.  17.  11.]\n",
      " [176.  12.   2.  42. 138.  10.]\n",
      " [139.  12.   2.   2.  17.  11.]]\n",
      "[50. 30. 60. ... 29. 24. 55.]\n"
     ]
    }
   ],
   "source": [
    "X = X.values.astype(\"float32\")\n",
    "print(X)\n",
    "y = y.values.astype(\"float32\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915,)\n",
      "(5915, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_model = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train=y_train.reshape(-1,1)\n",
    "# y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaler = scaler_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled = X_scaler.transform(X_train)\n",
    "# X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 197\n",
      "Trainable params: 197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation = 'relu', name='dense_1', kernel_initializer='random_uniform', input_dim=(input_dims)))\n",
    "model.add(Dense(8, activation='relu', name='dense_2', kernel_initializer='random_uniform'))\n",
    "model.add(Dense(1, activation='linear', name='predictions'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14194 samples, validate on 3549 samples\n",
      "Epoch 1/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 139.7942 - mse: 139.7943 - mae: 8.2651 - val_loss: 98.6939 - val_mse: 98.6938 - val_mae: 7.2942\n",
      "Epoch 2/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 101.3781 - mse: 101.3781 - mae: 7.0895 - val_loss: 91.0675 - val_mse: 91.0674 - val_mae: 6.9590\n",
      "Epoch 3/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 97.5953 - mse: 97.5953 - mae: 6.9313 - val_loss: 89.6754 - val_mse: 89.6754 - val_mae: 6.8015\n",
      "Epoch 4/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 96.8581 - mse: 96.8580 - mae: 6.9021 - val_loss: 89.0868 - val_mse: 89.0868 - val_mae: 6.7527\n",
      "Epoch 5/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 95.8499 - mse: 95.8499 - mae: 6.8688 - val_loss: 88.5389 - val_mse: 88.5389 - val_mae: 6.7594\n",
      "Epoch 6/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 95.4525 - mse: 95.4524 - mae: 6.8614 - val_loss: 89.5291 - val_mse: 89.5291 - val_mae: 6.7489\n",
      "Epoch 7/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 95.3250 - mse: 95.3250 - mae: 6.8506 - val_loss: 91.1911 - val_mse: 91.1911 - val_mae: 6.6721\n",
      "Epoch 8/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 95.2036 - mse: 95.2037 - mae: 6.8403 - val_loss: 91.1606 - val_mse: 91.1607 - val_mae: 6.6925\n",
      "Epoch 9/1000\n",
      "14194/14194 [==============================] - 1s 86us/step - loss: 95.2311 - mse: 95.2311 - mae: 6.8349 - val_loss: 88.2727 - val_mse: 88.2727 - val_mae: 6.7436\n",
      "Epoch 10/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 94.8801 - mse: 94.8802 - mae: 6.8288 - val_loss: 88.6857 - val_mse: 88.6857 - val_mae: 6.6538\n",
      "Epoch 11/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 94.6788 - mse: 94.6787 - mae: 6.8135 - val_loss: 87.8701 - val_mse: 87.8701 - val_mae: 6.6876\n",
      "Epoch 12/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 94.9082 - mse: 94.9081 - mae: 6.8137 - val_loss: 87.6309 - val_mse: 87.6309 - val_mae: 6.8057\n",
      "Epoch 13/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 94.6946 - mse: 94.6946 - mae: 6.8267 - val_loss: 89.7516 - val_mse: 89.7516 - val_mae: 6.6295\n",
      "Epoch 14/1000\n",
      "14194/14194 [==============================] - 1s 85us/step - loss: 94.4756 - mse: 94.4756 - mae: 6.8146 - val_loss: 87.7951 - val_mse: 87.7951 - val_mae: 6.6666\n",
      "Epoch 15/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 94.5089 - mse: 94.5089 - mae: 6.8076 - val_loss: 88.6547 - val_mse: 88.6547 - val_mae: 6.9224\n",
      "Epoch 16/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 94.3974 - mse: 94.3974 - mae: 6.7973 - val_loss: 88.5766 - val_mse: 88.5766 - val_mae: 6.6498\n",
      "Epoch 17/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 94.6994 - mse: 94.6994 - mae: 6.8150 - val_loss: 87.6058 - val_mse: 87.6057 - val_mae: 6.6562\n",
      "Epoch 18/1000\n",
      "14194/14194 [==============================] - 1s 87us/step - loss: 94.2381 - mse: 94.2381 - mae: 6.7963 - val_loss: 87.0860 - val_mse: 87.0861 - val_mae: 6.6788\n",
      "Epoch 19/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 94.3016 - mse: 94.3016 - mae: 6.7762 - val_loss: 87.4275 - val_mse: 87.4275 - val_mae: 6.7404\n",
      "Epoch 20/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 94.4906 - mse: 94.4906 - mae: 6.7972 - val_loss: 93.8537 - val_mse: 93.8537 - val_mae: 7.2484\n",
      "Epoch 21/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 94.1341 - mse: 94.1340 - mae: 6.7913 - val_loss: 87.6853 - val_mse: 87.6853 - val_mae: 6.6376\n",
      "Epoch 22/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 93.8198 - mse: 93.8197 - mae: 6.7785 - val_loss: 89.1851 - val_mse: 89.1852 - val_mae: 6.9697\n",
      "Epoch 23/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 94.1094 - mse: 94.1094 - mae: 6.7843 - val_loss: 86.8159 - val_mse: 86.8159 - val_mae: 6.6944\n",
      "Epoch 24/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.6693 - mse: 93.6693 - mae: 6.7788 - val_loss: 88.5959 - val_mse: 88.5959 - val_mae: 6.6144\n",
      "Epoch 25/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 93.8677 - mse: 93.8677 - mae: 6.7761 - val_loss: 89.3897 - val_mse: 89.3897 - val_mae: 7.0119\n",
      "Epoch 26/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 93.7723 - mse: 93.7723 - mae: 6.7689 - val_loss: 88.6206 - val_mse: 88.6206 - val_mae: 6.6459\n",
      "Epoch 27/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 93.6425 - mse: 93.6426 - mae: 6.7593 - val_loss: 90.4742 - val_mse: 90.4742 - val_mae: 7.1138\n",
      "Epoch 28/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.6968 - mse: 93.6969 - mae: 6.7695 - val_loss: 88.1100 - val_mse: 88.1100 - val_mae: 6.7647\n",
      "Epoch 29/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 93.6723 - mse: 93.6723 - mae: 6.7585 - val_loss: 88.8234 - val_mse: 88.8234 - val_mae: 6.5932\n",
      "Epoch 30/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 93.5671 - mse: 93.5671 - mae: 6.7685 - val_loss: 86.9701 - val_mse: 86.9702 - val_mae: 6.5920\n",
      "Epoch 31/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.6827 - mse: 93.6828 - mae: 6.7642 - val_loss: 87.3966 - val_mse: 87.3966 - val_mae: 6.7299\n",
      "Epoch 32/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 93.8211 - mse: 93.8211 - mae: 6.7757 - val_loss: 86.6236 - val_mse: 86.6236 - val_mae: 6.6655\n",
      "Epoch 33/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 93.3988 - mse: 93.3988 - mae: 6.7515 - val_loss: 88.6190 - val_mse: 88.6190 - val_mae: 6.9424\n",
      "Epoch 34/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 93.6655 - mse: 93.6655 - mae: 6.7724 - val_loss: 88.8317 - val_mse: 88.8317 - val_mae: 6.6221\n",
      "Epoch 35/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.3960 - mse: 93.3960 - mae: 6.7547 - val_loss: 87.7605 - val_mse: 87.7606 - val_mae: 6.8290\n",
      "Epoch 36/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.7255 - mse: 93.7255 - mae: 6.7859 - val_loss: 86.8604 - val_mse: 86.8604 - val_mae: 6.7291\n",
      "Epoch 37/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.7558 - mse: 93.7558 - mae: 6.7774 - val_loss: 87.7873 - val_mse: 87.7873 - val_mae: 6.6112\n",
      "Epoch 38/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.5571 - mse: 93.5572 - mae: 6.7675 - val_loss: 86.8207 - val_mse: 86.8207 - val_mae: 6.7226\n",
      "Epoch 39/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 93.7016 - mse: 93.7016 - mae: 6.7679 - val_loss: 88.9736 - val_mse: 88.9737 - val_mae: 6.6175\n",
      "Epoch 40/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 93.4776 - mse: 93.4776 - mae: 6.7592 - val_loss: 86.8633 - val_mse: 86.8633 - val_mae: 6.6373\n",
      "Epoch 41/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.3614 - mse: 93.3614 - mae: 6.7581 - val_loss: 88.9195 - val_mse: 88.9195 - val_mae: 6.5816\n",
      "Epoch 42/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 93.6420 - mse: 93.6420 - mae: 6.7661 - val_loss: 86.7407 - val_mse: 86.7407 - val_mae: 6.7871\n",
      "Epoch 43/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 93.4948 - mse: 93.4949 - mae: 6.7667 - val_loss: 86.7513 - val_mse: 86.7513 - val_mae: 6.7661\n",
      "Epoch 44/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.1464 - mse: 93.1464 - mae: 6.7526 - val_loss: 86.4619 - val_mse: 86.4619 - val_mae: 6.6933\n",
      "Epoch 45/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 93.3723 - mse: 93.3722 - mae: 6.7561 - val_loss: 87.3076 - val_mse: 87.3077 - val_mae: 6.7696\n",
      "Epoch 46/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 93.3852 - mse: 93.3852 - mae: 6.7681 - val_loss: 86.5631 - val_mse: 86.5631 - val_mae: 6.7063\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 124us/step - loss: 93.4929 - mse: 93.4931 - mae: 6.7573 - val_loss: 87.6619 - val_mse: 87.6619 - val_mae: 6.8157\n",
      "Epoch 48/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.4529 - mse: 93.4529 - mae: 6.7557 - val_loss: 89.8799 - val_mse: 89.8799 - val_mae: 7.1062\n",
      "Epoch 49/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 93.3478 - mse: 93.3477 - mae: 6.7547 - val_loss: 86.5305 - val_mse: 86.5306 - val_mae: 6.6135\n",
      "Epoch 50/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 93.4657 - mse: 93.4658 - mae: 6.7583 - val_loss: 86.8877 - val_mse: 86.8877 - val_mae: 6.8060\n",
      "Epoch 51/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.3751 - mse: 93.3751 - mae: 6.7569 - val_loss: 86.3853 - val_mse: 86.3853 - val_mae: 6.6412\n",
      "Epoch 52/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 93.3600 - mse: 93.3599 - mae: 6.7514 - val_loss: 87.0753 - val_mse: 87.0753 - val_mae: 6.7965\n",
      "Epoch 53/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 93.0636 - mse: 93.0636 - mae: 6.7502 - val_loss: 86.7027 - val_mse: 86.7026 - val_mae: 6.5942\n",
      "Epoch 54/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.5632 - mse: 93.5632 - mae: 6.7606 - val_loss: 89.1419 - val_mse: 89.1419 - val_mae: 6.5894\n",
      "Epoch 55/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 93.3240 - mse: 93.3240 - mae: 6.7525 - val_loss: 87.5521 - val_mse: 87.5521 - val_mae: 6.6175\n",
      "Epoch 56/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 93.2089 - mse: 93.2090 - mae: 6.7543 - val_loss: 86.6908 - val_mse: 86.6908 - val_mae: 6.7540\n",
      "Epoch 57/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 93.0737 - mse: 93.0737 - mae: 6.7361 - val_loss: 86.4116 - val_mse: 86.4116 - val_mae: 6.7422\n",
      "Epoch 58/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 93.2181 - mse: 93.2181 - mae: 6.7524 - val_loss: 88.8583 - val_mse: 88.8583 - val_mae: 6.5681\n",
      "Epoch 59/1000\n",
      "14194/14194 [==============================] - 3s 202us/step - loss: 93.1420 - mse: 93.1420 - mae: 6.7522 - val_loss: 86.5204 - val_mse: 86.5204 - val_mae: 6.5866\n",
      "Epoch 60/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 93.2200 - mse: 93.2200 - mae: 6.7405 - val_loss: 88.6152 - val_mse: 88.6151 - val_mae: 6.5611\n",
      "Epoch 61/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 93.0956 - mse: 93.0956 - mae: 6.7404 - val_loss: 86.4215 - val_mse: 86.4215 - val_mae: 6.6400\n",
      "Epoch 62/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 93.0118 - mse: 93.0118 - mae: 6.7520 - val_loss: 86.5926 - val_mse: 86.5926 - val_mae: 6.7601\n",
      "Epoch 63/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 93.2386 - mse: 93.2386 - mae: 6.7455 - val_loss: 87.0417 - val_mse: 87.0417 - val_mae: 6.7147\n",
      "Epoch 64/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 93.1003 - mse: 93.1004 - mae: 6.7448 - val_loss: 86.3022 - val_mse: 86.3022 - val_mae: 6.6388\n",
      "Epoch 65/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 92.5996 - mse: 92.5996 - mae: 6.7359 - val_loss: 88.7721 - val_mse: 88.7721 - val_mae: 6.5717\n",
      "Epoch 66/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 92.9684 - mse: 92.9684 - mae: 6.7490 - val_loss: 88.8438 - val_mse: 88.8438 - val_mae: 6.9177\n",
      "Epoch 67/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 93.2302 - mse: 93.2303 - mae: 6.7519 - val_loss: 86.1643 - val_mse: 86.1643 - val_mae: 6.6692\n",
      "Epoch 68/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 92.9075 - mse: 92.9075 - mae: 6.7423 - val_loss: 86.5351 - val_mse: 86.5351 - val_mae: 6.6555\n",
      "Epoch 69/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 92.8264 - mse: 92.8265 - mae: 6.7430 - val_loss: 86.7761 - val_mse: 86.7762 - val_mae: 6.7744\n",
      "Epoch 70/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 92.8754 - mse: 92.8754 - mae: 6.7378 - val_loss: 88.5227 - val_mse: 88.5227 - val_mae: 6.9465\n",
      "Epoch 71/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 92.9044 - mse: 92.9043 - mae: 6.7298 - val_loss: 86.4522 - val_mse: 86.4522 - val_mae: 6.6606\n",
      "Epoch 72/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 92.5675 - mse: 92.5676 - mae: 6.7394 - val_loss: 90.3567 - val_mse: 90.3567 - val_mae: 7.0245\n",
      "Epoch 73/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 92.3747 - mse: 92.3747 - mae: 6.7158 - val_loss: 85.9657 - val_mse: 85.9657 - val_mae: 6.5834\n",
      "Epoch 74/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 92.2029 - mse: 92.2029 - mae: 6.7138 - val_loss: 84.9104 - val_mse: 84.9104 - val_mae: 6.6118\n",
      "Epoch 75/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 92.0297 - mse: 92.0297 - mae: 6.7030 - val_loss: 86.3544 - val_mse: 86.3544 - val_mae: 6.5507\n",
      "Epoch 76/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 91.8760 - mse: 91.8761 - mae: 6.6996 - val_loss: 84.7731 - val_mse: 84.7731 - val_mae: 6.6260\n",
      "Epoch 77/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 91.5241 - mse: 91.5240 - mae: 6.6791 - val_loss: 85.6853 - val_mse: 85.6853 - val_mae: 6.6137\n",
      "Epoch 78/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 91.6678 - mse: 91.6678 - mae: 6.6865 - val_loss: 85.8407 - val_mse: 85.8407 - val_mae: 6.5351\n",
      "Epoch 79/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 91.2512 - mse: 91.2511 - mae: 6.6890 - val_loss: 84.4490 - val_mse: 84.4490 - val_mae: 6.5740\n",
      "Epoch 80/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 91.1209 - mse: 91.1209 - mae: 6.6795 - val_loss: 85.0129 - val_mse: 85.0129 - val_mae: 6.5637\n",
      "Epoch 81/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 90.9723 - mse: 90.9723 - mae: 6.6700 - val_loss: 85.8006 - val_mse: 85.8006 - val_mae: 6.4798\n",
      "Epoch 82/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 90.7916 - mse: 90.7916 - mae: 6.6518 - val_loss: 84.3881 - val_mse: 84.3882 - val_mae: 6.5235\n",
      "Epoch 83/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 90.6349 - mse: 90.6349 - mae: 6.6544 - val_loss: 84.4370 - val_mse: 84.4370 - val_mae: 6.5038\n",
      "Epoch 84/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 90.6535 - mse: 90.6534 - mae: 6.6436 - val_loss: 85.7862 - val_mse: 85.7862 - val_mae: 6.8403\n",
      "Epoch 85/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 90.6868 - mse: 90.6869 - mae: 6.6544 - val_loss: 85.9704 - val_mse: 85.9704 - val_mae: 6.5173\n",
      "Epoch 86/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 90.5485 - mse: 90.5485 - mae: 6.6506 - val_loss: 84.6505 - val_mse: 84.6505 - val_mae: 6.4852\n",
      "Epoch 87/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 90.3925 - mse: 90.3926 - mae: 6.6387 - val_loss: 83.6690 - val_mse: 83.6690 - val_mae: 6.5737\n",
      "Epoch 88/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 90.6464 - mse: 90.6464 - mae: 6.6535 - val_loss: 86.2054 - val_mse: 86.2054 - val_mae: 6.4832\n",
      "Epoch 89/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 90.2422 - mse: 90.2422 - mae: 6.6384 - val_loss: 85.0583 - val_mse: 85.0583 - val_mae: 6.7125\n",
      "Epoch 90/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 90.1881 - mse: 90.1881 - mae: 6.6327 - val_loss: 83.7709 - val_mse: 83.7709 - val_mae: 6.6341\n",
      "Epoch 91/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 90.1471 - mse: 90.1471 - mae: 6.6301 - val_loss: 84.6137 - val_mse: 84.6138 - val_mae: 6.7534\n",
      "Epoch 92/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 90.1187 - mse: 90.1187 - mae: 6.6309 - val_loss: 84.4013 - val_mse: 84.4013 - val_mae: 6.4829\n",
      "Epoch 93/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 126us/step - loss: 90.2052 - mse: 90.2051 - mae: 6.6368 - val_loss: 84.6703 - val_mse: 84.6703 - val_mae: 6.4928\n",
      "Epoch 94/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 90.1390 - mse: 90.1389 - mae: 6.6388 - val_loss: 84.1116 - val_mse: 84.1116 - val_mae: 6.5192\n",
      "Epoch 95/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 89.7908 - mse: 89.7908 - mae: 6.6127 - val_loss: 85.8215 - val_mse: 85.8216 - val_mae: 6.4849\n",
      "Epoch 96/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 90.3127 - mse: 90.3127 - mae: 6.6429 - val_loss: 83.8791 - val_mse: 83.8791 - val_mae: 6.6645\n",
      "Epoch 97/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 90.1887 - mse: 90.1888 - mae: 6.6343 - val_loss: 83.6919 - val_mse: 83.6919 - val_mae: 6.4550\n",
      "Epoch 98/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 90.1274 - mse: 90.1273 - mae: 6.6216 - val_loss: 84.1070 - val_mse: 84.1070 - val_mae: 6.4780\n",
      "Epoch 99/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 90.0642 - mse: 90.0642 - mae: 6.6454 - val_loss: 84.4046 - val_mse: 84.4047 - val_mae: 6.7080\n",
      "Epoch 100/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 89.9586 - mse: 89.9585 - mae: 6.6222 - val_loss: 84.1533 - val_mse: 84.1533 - val_mae: 6.6543\n",
      "Epoch 101/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 90.0118 - mse: 90.0119 - mae: 6.6248 - val_loss: 83.9194 - val_mse: 83.9194 - val_mae: 6.5973\n",
      "Epoch 102/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 90.1321 - mse: 90.1321 - mae: 6.6291 - val_loss: 85.2007 - val_mse: 85.2007 - val_mae: 6.8205\n",
      "Epoch 103/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 90.1260 - mse: 90.1260 - mae: 6.6311 - val_loss: 83.7430 - val_mse: 83.7430 - val_mae: 6.6381\n",
      "Epoch 104/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 89.9403 - mse: 89.9403 - mae: 6.6251 - val_loss: 86.8692 - val_mse: 86.8691 - val_mae: 6.5187\n",
      "Epoch 105/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 90.2410 - mse: 90.2411 - mae: 6.6245 - val_loss: 83.7078 - val_mse: 83.7078 - val_mae: 6.5058\n",
      "Epoch 106/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 90.0017 - mse: 90.0016 - mae: 6.6196 - val_loss: 83.8375 - val_mse: 83.8375 - val_mae: 6.5539\n",
      "Epoch 107/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 89.7791 - mse: 89.7791 - mae: 6.6297 - val_loss: 83.4247 - val_mse: 83.4246 - val_mae: 6.5423\n",
      "Epoch 108/1000\n",
      "14194/14194 [==============================] - 2s 150us/step - loss: 89.8863 - mse: 89.8864 - mae: 6.6120 - val_loss: 85.9557 - val_mse: 85.9557 - val_mae: 6.4727\n",
      "Epoch 109/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 89.9675 - mse: 89.9674 - mae: 6.6173 - val_loss: 84.5633 - val_mse: 84.5634 - val_mae: 6.5342\n",
      "Epoch 110/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 89.7747 - mse: 89.7747 - mae: 6.6142 - val_loss: 85.3425 - val_mse: 85.3425 - val_mae: 6.4570\n",
      "Epoch 111/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 89.9010 - mse: 89.9009 - mae: 6.6131 - val_loss: 84.7679 - val_mse: 84.7679 - val_mae: 6.6990\n",
      "Epoch 112/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 90.0510 - mse: 90.0510 - mae: 6.6274 - val_loss: 83.6459 - val_mse: 83.6459 - val_mae: 6.5497\n",
      "Epoch 113/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7818 - mse: 89.7818 - mae: 6.6143 - val_loss: 83.7621 - val_mse: 83.7621 - val_mae: 6.6786\n",
      "Epoch 114/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 89.7607 - mse: 89.7606 - mae: 6.6283 - val_loss: 84.6815 - val_mse: 84.6815 - val_mae: 6.4707\n",
      "Epoch 115/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 89.8741 - mse: 89.8740 - mae: 6.6221 - val_loss: 86.7136 - val_mse: 86.7136 - val_mae: 6.4472\n",
      "Epoch 116/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.9259 - mse: 89.9260 - mae: 6.6242 - val_loss: 83.6436 - val_mse: 83.6436 - val_mae: 6.5314\n",
      "Epoch 117/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.9020 - mse: 89.9020 - mae: 6.6178 - val_loss: 83.5461 - val_mse: 83.5461 - val_mae: 6.4884\n",
      "Epoch 118/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 90.2839 - mse: 90.2839 - mae: 6.6356 - val_loss: 83.9850 - val_mse: 83.9850 - val_mae: 6.4766\n",
      "Epoch 119/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 90.0811 - mse: 90.0812 - mae: 6.6237 - val_loss: 84.7310 - val_mse: 84.7310 - val_mae: 6.4720\n",
      "Epoch 120/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.7902 - mse: 89.7903 - mae: 6.6124 - val_loss: 84.1759 - val_mse: 84.1759 - val_mae: 6.5475\n",
      "Epoch 121/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7716 - mse: 89.7715 - mae: 6.6085 - val_loss: 83.4560 - val_mse: 83.4561 - val_mae: 6.6292\n",
      "Epoch 122/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 90.0394 - mse: 90.0394 - mae: 6.6208 - val_loss: 84.4028 - val_mse: 84.4028 - val_mae: 6.7412\n",
      "Epoch 123/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7333 - mse: 89.7333 - mae: 6.6136 - val_loss: 83.5455 - val_mse: 83.5454 - val_mae: 6.5671\n",
      "Epoch 124/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.8550 - mse: 89.8550 - mae: 6.6140 - val_loss: 83.9195 - val_mse: 83.9195 - val_mae: 6.4943\n",
      "Epoch 125/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 89.8036 - mse: 89.8035 - mae: 6.6127 - val_loss: 83.2701 - val_mse: 83.2701 - val_mae: 6.5888\n",
      "Epoch 126/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 89.8128 - mse: 89.8127 - mae: 6.6198 - val_loss: 86.0816 - val_mse: 86.0816 - val_mae: 6.5222\n",
      "Epoch 127/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 89.7194 - mse: 89.7194 - mae: 6.5995 - val_loss: 83.4777 - val_mse: 83.4777 - val_mae: 6.6262\n",
      "Epoch 128/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7488 - mse: 89.7487 - mae: 6.6130 - val_loss: 83.2245 - val_mse: 83.2246 - val_mae: 6.5105\n",
      "Epoch 129/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 90.0451 - mse: 90.0451 - mae: 6.6210 - val_loss: 83.3109 - val_mse: 83.3109 - val_mae: 6.5103\n",
      "Epoch 130/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.8967 - mse: 89.8967 - mae: 6.6163 - val_loss: 84.5217 - val_mse: 84.5217 - val_mae: 6.4703\n",
      "Epoch 131/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.9404 - mse: 89.9403 - mae: 6.6069 - val_loss: 83.3991 - val_mse: 83.3990 - val_mae: 6.5036\n",
      "Epoch 132/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7722 - mse: 89.7722 - mae: 6.6141 - val_loss: 83.1759 - val_mse: 83.1759 - val_mae: 6.4654\n",
      "Epoch 133/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 89.8018 - mse: 89.8018 - mae: 6.6178 - val_loss: 83.6490 - val_mse: 83.6491 - val_mae: 6.5988\n",
      "Epoch 134/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 89.6240 - mse: 89.6241 - mae: 6.6078 - val_loss: 83.4540 - val_mse: 83.4540 - val_mae: 6.5653\n",
      "Epoch 135/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.5830 - mse: 89.5830 - mae: 6.6233 - val_loss: 84.4440 - val_mse: 84.4440 - val_mae: 6.4436\n",
      "Epoch 136/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 89.7333 - mse: 89.7333 - mae: 6.6093 - val_loss: 84.1209 - val_mse: 84.1209 - val_mae: 6.4616\n",
      "Epoch 137/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.7277 - mse: 89.7276 - mae: 6.6190 - val_loss: 84.4382 - val_mse: 84.4382 - val_mae: 6.4431\n",
      "Epoch 138/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 89.3933 - mse: 89.3933 - mae: 6.6000 - val_loss: 84.0198 - val_mse: 84.0198 - val_mae: 6.4523\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 157us/step - loss: 89.5553 - mse: 89.5553 - mae: 6.6126 - val_loss: 83.9034 - val_mse: 83.9035 - val_mae: 6.4616\n",
      "Epoch 140/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.3222 - mse: 89.3222 - mae: 6.5909 - val_loss: 83.1560 - val_mse: 83.1560 - val_mae: 6.4760\n",
      "Epoch 141/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 89.6121 - mse: 89.6120 - mae: 6.6044 - val_loss: 83.6941 - val_mse: 83.6941 - val_mae: 6.4955\n",
      "Epoch 142/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 89.8543 - mse: 89.8543 - mae: 6.6150 - val_loss: 83.5483 - val_mse: 83.5483 - val_mae: 6.5720\n",
      "Epoch 143/1000\n",
      "14194/14194 [==============================] - 3s 194us/step - loss: 89.5466 - mse: 89.5466 - mae: 6.6095 - val_loss: 85.3715 - val_mse: 85.3715 - val_mae: 6.4159\n",
      "Epoch 144/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.5277 - mse: 89.5277 - mae: 6.6081 - val_loss: 83.3663 - val_mse: 83.3663 - val_mae: 6.5333\n",
      "Epoch 145/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 89.7188 - mse: 89.7188 - mae: 6.6134 - val_loss: 83.8562 - val_mse: 83.8562 - val_mae: 6.4257\n",
      "Epoch 146/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 89.5024 - mse: 89.5024 - mae: 6.6130 - val_loss: 85.0707 - val_mse: 85.0707 - val_mae: 6.7646\n",
      "Epoch 147/1000\n",
      "14194/14194 [==============================] - 2s 154us/step - loss: 89.5011 - mse: 89.5010 - mae: 6.6002 - val_loss: 83.5080 - val_mse: 83.5080 - val_mae: 6.4933\n",
      "Epoch 148/1000\n",
      "14194/14194 [==============================] - 2s 147us/step - loss: 89.4788 - mse: 89.4789 - mae: 6.6113 - val_loss: 86.4982 - val_mse: 86.4982 - val_mae: 6.4449\n",
      "Epoch 149/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 89.4470 - mse: 89.4471 - mae: 6.5917 - val_loss: 83.9444 - val_mse: 83.9443 - val_mae: 6.5446\n",
      "Epoch 150/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 89.4701 - mse: 89.4701 - mae: 6.6054 - val_loss: 84.3625 - val_mse: 84.3625 - val_mae: 6.4546\n",
      "Epoch 151/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.2830 - mse: 89.2830 - mae: 6.5962 - val_loss: 86.8163 - val_mse: 86.8163 - val_mae: 6.4306\n",
      "Epoch 152/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 89.5919 - mse: 89.5919 - mae: 6.6016 - val_loss: 83.6942 - val_mse: 83.6941 - val_mae: 6.4129\n",
      "Epoch 153/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.1864 - mse: 89.1865 - mae: 6.5967 - val_loss: 83.4570 - val_mse: 83.4570 - val_mae: 6.6325\n",
      "Epoch 154/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.7512 - mse: 89.7512 - mae: 6.6127 - val_loss: 82.6728 - val_mse: 82.6728 - val_mae: 6.5034\n",
      "Epoch 155/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 89.0693 - mse: 89.0694 - mae: 6.5922 - val_loss: 83.0094 - val_mse: 83.0094 - val_mae: 6.4719\n",
      "Epoch 156/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 89.3061 - mse: 89.3062 - mae: 6.5950 - val_loss: 85.0800 - val_mse: 85.0800 - val_mae: 6.7893\n",
      "Epoch 157/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 89.3777 - mse: 89.3777 - mae: 6.5986 - val_loss: 83.0511 - val_mse: 83.0511 - val_mae: 6.5688\n",
      "Epoch 158/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.3577 - mse: 89.3577 - mae: 6.6011 - val_loss: 86.6936 - val_mse: 86.6936 - val_mae: 6.8586\n",
      "Epoch 159/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 89.1915 - mse: 89.1915 - mae: 6.5964 - val_loss: 83.5423 - val_mse: 83.5423 - val_mae: 6.5139\n",
      "Epoch 160/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 89.4582 - mse: 89.4581 - mae: 6.6064 - val_loss: 83.5541 - val_mse: 83.5540 - val_mae: 6.4626\n",
      "Epoch 161/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 89.4036 - mse: 89.4036 - mae: 6.5975 - val_loss: 83.5525 - val_mse: 83.5525 - val_mae: 6.4339\n",
      "Epoch 162/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.5300 - mse: 89.5299 - mae: 6.6008 - val_loss: 84.3216 - val_mse: 84.3216 - val_mae: 6.7342\n",
      "Epoch 163/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 89.5887 - mse: 89.5886 - mae: 6.6188 - val_loss: 85.6765 - val_mse: 85.6766 - val_mae: 6.4213\n",
      "Epoch 164/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 89.1445 - mse: 89.1445 - mae: 6.5872 - val_loss: 83.0755 - val_mse: 83.0755 - val_mae: 6.5145\n",
      "Epoch 165/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 89.3986 - mse: 89.3986 - mae: 6.5967 - val_loss: 83.3942 - val_mse: 83.3943 - val_mae: 6.4632\n",
      "Epoch 166/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 89.3784 - mse: 89.3784 - mae: 6.6042 - val_loss: 82.7120 - val_mse: 82.7120 - val_mae: 6.5077\n",
      "Epoch 167/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.9900 - mse: 88.9900 - mae: 6.5891 - val_loss: 83.4128 - val_mse: 83.4128 - val_mae: 6.4739\n",
      "Epoch 168/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 89.4822 - mse: 89.4823 - mae: 6.5917 - val_loss: 84.2731 - val_mse: 84.2732 - val_mae: 6.5728\n",
      "Epoch 169/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 89.3645 - mse: 89.3646 - mae: 6.6057 - val_loss: 83.0772 - val_mse: 83.0772 - val_mae: 6.5929\n",
      "Epoch 170/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 89.2875 - mse: 89.2875 - mae: 6.5978 - val_loss: 83.8503 - val_mse: 83.8502 - val_mae: 6.7108\n",
      "Epoch 171/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 89.3628 - mse: 89.3627 - mae: 6.5955 - val_loss: 83.7337 - val_mse: 83.7337 - val_mae: 6.6381\n",
      "Epoch 172/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 89.1304 - mse: 89.1305 - mae: 6.5873 - val_loss: 83.6255 - val_mse: 83.6255 - val_mae: 6.6640\n",
      "Epoch 173/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 89.2634 - mse: 89.2634 - mae: 6.5965 - val_loss: 83.8018 - val_mse: 83.8018 - val_mae: 6.7006\n",
      "Epoch 174/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 89.0952 - mse: 89.0951 - mae: 6.5825 - val_loss: 83.4035 - val_mse: 83.4035 - val_mae: 6.6848\n",
      "Epoch 175/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 89.1622 - mse: 89.1622 - mae: 6.6019 - val_loss: 84.0153 - val_mse: 84.0153 - val_mae: 6.6697\n",
      "Epoch 176/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.2697 - mse: 89.2697 - mae: 6.5991 - val_loss: 83.3800 - val_mse: 83.3800 - val_mae: 6.4489\n",
      "Epoch 177/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 89.3164 - mse: 89.3164 - mae: 6.5832 - val_loss: 83.5460 - val_mse: 83.5460 - val_mae: 6.5190\n",
      "Epoch 178/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 89.1255 - mse: 89.1255 - mae: 6.5868 - val_loss: 83.1121 - val_mse: 83.1121 - val_mae: 6.5833\n",
      "Epoch 179/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 89.3806 - mse: 89.3806 - mae: 6.6014 - val_loss: 85.1496 - val_mse: 85.1496 - val_mae: 6.4155\n",
      "Epoch 180/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 89.0555 - mse: 89.0554 - mae: 6.5903 - val_loss: 82.8724 - val_mse: 82.8724 - val_mae: 6.4433\n",
      "Epoch 181/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 89.2427 - mse: 89.2427 - mae: 6.6057 - val_loss: 82.7497 - val_mse: 82.7497 - val_mae: 6.5165\n",
      "Epoch 182/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 89.1409 - mse: 89.1409 - mae: 6.5947 - val_loss: 82.8192 - val_mse: 82.8192 - val_mae: 6.4671\n",
      "Epoch 183/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 88.7520 - mse: 88.7520 - mae: 6.5868 - val_loss: 83.6181 - val_mse: 83.6181 - val_mae: 6.4596\n",
      "Epoch 184/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 89.2156 - mse: 89.2157 - mae: 6.5843 - val_loss: 83.3034 - val_mse: 83.3034 - val_mae: 6.6534\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 150us/step - loss: 89.0116 - mse: 89.0116 - mae: 6.5913 - val_loss: 83.4751 - val_mse: 83.4750 - val_mae: 6.6750\n",
      "Epoch 186/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 89.0086 - mse: 89.0086 - mae: 6.5798 - val_loss: 85.1393 - val_mse: 85.1393 - val_mae: 6.8708\n",
      "Epoch 187/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 89.2622 - mse: 89.2622 - mae: 6.5955 - val_loss: 84.3270 - val_mse: 84.3270 - val_mae: 6.7161\n",
      "Epoch 188/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.2107 - mse: 89.2106 - mae: 6.5876 - val_loss: 82.8854 - val_mse: 82.8854 - val_mae: 6.5804\n",
      "Epoch 189/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 89.1315 - mse: 89.1315 - mae: 6.5939 - val_loss: 82.6758 - val_mse: 82.6758 - val_mae: 6.4718\n",
      "Epoch 190/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.0545 - mse: 89.0544 - mae: 6.5872 - val_loss: 83.0510 - val_mse: 83.0510 - val_mae: 6.5720\n",
      "Epoch 191/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 89.1528 - mse: 89.1527 - mae: 6.5979 - val_loss: 82.7604 - val_mse: 82.7605 - val_mae: 6.5042\n",
      "Epoch 192/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 88.9909 - mse: 88.9909 - mae: 6.5823 - val_loss: 82.7359 - val_mse: 82.7358 - val_mae: 6.5045\n",
      "Epoch 193/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 89.0064 - mse: 89.0063 - mae: 6.5879 - val_loss: 83.0121 - val_mse: 83.0121 - val_mae: 6.4300\n",
      "Epoch 194/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.2231 - mse: 89.2231 - mae: 6.5800 - val_loss: 82.8289 - val_mse: 82.8289 - val_mae: 6.4985\n",
      "Epoch 195/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.2963 - mse: 89.2962 - mae: 6.5893 - val_loss: 84.2947 - val_mse: 84.2947 - val_mae: 6.4916\n",
      "Epoch 196/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.0862 - mse: 89.0862 - mae: 6.5924 - val_loss: 84.8986 - val_mse: 84.8986 - val_mae: 6.5621\n",
      "Epoch 197/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 89.1768 - mse: 89.1768 - mae: 6.5921 - val_loss: 83.2483 - val_mse: 83.2483 - val_mae: 6.5578\n",
      "Epoch 198/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.1335 - mse: 89.1335 - mae: 6.5857 - val_loss: 82.7437 - val_mse: 82.7437 - val_mae: 6.4835\n",
      "Epoch 199/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 89.0493 - mse: 89.0493 - mae: 6.5830 - val_loss: 82.8437 - val_mse: 82.8437 - val_mae: 6.5390\n",
      "Epoch 200/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.0062 - mse: 89.0060 - mae: 6.5696 - val_loss: 83.3192 - val_mse: 83.3192 - val_mae: 6.4745\n",
      "Epoch 201/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.0160 - mse: 89.0160 - mae: 6.5790 - val_loss: 83.3409 - val_mse: 83.3409 - val_mae: 6.5640\n",
      "Epoch 202/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 89.1852 - mse: 89.1852 - mae: 6.5765 - val_loss: 83.1184 - val_mse: 83.1184 - val_mae: 6.4636\n",
      "Epoch 203/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.9036 - mse: 88.9036 - mae: 6.5834 - val_loss: 82.9587 - val_mse: 82.9587 - val_mae: 6.6038\n",
      "Epoch 204/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.1548 - mse: 89.1549 - mae: 6.5948 - val_loss: 82.3765 - val_mse: 82.3765 - val_mae: 6.4763\n",
      "Epoch 205/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.9186 - mse: 88.9187 - mae: 6.5708 - val_loss: 83.3001 - val_mse: 83.3001 - val_mae: 6.6286\n",
      "Epoch 206/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 88.9373 - mse: 88.9372 - mae: 6.5872 - val_loss: 82.4685 - val_mse: 82.4685 - val_mae: 6.4952\n",
      "Epoch 207/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 89.1849 - mse: 89.1849 - mae: 6.5927 - val_loss: 84.4744 - val_mse: 84.4744 - val_mae: 6.3960\n",
      "Epoch 208/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 89.1156 - mse: 89.1155 - mae: 6.5809 - val_loss: 84.0426 - val_mse: 84.0425 - val_mae: 6.6930\n",
      "Epoch 209/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.8929 - mse: 88.8929 - mae: 6.5708 - val_loss: 83.6210 - val_mse: 83.6210 - val_mae: 6.6755\n",
      "Epoch 210/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.1110 - mse: 89.1109 - mae: 6.5827 - val_loss: 82.5275 - val_mse: 82.5275 - val_mae: 6.4959\n",
      "Epoch 211/1000\n",
      "14194/14194 [==============================] - 2s 155us/step - loss: 89.1292 - mse: 89.1291 - mae: 6.5878 - val_loss: 82.4359 - val_mse: 82.4360 - val_mae: 6.4931\n",
      "Epoch 212/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.9594 - mse: 88.9593 - mae: 6.5747 - val_loss: 83.0109 - val_mse: 83.0109 - val_mae: 6.4412\n",
      "Epoch 213/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 89.0102 - mse: 89.0101 - mae: 6.5729 - val_loss: 83.4779 - val_mse: 83.4779 - val_mae: 6.6677\n",
      "Epoch 214/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 89.0450 - mse: 89.0449 - mae: 6.5939 - val_loss: 83.2309 - val_mse: 83.2309 - val_mae: 6.4461\n",
      "Epoch 215/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.7598 - mse: 88.7598 - mae: 6.5684 - val_loss: 82.7974 - val_mse: 82.7974 - val_mae: 6.4476\n",
      "Epoch 216/1000\n",
      "14194/14194 [==============================] - 2s 155us/step - loss: 88.8762 - mse: 88.8761 - mae: 6.5669 - val_loss: 82.5942 - val_mse: 82.5942 - val_mae: 6.4681\n",
      "Epoch 217/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.9218 - mse: 88.9218 - mae: 6.5655 - val_loss: 83.1470 - val_mse: 83.1470 - val_mae: 6.6069\n",
      "Epoch 218/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 88.8893 - mse: 88.8893 - mae: 6.5746 - val_loss: 82.4812 - val_mse: 82.4812 - val_mae: 6.5625\n",
      "Epoch 219/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 89.0506 - mse: 89.0506 - mae: 6.5869 - val_loss: 84.3627 - val_mse: 84.3627 - val_mae: 6.7113\n",
      "Epoch 220/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.6355 - mse: 88.6355 - mae: 6.5807 - val_loss: 83.1451 - val_mse: 83.1451 - val_mae: 6.6412\n",
      "Epoch 221/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.8405 - mse: 88.8405 - mae: 6.5667 - val_loss: 83.1349 - val_mse: 83.1349 - val_mae: 6.6154\n",
      "Epoch 222/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 88.8520 - mse: 88.8519 - mae: 6.5838 - val_loss: 82.6565 - val_mse: 82.6565 - val_mae: 6.5009\n",
      "Epoch 223/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 89.0289 - mse: 89.0288 - mae: 6.5855 - val_loss: 82.6375 - val_mse: 82.6375 - val_mae: 6.5456\n",
      "Epoch 224/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 88.6588 - mse: 88.6589 - mae: 6.5724 - val_loss: 84.9366 - val_mse: 84.9365 - val_mae: 6.4168\n",
      "Epoch 225/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 88.7082 - mse: 88.7082 - mae: 6.5655 - val_loss: 82.7790 - val_mse: 82.7790 - val_mae: 6.4634\n",
      "Epoch 226/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 89.0042 - mse: 89.0042 - mae: 6.5714 - val_loss: 84.4687 - val_mse: 84.4687 - val_mae: 6.7995\n",
      "Epoch 227/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 89.0245 - mse: 89.0244 - mae: 6.5858 - val_loss: 83.4259 - val_mse: 83.4259 - val_mae: 6.4456\n",
      "Epoch 228/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 88.6877 - mse: 88.6877 - mae: 6.5770 - val_loss: 84.7566 - val_mse: 84.7566 - val_mae: 6.4123\n",
      "Epoch 229/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 88.6386 - mse: 88.6385 - mae: 6.5776 - val_loss: 84.7589 - val_mse: 84.7589 - val_mae: 6.4111\n",
      "Epoch 230/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.5988 - mse: 88.5988 - mae: 6.5574 - val_loss: 82.5245 - val_mse: 82.5245 - val_mae: 6.4911\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.7781 - mse: 88.7781 - mae: 6.5683 - val_loss: 82.8162 - val_mse: 82.8163 - val_mae: 6.5115\n",
      "Epoch 232/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.9214 - mse: 88.9215 - mae: 6.5768 - val_loss: 82.9280 - val_mse: 82.9279 - val_mae: 6.4922\n",
      "Epoch 233/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.6070 - mse: 88.6070 - mae: 6.5736 - val_loss: 82.6222 - val_mse: 82.6221 - val_mae: 6.5268\n",
      "Epoch 234/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.7689 - mse: 88.7689 - mae: 6.5755 - val_loss: 82.6326 - val_mse: 82.6326 - val_mae: 6.5095\n",
      "Epoch 235/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.8343 - mse: 88.8344 - mae: 6.5840 - val_loss: 82.8871 - val_mse: 82.8871 - val_mae: 6.5008\n",
      "Epoch 236/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.6636 - mse: 88.6636 - mae: 6.5727 - val_loss: 83.3058 - val_mse: 83.3059 - val_mae: 6.5518\n",
      "Epoch 237/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 88.7879 - mse: 88.7880 - mae: 6.5732 - val_loss: 85.5137 - val_mse: 85.5138 - val_mae: 6.7869\n",
      "Epoch 238/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 88.8657 - mse: 88.8657 - mae: 6.5846 - val_loss: 82.7004 - val_mse: 82.7004 - val_mae: 6.4921\n",
      "Epoch 239/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 88.6722 - mse: 88.6723 - mae: 6.5680 - val_loss: 82.9260 - val_mse: 82.9260 - val_mae: 6.6031\n",
      "Epoch 240/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.7110 - mse: 88.7110 - mae: 6.5746 - val_loss: 84.1640 - val_mse: 84.1640 - val_mae: 6.4187\n",
      "Epoch 241/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 88.8154 - mse: 88.8154 - mae: 6.5752 - val_loss: 83.6213 - val_mse: 83.6213 - val_mae: 6.4494\n",
      "Epoch 242/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.6604 - mse: 88.6604 - mae: 6.5749 - val_loss: 82.5805 - val_mse: 82.5805 - val_mae: 6.5313\n",
      "Epoch 243/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 88.7212 - mse: 88.7212 - mae: 6.5820 - val_loss: 82.5926 - val_mse: 82.5926 - val_mae: 6.5412\n",
      "Epoch 244/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.8491 - mse: 88.8491 - mae: 6.5849 - val_loss: 83.5422 - val_mse: 83.5421 - val_mae: 6.4072\n",
      "Epoch 245/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 88.8098 - mse: 88.8097 - mae: 6.5811 - val_loss: 82.9721 - val_mse: 82.9721 - val_mae: 6.4828\n",
      "Epoch 246/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.8971 - mse: 88.8971 - mae: 6.5784 - val_loss: 82.2153 - val_mse: 82.2154 - val_mae: 6.4609\n",
      "Epoch 247/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.7613 - mse: 88.7614 - mae: 6.5857 - val_loss: 82.7343 - val_mse: 82.7343 - val_mae: 6.5752\n",
      "Epoch 248/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.6687 - mse: 88.6687 - mae: 6.5706 - val_loss: 82.7798 - val_mse: 82.7798 - val_mae: 6.5466\n",
      "Epoch 249/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.5800 - mse: 88.5799 - mae: 6.5771 - val_loss: 82.6285 - val_mse: 82.6285 - val_mae: 6.4301\n",
      "Epoch 250/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.5191 - mse: 88.5191 - mae: 6.5782 - val_loss: 84.4135 - val_mse: 84.4135 - val_mae: 6.4234\n",
      "Epoch 251/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.8466 - mse: 88.8466 - mae: 6.5844 - val_loss: 82.3171 - val_mse: 82.3172 - val_mae: 6.5200\n",
      "Epoch 252/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.7948 - mse: 88.7947 - mae: 6.5789 - val_loss: 82.6542 - val_mse: 82.6542 - val_mae: 6.5451\n",
      "Epoch 253/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.7831 - mse: 88.7832 - mae: 6.5755 - val_loss: 82.5220 - val_mse: 82.5219 - val_mae: 6.5715\n",
      "Epoch 254/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.7937 - mse: 88.7938 - mae: 6.5740 - val_loss: 85.9695 - val_mse: 85.9695 - val_mae: 6.4118\n",
      "Epoch 255/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.5939 - mse: 88.5940 - mae: 6.5746 - val_loss: 82.4377 - val_mse: 82.4377 - val_mae: 6.4861\n",
      "Epoch 256/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.7400 - mse: 88.7399 - mae: 6.5794 - val_loss: 82.7345 - val_mse: 82.7345 - val_mae: 6.5263\n",
      "Epoch 257/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.7209 - mse: 88.7208 - mae: 6.5742 - val_loss: 82.8045 - val_mse: 82.8044 - val_mae: 6.5934\n",
      "Epoch 258/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 88.7247 - mse: 88.7247 - mae: 6.5687 - val_loss: 83.9252 - val_mse: 83.9252 - val_mae: 6.7629\n",
      "Epoch 259/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 88.8408 - mse: 88.8408 - mae: 6.5910 - val_loss: 82.5305 - val_mse: 82.5305 - val_mae: 6.5089\n",
      "Epoch 260/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 88.4088 - mse: 88.4087 - mae: 6.5763 - val_loss: 88.0933 - val_mse: 88.0933 - val_mae: 6.9522\n",
      "Epoch 261/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 88.7542 - mse: 88.7542 - mae: 6.5721 - val_loss: 83.2701 - val_mse: 83.2701 - val_mae: 6.6652\n",
      "Epoch 262/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 88.9432 - mse: 88.9432 - mae: 6.5743 - val_loss: 83.5395 - val_mse: 83.5396 - val_mae: 6.7502\n",
      "Epoch 263/1000\n",
      "14194/14194 [==============================] - 2s 148us/step - loss: 88.5241 - mse: 88.5241 - mae: 6.5682 - val_loss: 82.8506 - val_mse: 82.8506 - val_mae: 6.5652\n",
      "Epoch 264/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 88.4072 - mse: 88.4073 - mae: 6.5663 - val_loss: 83.0554 - val_mse: 83.0554 - val_mae: 6.6520\n",
      "Epoch 265/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.7031 - mse: 88.7031 - mae: 6.5862 - val_loss: 82.5478 - val_mse: 82.5478 - val_mae: 6.5775\n",
      "Epoch 266/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.6058 - mse: 88.6058 - mae: 6.5770 - val_loss: 84.3328 - val_mse: 84.3328 - val_mae: 6.4627\n",
      "Epoch 267/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 88.6813 - mse: 88.6812 - mae: 6.5786 - val_loss: 83.5948 - val_mse: 83.5948 - val_mae: 6.4117\n",
      "Epoch 268/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.6217 - mse: 88.6217 - mae: 6.5689 - val_loss: 83.1699 - val_mse: 83.1699 - val_mae: 6.6185\n",
      "Epoch 269/1000\n",
      "14194/14194 [==============================] - 2s 169us/step - loss: 88.8659 - mse: 88.8659 - mae: 6.5862 - val_loss: 83.3791 - val_mse: 83.3791 - val_mae: 6.6841\n",
      "Epoch 270/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.6966 - mse: 88.6967 - mae: 6.5714 - val_loss: 83.2985 - val_mse: 83.2985 - val_mae: 6.6963\n",
      "Epoch 271/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 88.3455 - mse: 88.3454 - mae: 6.5689 - val_loss: 82.7011 - val_mse: 82.7011 - val_mae: 6.5091\n",
      "Epoch 272/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 88.8246 - mse: 88.8246 - mae: 6.5783 - val_loss: 82.1629 - val_mse: 82.1629 - val_mae: 6.4683\n",
      "Epoch 273/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.6769 - mse: 88.6769 - mae: 6.5830 - val_loss: 83.6085 - val_mse: 83.6086 - val_mae: 6.4150\n",
      "Epoch 274/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 88.4026 - mse: 88.4025 - mae: 6.5757 - val_loss: 85.9638 - val_mse: 85.9638 - val_mae: 6.8061\n",
      "Epoch 275/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 88.7322 - mse: 88.7321 - mae: 6.5762 - val_loss: 84.0143 - val_mse: 84.0143 - val_mae: 6.4751\n",
      "Epoch 276/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 88.7926 - mse: 88.7926 - mae: 6.5803 - val_loss: 82.4854 - val_mse: 82.4854 - val_mae: 6.5687\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.7494 - mse: 88.7494 - mae: 6.5733 - val_loss: 82.4884 - val_mse: 82.4884 - val_mae: 6.4555\n",
      "Epoch 278/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.6651 - mse: 88.6651 - mae: 6.5685 - val_loss: 82.7969 - val_mse: 82.7970 - val_mae: 6.5244\n",
      "Epoch 279/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.6750 - mse: 88.6749 - mae: 6.5695 - val_loss: 82.4544 - val_mse: 82.4544 - val_mae: 6.4896\n",
      "Epoch 280/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.6192 - mse: 88.6192 - mae: 6.5695 - val_loss: 82.6167 - val_mse: 82.6167 - val_mae: 6.4591\n",
      "Epoch 281/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 88.6032 - mse: 88.6032 - mae: 6.5860 - val_loss: 83.9883 - val_mse: 83.9883 - val_mae: 6.4350\n",
      "Epoch 282/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 88.5457 - mse: 88.5457 - mae: 6.5687 - val_loss: 82.4559 - val_mse: 82.4559 - val_mae: 6.5158\n",
      "Epoch 283/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 88.6138 - mse: 88.6138 - mae: 6.5815 - val_loss: 82.6819 - val_mse: 82.6819 - val_mae: 6.4308\n",
      "Epoch 284/1000\n",
      "14194/14194 [==============================] - 2s 165us/step - loss: 88.6827 - mse: 88.6827 - mae: 6.5630 - val_loss: 82.5756 - val_mse: 82.5756 - val_mae: 6.5679\n",
      "Epoch 285/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 88.5724 - mse: 88.5725 - mae: 6.5746 - val_loss: 82.9035 - val_mse: 82.9035 - val_mae: 6.5997\n",
      "Epoch 286/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.6721 - mse: 88.6721 - mae: 6.5681 - val_loss: 82.1835 - val_mse: 82.1835 - val_mae: 6.5092\n",
      "Epoch 287/1000\n",
      "14194/14194 [==============================] - 2s 158us/step - loss: 88.3700 - mse: 88.3700 - mae: 6.5764 - val_loss: 82.3694 - val_mse: 82.3694 - val_mae: 6.4527\n",
      "Epoch 288/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 88.7645 - mse: 88.7646 - mae: 6.5788 - val_loss: 82.8060 - val_mse: 82.8060 - val_mae: 6.5718\n",
      "Epoch 289/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.6701 - mse: 88.6701 - mae: 6.5680 - val_loss: 86.7896 - val_mse: 86.7897 - val_mae: 6.4414\n",
      "Epoch 290/1000\n",
      "14194/14194 [==============================] - 2s 157us/step - loss: 88.6257 - mse: 88.6257 - mae: 6.5743 - val_loss: 82.5833 - val_mse: 82.5833 - val_mae: 6.4742\n",
      "Epoch 291/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 88.5043 - mse: 88.5043 - mae: 6.5625 - val_loss: 82.1101 - val_mse: 82.1101 - val_mae: 6.4853\n",
      "Epoch 292/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.5622 - mse: 88.5622 - mae: 6.5606 - val_loss: 85.1453 - val_mse: 85.1453 - val_mae: 6.8472\n",
      "Epoch 293/1000\n",
      "14194/14194 [==============================] - 2s 172us/step - loss: 88.6359 - mse: 88.6358 - mae: 6.5833 - val_loss: 82.5260 - val_mse: 82.5260 - val_mae: 6.4704\n",
      "Epoch 294/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.5725 - mse: 88.5724 - mae: 6.5821 - val_loss: 82.5452 - val_mse: 82.5452 - val_mae: 6.5833\n",
      "Epoch 295/1000\n",
      "14194/14194 [==============================] - 2s 164us/step - loss: 88.7611 - mse: 88.7610 - mae: 6.5758 - val_loss: 82.5678 - val_mse: 82.5678 - val_mae: 6.4323\n",
      "Epoch 296/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.5688 - mse: 88.5688 - mae: 6.5730 - val_loss: 82.8997 - val_mse: 82.8997 - val_mae: 6.4849\n",
      "Epoch 297/1000\n",
      "14194/14194 [==============================] - 2s 166us/step - loss: 88.5031 - mse: 88.5031 - mae: 6.5755 - val_loss: 83.9512 - val_mse: 83.9511 - val_mae: 6.7113\n",
      "Epoch 298/1000\n",
      "14194/14194 [==============================] - 2s 163us/step - loss: 88.8899 - mse: 88.8899 - mae: 6.5809 - val_loss: 82.7182 - val_mse: 82.7182 - val_mae: 6.6402\n",
      "Epoch 299/1000\n",
      "14194/14194 [==============================] - 2s 159us/step - loss: 88.5136 - mse: 88.5137 - mae: 6.5762 - val_loss: 83.8491 - val_mse: 83.8491 - val_mae: 6.4047\n",
      "Epoch 300/1000\n",
      "14194/14194 [==============================] - 2s 168us/step - loss: 88.7557 - mse: 88.7558 - mae: 6.5705 - val_loss: 82.6983 - val_mse: 82.6982 - val_mae: 6.4249\n",
      "Epoch 301/1000\n",
      "14194/14194 [==============================] - 2s 161us/step - loss: 88.5377 - mse: 88.5376 - mae: 6.5762 - val_loss: 82.1965 - val_mse: 82.1965 - val_mae: 6.4829\n",
      "Epoch 302/1000\n",
      "14194/14194 [==============================] - 2s 162us/step - loss: 88.4847 - mse: 88.4847 - mae: 6.5677 - val_loss: 82.6756 - val_mse: 82.6756 - val_mae: 6.4575\n",
      "Epoch 303/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 88.4670 - mse: 88.4671 - mae: 6.5714 - val_loss: 84.2306 - val_mse: 84.2306 - val_mae: 6.4738\n",
      "Epoch 304/1000\n",
      "14194/14194 [==============================] - 2s 176us/step - loss: 88.5126 - mse: 88.5126 - mae: 6.5756 - val_loss: 82.9611 - val_mse: 82.9612 - val_mae: 6.4617\n",
      "Epoch 305/1000\n",
      "14194/14194 [==============================] - 3s 189us/step - loss: 88.5460 - mse: 88.5461 - mae: 6.5757 - val_loss: 83.1095 - val_mse: 83.1095 - val_mae: 6.4340\n",
      "Epoch 306/1000\n",
      "14194/14194 [==============================] - 2s 167us/step - loss: 88.3707 - mse: 88.3707 - mae: 6.5758 - val_loss: 83.2219 - val_mse: 83.2219 - val_mae: 6.4133\n",
      "Epoch 307/1000\n",
      "14194/14194 [==============================] - 2s 155us/step - loss: 88.5195 - mse: 88.5195 - mae: 6.5633 - val_loss: 82.9012 - val_mse: 82.9012 - val_mae: 6.4565\n",
      "Epoch 308/1000\n",
      "14194/14194 [==============================] - 2s 170us/step - loss: 88.8112 - mse: 88.8112 - mae: 6.5796 - val_loss: 82.3460 - val_mse: 82.3460 - val_mae: 6.4965\n",
      "Epoch 309/1000\n",
      "14194/14194 [==============================] - 9s 627us/step - loss: 88.6696 - mse: 88.6696 - mae: 6.5643 - val_loss: 82.5039 - val_mse: 82.5040 - val_mae: 6.5381\n",
      "Epoch 310/1000\n",
      "14194/14194 [==============================] - 6s 408us/step - loss: 88.5259 - mse: 88.5259 - mae: 6.5619 - val_loss: 89.2411 - val_mse: 89.2411 - val_mae: 7.0704\n",
      "Epoch 311/1000\n",
      "14194/14194 [==============================] - 4s 307us/step - loss: 88.6230 - mse: 88.6231 - mae: 6.5816 - val_loss: 82.9575 - val_mse: 82.9576 - val_mae: 6.4229\n",
      "Epoch 312/1000\n",
      "14194/14194 [==============================] - 4s 294us/step - loss: 88.8159 - mse: 88.8158 - mae: 6.5723 - val_loss: 82.8659 - val_mse: 82.8659 - val_mae: 6.4430\n",
      "Epoch 313/1000\n",
      "14194/14194 [==============================] - 4s 288us/step - loss: 88.6711 - mse: 88.6710 - mae: 6.5697 - val_loss: 84.0649 - val_mse: 84.0649 - val_mae: 6.7325\n",
      "Epoch 314/1000\n",
      "14194/14194 [==============================] - 8097s 570ms/step - loss: 88.6883 - mse: 88.6883 - mae: 6.5712 - val_loss: 82.5518 - val_mse: 82.5518 - val_mae: 6.5436\n",
      "Epoch 315/1000\n",
      "14194/14194 [==============================] - 4s 280us/step - loss: 88.9110 - mse: 88.9110 - mae: 6.5737 - val_loss: 82.3329 - val_mse: 82.3329 - val_mae: 6.5177\n",
      "Epoch 316/1000\n",
      "14194/14194 [==============================] - 3s 205us/step - loss: 88.8132 - mse: 88.8131 - mae: 6.5790 - val_loss: 82.7451 - val_mse: 82.7451 - val_mae: 6.5373\n",
      "Epoch 317/1000\n",
      "14194/14194 [==============================] - 3s 225us/step - loss: 88.4452 - mse: 88.4452 - mae: 6.5645 - val_loss: 82.7198 - val_mse: 82.7198 - val_mae: 6.5668\n",
      "Epoch 318/1000\n",
      "14194/14194 [==============================] - 3s 211us/step - loss: 88.3695 - mse: 88.3695 - mae: 6.5642 - val_loss: 82.7677 - val_mse: 82.7678 - val_mae: 6.6043\n",
      "Epoch 319/1000\n",
      "14194/14194 [==============================] - 3s 181us/step - loss: 88.6620 - mse: 88.6620 - mae: 6.5794 - val_loss: 82.8709 - val_mse: 82.8709 - val_mae: 6.4706\n",
      "Epoch 320/1000\n",
      "14194/14194 [==============================] - 3s 177us/step - loss: 88.4419 - mse: 88.4419 - mae: 6.5744 - val_loss: 82.5087 - val_mse: 82.5087 - val_mae: 6.4959\n",
      "Epoch 321/1000\n",
      "14194/14194 [==============================] - 3s 196us/step - loss: 88.7437 - mse: 88.7437 - mae: 6.5728 - val_loss: 82.7454 - val_mse: 82.7454 - val_mae: 6.5652\n",
      "Epoch 322/1000\n",
      "14194/14194 [==============================] - 2s 160us/step - loss: 88.5697 - mse: 88.5696 - mae: 6.5709 - val_loss: 82.5631 - val_mse: 82.5631 - val_mae: 6.5798\n",
      "Epoch 323/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.7033 - mse: 88.7033 - mae: 6.5723 - val_loss: 82.5876 - val_mse: 82.5876 - val_mae: 6.4496\n",
      "Epoch 324/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.3898 - mse: 88.3898 - mae: 6.5646 - val_loss: 82.4087 - val_mse: 82.4088 - val_mae: 6.4515\n",
      "Epoch 325/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 88.7191 - mse: 88.7191 - mae: 6.5707 - val_loss: 82.1665 - val_mse: 82.1665 - val_mae: 6.4675\n",
      "Epoch 326/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 88.5719 - mse: 88.5719 - mae: 6.5687 - val_loss: 83.4045 - val_mse: 83.4046 - val_mae: 6.4702\n",
      "Epoch 327/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.3233 - mse: 88.3233 - mae: 6.5659 - val_loss: 82.7552 - val_mse: 82.7552 - val_mae: 6.6093\n",
      "Epoch 328/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.6065 - mse: 88.6065 - mae: 6.5776 - val_loss: 82.9236 - val_mse: 82.9236 - val_mae: 6.4503\n",
      "Epoch 329/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.7006 - mse: 88.7007 - mae: 6.5741 - val_loss: 82.2573 - val_mse: 82.2574 - val_mae: 6.5389\n",
      "Epoch 330/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.6815 - mse: 88.6816 - mae: 6.5797 - val_loss: 84.9669 - val_mse: 84.9668 - val_mae: 6.8056\n",
      "Epoch 331/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.6474 - mse: 88.6475 - mae: 6.5703 - val_loss: 84.0129 - val_mse: 84.0129 - val_mae: 6.5228\n",
      "Epoch 332/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.4575 - mse: 88.4575 - mae: 6.5697 - val_loss: 82.7618 - val_mse: 82.7618 - val_mae: 6.4327\n",
      "Epoch 333/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.5976 - mse: 88.5977 - mae: 6.5734 - val_loss: 82.2033 - val_mse: 82.2033 - val_mae: 6.5330\n",
      "Epoch 334/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 88.5781 - mse: 88.5781 - mae: 6.5763 - val_loss: 82.4364 - val_mse: 82.4364 - val_mae: 6.4351\n",
      "Epoch 335/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.4720 - mse: 88.4720 - mae: 6.5605 - val_loss: 83.2861 - val_mse: 83.2862 - val_mae: 6.5356\n",
      "Epoch 336/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 88.4007 - mse: 88.4007 - mae: 6.5737 - val_loss: 84.8235 - val_mse: 84.8235 - val_mae: 6.4428\n",
      "Epoch 337/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.4750 - mse: 88.4750 - mae: 6.5740 - val_loss: 82.1893 - val_mse: 82.1893 - val_mae: 6.4758\n",
      "Epoch 338/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.5435 - mse: 88.5435 - mae: 6.5740 - val_loss: 82.4312 - val_mse: 82.4312 - val_mae: 6.6008\n",
      "Epoch 339/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.5783 - mse: 88.5783 - mae: 6.5704 - val_loss: 82.8715 - val_mse: 82.8715 - val_mae: 6.5963\n",
      "Epoch 340/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.5173 - mse: 88.5173 - mae: 6.5665 - val_loss: 82.5613 - val_mse: 82.5613 - val_mae: 6.5514\n",
      "Epoch 341/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.4449 - mse: 88.4450 - mae: 6.5703 - val_loss: 82.5491 - val_mse: 82.5491 - val_mae: 6.4483\n",
      "Epoch 342/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.5786 - mse: 88.5787 - mae: 6.5688 - val_loss: 82.0325 - val_mse: 82.0325 - val_mae: 6.5008\n",
      "Epoch 343/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.7989 - mse: 88.7989 - mae: 6.5746 - val_loss: 83.4908 - val_mse: 83.4908 - val_mae: 6.4531\n",
      "Epoch 344/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.7548 - mse: 88.7549 - mae: 6.5781 - val_loss: 82.4359 - val_mse: 82.4359 - val_mae: 6.4532\n",
      "Epoch 345/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.8396 - mse: 88.8396 - mae: 6.5658 - val_loss: 82.3459 - val_mse: 82.3459 - val_mae: 6.4818\n",
      "Epoch 346/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.7573 - mse: 88.7572 - mae: 6.5804 - val_loss: 82.6602 - val_mse: 82.6603 - val_mae: 6.4443\n",
      "Epoch 347/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.6351 - mse: 88.6351 - mae: 6.5702 - val_loss: 83.1111 - val_mse: 83.1111 - val_mae: 6.6341\n",
      "Epoch 348/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.4758 - mse: 88.4757 - mae: 6.5666 - val_loss: 82.3806 - val_mse: 82.3805 - val_mae: 6.5207\n",
      "Epoch 349/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.6142 - mse: 88.6142 - mae: 6.5696 - val_loss: 82.0108 - val_mse: 82.0108 - val_mae: 6.4938\n",
      "Epoch 350/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.5103 - mse: 88.5102 - mae: 6.5593 - val_loss: 82.7661 - val_mse: 82.7661 - val_mae: 6.5729\n",
      "Epoch 351/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 88.5334 - mse: 88.5334 - mae: 6.5652 - val_loss: 82.5452 - val_mse: 82.5452 - val_mae: 6.5081\n",
      "Epoch 352/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.6661 - mse: 88.6661 - mae: 6.5622 - val_loss: 82.4725 - val_mse: 82.4724 - val_mae: 6.5803\n",
      "Epoch 353/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.4919 - mse: 88.4919 - mae: 6.5717 - val_loss: 84.4414 - val_mse: 84.4414 - val_mae: 6.4104\n",
      "Epoch 354/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.4796 - mse: 88.4796 - mae: 6.5616 - val_loss: 82.3397 - val_mse: 82.3397 - val_mae: 6.5412\n",
      "Epoch 355/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 88.2677 - mse: 88.2677 - mae: 6.5579 - val_loss: 84.1181 - val_mse: 84.1181 - val_mae: 6.6828\n",
      "Epoch 356/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.4124 - mse: 88.4124 - mae: 6.5584 - val_loss: 82.5026 - val_mse: 82.5026 - val_mae: 6.4988\n",
      "Epoch 357/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.7337 - mse: 88.7337 - mae: 6.5787 - val_loss: 83.6853 - val_mse: 83.6853 - val_mae: 6.4116\n",
      "Epoch 358/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.7257 - mse: 88.7257 - mae: 6.5735 - val_loss: 82.7787 - val_mse: 82.7787 - val_mae: 6.6273\n",
      "Epoch 359/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.6490 - mse: 88.6490 - mae: 6.5700 - val_loss: 82.1905 - val_mse: 82.1905 - val_mae: 6.5646\n",
      "Epoch 360/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.5703 - mse: 88.5703 - mae: 6.5726 - val_loss: 82.5317 - val_mse: 82.5317 - val_mae: 6.6008\n",
      "Epoch 361/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 88.6702 - mse: 88.6702 - mae: 6.5662 - val_loss: 82.3614 - val_mse: 82.3614 - val_mae: 6.4481\n",
      "Epoch 362/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 88.3876 - mse: 88.3877 - mae: 6.5712 - val_loss: 83.1393 - val_mse: 83.1393 - val_mae: 6.4337\n",
      "Epoch 363/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.4308 - mse: 88.4309 - mae: 6.5664 - val_loss: 84.2555 - val_mse: 84.2555 - val_mae: 6.4109\n",
      "Epoch 364/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.1600 - mse: 88.1600 - mae: 6.5613 - val_loss: 82.3126 - val_mse: 82.3127 - val_mae: 6.4902\n",
      "Epoch 365/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.4798 - mse: 88.4799 - mae: 6.5669 - val_loss: 82.5717 - val_mse: 82.5717 - val_mae: 6.5567\n",
      "Epoch 366/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.3291 - mse: 88.3292 - mae: 6.5700 - val_loss: 82.6513 - val_mse: 82.6513 - val_mae: 6.5534\n",
      "Epoch 367/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.5003 - mse: 88.5004 - mae: 6.5730 - val_loss: 82.0793 - val_mse: 82.0793 - val_mae: 6.5023\n",
      "Epoch 368/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.7510 - mse: 88.7511 - mae: 6.5789 - val_loss: 82.8706 - val_mse: 82.8706 - val_mae: 6.4340\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.6233 - mse: 88.6232 - mae: 6.5751 - val_loss: 82.7350 - val_mse: 82.7350 - val_mae: 6.5406\n",
      "Epoch 370/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.6026 - mse: 88.6026 - mae: 6.5681 - val_loss: 82.8474 - val_mse: 82.8474 - val_mae: 6.5384\n",
      "Epoch 371/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 88.2784 - mse: 88.2785 - mae: 6.5679 - val_loss: 85.1697 - val_mse: 85.1697 - val_mae: 6.9008\n",
      "Epoch 372/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.3263 - mse: 88.3262 - mae: 6.5557 - val_loss: 82.6791 - val_mse: 82.6791 - val_mae: 6.4511\n",
      "Epoch 373/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.7206 - mse: 88.7206 - mae: 6.5792 - val_loss: 82.1635 - val_mse: 82.1635 - val_mae: 6.5016\n",
      "Epoch 374/1000\n",
      "14194/14194 [==============================] - 2s 154us/step - loss: 88.4860 - mse: 88.4860 - mae: 6.5601 - val_loss: 82.3270 - val_mse: 82.3270 - val_mae: 6.5497\n",
      "Epoch 375/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.2460 - mse: 88.2459 - mae: 6.5575 - val_loss: 82.7911 - val_mse: 82.7911 - val_mae: 6.4838\n",
      "Epoch 376/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 88.4434 - mse: 88.4433 - mae: 6.5597 - val_loss: 82.6932 - val_mse: 82.6931 - val_mae: 6.4679\n",
      "Epoch 377/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.3241 - mse: 88.3242 - mae: 6.5669 - val_loss: 82.2749 - val_mse: 82.2748 - val_mae: 6.4437\n",
      "Epoch 378/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.6217 - mse: 88.6218 - mae: 6.5720 - val_loss: 82.3040 - val_mse: 82.3039 - val_mae: 6.4558\n",
      "Epoch 379/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.5778 - mse: 88.5778 - mae: 6.5618 - val_loss: 82.4912 - val_mse: 82.4912 - val_mae: 6.4538\n",
      "Epoch 380/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.3968 - mse: 88.3969 - mae: 6.5687 - val_loss: 84.0714 - val_mse: 84.0714 - val_mae: 6.4959\n",
      "Epoch 381/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.7303 - mse: 88.7303 - mae: 6.5750 - val_loss: 82.7543 - val_mse: 82.7544 - val_mae: 6.6373\n",
      "Epoch 382/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.6000 - mse: 88.6000 - mae: 6.5858 - val_loss: 84.5389 - val_mse: 84.5389 - val_mae: 6.3906\n",
      "Epoch 383/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.3069 - mse: 88.3070 - mae: 6.5615 - val_loss: 82.1331 - val_mse: 82.1331 - val_mae: 6.5268\n",
      "Epoch 384/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.5978 - mse: 88.5978 - mae: 6.5637 - val_loss: 82.2379 - val_mse: 82.2380 - val_mae: 6.4953\n",
      "Epoch 385/1000\n",
      "14194/14194 [==============================] - 2s 173us/step - loss: 88.5642 - mse: 88.5642 - mae: 6.5633 - val_loss: 82.7460 - val_mse: 82.7460 - val_mae: 6.4292\n",
      "Epoch 386/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.1846 - mse: 88.1846 - mae: 6.5579 - val_loss: 82.2749 - val_mse: 82.2749 - val_mae: 6.5024\n",
      "Epoch 387/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.6356 - mse: 88.6356 - mae: 6.5784 - val_loss: 82.6214 - val_mse: 82.6214 - val_mae: 6.4360\n",
      "Epoch 388/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.2998 - mse: 88.2998 - mae: 6.5742 - val_loss: 85.2381 - val_mse: 85.2381 - val_mae: 6.4026\n",
      "Epoch 389/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.6056 - mse: 88.6057 - mae: 6.5664 - val_loss: 82.7032 - val_mse: 82.7032 - val_mae: 6.5496\n",
      "Epoch 390/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.3728 - mse: 88.3727 - mae: 6.5532 - val_loss: 83.2824 - val_mse: 83.2823 - val_mae: 6.4500\n",
      "Epoch 391/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.4074 - mse: 88.4073 - mae: 6.5729 - val_loss: 82.8908 - val_mse: 82.8908 - val_mae: 6.4324\n",
      "Epoch 392/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.3667 - mse: 88.3667 - mae: 6.5584 - val_loss: 83.3451 - val_mse: 83.3451 - val_mae: 6.5311\n",
      "Epoch 393/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 88.5448 - mse: 88.5448 - mae: 6.5818 - val_loss: 82.8379 - val_mse: 82.8379 - val_mae: 6.4099\n",
      "Epoch 394/1000\n",
      "14194/14194 [==============================] - 2s 151us/step - loss: 88.7714 - mse: 88.7713 - mae: 6.5668 - val_loss: 82.8869 - val_mse: 82.8870 - val_mae: 6.5699\n",
      "Epoch 395/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 88.3674 - mse: 88.3672 - mae: 6.5573 - val_loss: 83.2274 - val_mse: 83.2274 - val_mae: 6.7177\n",
      "Epoch 396/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.4085 - mse: 88.4086 - mae: 6.5688 - val_loss: 82.8484 - val_mse: 82.8485 - val_mae: 6.4677\n",
      "Epoch 397/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.5173 - mse: 88.5174 - mae: 6.5700 - val_loss: 82.7120 - val_mse: 82.7120 - val_mae: 6.5361\n",
      "Epoch 398/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.2375 - mse: 88.2375 - mae: 6.5616 - val_loss: 82.6895 - val_mse: 82.6895 - val_mae: 6.5209\n",
      "Epoch 399/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.5576 - mse: 88.5576 - mae: 6.5712 - val_loss: 82.3548 - val_mse: 82.3548 - val_mae: 6.5288\n",
      "Epoch 400/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.4115 - mse: 88.4115 - mae: 6.5706 - val_loss: 82.6445 - val_mse: 82.6445 - val_mae: 6.5460\n",
      "Epoch 401/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 88.6321 - mse: 88.6321 - mae: 6.5700 - val_loss: 82.9109 - val_mse: 82.9109 - val_mae: 6.4300\n",
      "Epoch 402/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.4192 - mse: 88.4192 - mae: 6.5702 - val_loss: 83.9177 - val_mse: 83.9176 - val_mae: 6.4011\n",
      "Epoch 403/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.3844 - mse: 88.3843 - mae: 6.5699 - val_loss: 82.9702 - val_mse: 82.9702 - val_mae: 6.4334\n",
      "Epoch 404/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.4050 - mse: 88.4050 - mae: 6.5697 - val_loss: 83.0593 - val_mse: 83.0593 - val_mae: 6.4673\n",
      "Epoch 405/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.5187 - mse: 88.5187 - mae: 6.5743 - val_loss: 82.0120 - val_mse: 82.0120 - val_mae: 6.4693\n",
      "Epoch 406/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 88.5386 - mse: 88.5386 - mae: 6.5632 - val_loss: 82.3398 - val_mse: 82.3399 - val_mae: 6.5168\n",
      "Epoch 407/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.3111 - mse: 88.3111 - mae: 6.5604 - val_loss: 82.8462 - val_mse: 82.8462 - val_mae: 6.4298\n",
      "Epoch 408/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.6057 - mse: 88.6056 - mae: 6.5668 - val_loss: 82.2044 - val_mse: 82.2044 - val_mae: 6.5488\n",
      "Epoch 409/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.4300 - mse: 88.4300 - mae: 6.5623 - val_loss: 82.8052 - val_mse: 82.8052 - val_mae: 6.4364\n",
      "Epoch 410/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.5103 - mse: 88.5103 - mae: 6.5709 - val_loss: 82.1318 - val_mse: 82.1318 - val_mae: 6.4463\n",
      "Epoch 411/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.5719 - mse: 88.5719 - mae: 6.5726 - val_loss: 82.4611 - val_mse: 82.4611 - val_mae: 6.4534\n",
      "Epoch 412/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.5918 - mse: 88.5917 - mae: 6.5765 - val_loss: 84.9714 - val_mse: 84.9714 - val_mae: 6.4296\n",
      "Epoch 413/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.8249 - mse: 88.8249 - mae: 6.5755 - val_loss: 82.0572 - val_mse: 82.0572 - val_mae: 6.4967\n",
      "Epoch 414/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.3554 - mse: 88.3554 - mae: 6.5662 - val_loss: 83.6183 - val_mse: 83.6183 - val_mae: 6.7683\n",
      "Epoch 415/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 135us/step - loss: 88.5771 - mse: 88.5772 - mae: 6.5807 - val_loss: 83.7943 - val_mse: 83.7943 - val_mae: 6.4331\n",
      "Epoch 416/1000\n",
      "14194/14194 [==============================] - 2s 139us/step - loss: 88.6322 - mse: 88.6322 - mae: 6.5739 - val_loss: 82.2125 - val_mse: 82.2125 - val_mae: 6.4777\n",
      "Epoch 417/1000\n",
      "14194/14194 [==============================] - 2s 134us/step - loss: 88.3282 - mse: 88.3282 - mae: 6.5627 - val_loss: 82.3716 - val_mse: 82.3716 - val_mae: 6.4551\n",
      "Epoch 418/1000\n",
      "14194/14194 [==============================] - 2s 145us/step - loss: 88.6491 - mse: 88.6491 - mae: 6.5706 - val_loss: 82.5020 - val_mse: 82.5020 - val_mae: 6.4716\n",
      "Epoch 419/1000\n",
      "14194/14194 [==============================] - 2s 152us/step - loss: 88.4742 - mse: 88.4743 - mae: 6.5683 - val_loss: 82.6857 - val_mse: 82.6857 - val_mae: 6.4322\n",
      "Epoch 420/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.3334 - mse: 88.3334 - mae: 6.5515 - val_loss: 82.6505 - val_mse: 82.6505 - val_mae: 6.5939\n",
      "Epoch 421/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.5022 - mse: 88.5022 - mae: 6.5782 - val_loss: 82.8579 - val_mse: 82.8579 - val_mae: 6.4221\n",
      "Epoch 422/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.3389 - mse: 88.3389 - mae: 6.5561 - val_loss: 83.7226 - val_mse: 83.7226 - val_mae: 6.4025\n",
      "Epoch 423/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.6212 - mse: 88.6212 - mae: 6.5711 - val_loss: 82.6555 - val_mse: 82.6555 - val_mae: 6.4601\n",
      "Epoch 424/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.3272 - mse: 88.3272 - mae: 6.5581 - val_loss: 82.7759 - val_mse: 82.7759 - val_mae: 6.4900\n",
      "Epoch 425/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.3668 - mse: 88.3668 - mae: 6.5623 - val_loss: 82.7469 - val_mse: 82.7469 - val_mae: 6.4173\n",
      "Epoch 426/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.5057 - mse: 88.5058 - mae: 6.5675 - val_loss: 83.4171 - val_mse: 83.4171 - val_mae: 6.4969\n",
      "Epoch 427/1000\n",
      "14194/14194 [==============================] - 1s 90us/step - loss: 88.2823 - mse: 88.2824 - mae: 6.5587 - val_loss: 83.3777 - val_mse: 83.3777 - val_mae: 6.5169\n",
      "Epoch 428/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.6057 - mse: 88.6057 - mae: 6.5788 - val_loss: 81.9627 - val_mse: 81.9627 - val_mae: 6.4727\n",
      "Epoch 429/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.3991 - mse: 88.3991 - mae: 6.5664 - val_loss: 82.3017 - val_mse: 82.3018 - val_mae: 6.4763\n",
      "Epoch 430/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.5448 - mse: 88.5448 - mae: 6.5634 - val_loss: 83.0754 - val_mse: 83.0754 - val_mae: 6.6896\n",
      "Epoch 431/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.5781 - mse: 88.5782 - mae: 6.5683 - val_loss: 82.4420 - val_mse: 82.4421 - val_mae: 6.5360\n",
      "Epoch 432/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.4247 - mse: 88.4247 - mae: 6.5739 - val_loss: 84.9817 - val_mse: 84.9817 - val_mae: 6.4545\n",
      "Epoch 433/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.4978 - mse: 88.4977 - mae: 6.5612 - val_loss: 82.4937 - val_mse: 82.4937 - val_mae: 6.4893\n",
      "Epoch 434/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.5104 - mse: 88.5104 - mae: 6.5657 - val_loss: 84.6770 - val_mse: 84.6769 - val_mae: 6.4211\n",
      "Epoch 435/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4073 - mse: 88.4072 - mae: 6.5596 - val_loss: 85.1363 - val_mse: 85.1363 - val_mae: 6.4117\n",
      "Epoch 436/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.3995 - mse: 88.3995 - mae: 6.5646 - val_loss: 85.4519 - val_mse: 85.4519 - val_mae: 6.8069\n",
      "Epoch 437/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.3943 - mse: 88.3943 - mae: 6.5613 - val_loss: 82.0697 - val_mse: 82.0698 - val_mae: 6.5122\n",
      "Epoch 438/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.4901 - mse: 88.4901 - mae: 6.5691 - val_loss: 82.7864 - val_mse: 82.7864 - val_mae: 6.4495\n",
      "Epoch 439/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.6058 - mse: 88.6058 - mae: 6.5715 - val_loss: 83.7096 - val_mse: 83.7096 - val_mae: 6.3985\n",
      "Epoch 440/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.4475 - mse: 88.4475 - mae: 6.5532 - val_loss: 82.4745 - val_mse: 82.4745 - val_mae: 6.5804\n",
      "Epoch 441/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4117 - mse: 88.4117 - mae: 6.5617 - val_loss: 82.2070 - val_mse: 82.2070 - val_mae: 6.4651\n",
      "Epoch 442/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.4664 - mse: 88.4664 - mae: 6.5665 - val_loss: 82.5013 - val_mse: 82.5013 - val_mae: 6.5833\n",
      "Epoch 443/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.5752 - mse: 88.5752 - mae: 6.5867 - val_loss: 83.5931 - val_mse: 83.5931 - val_mae: 6.4559\n",
      "Epoch 444/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.6328 - mse: 88.6329 - mae: 6.5760 - val_loss: 81.8894 - val_mse: 81.8894 - val_mae: 6.4898\n",
      "Epoch 445/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.6263 - mse: 88.6263 - mae: 6.5788 - val_loss: 81.9354 - val_mse: 81.9354 - val_mae: 6.4733\n",
      "Epoch 446/1000\n",
      "14194/14194 [==============================] - 1s 88us/step - loss: 88.5809 - mse: 88.5810 - mae: 6.5768 - val_loss: 84.9004 - val_mse: 84.9004 - val_mae: 6.8015\n",
      "Epoch 447/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.1900 - mse: 88.1898 - mae: 6.5701 - val_loss: 85.4348 - val_mse: 85.4348 - val_mae: 6.7315\n",
      "Epoch 448/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4034 - mse: 88.4034 - mae: 6.5696 - val_loss: 82.4706 - val_mse: 82.4706 - val_mae: 6.5620\n",
      "Epoch 449/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.3499 - mse: 88.3499 - mae: 6.5688 - val_loss: 84.7175 - val_mse: 84.7175 - val_mae: 6.7823\n",
      "Epoch 450/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 88.5199 - mse: 88.5199 - mae: 6.5713 - val_loss: 84.5819 - val_mse: 84.5819 - val_mae: 6.7841\n",
      "Epoch 451/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.5295 - mse: 88.5294 - mae: 6.5641 - val_loss: 82.1509 - val_mse: 82.1509 - val_mae: 6.4600\n",
      "Epoch 452/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.4040 - mse: 88.4040 - mae: 6.5583 - val_loss: 82.4279 - val_mse: 82.4279 - val_mae: 6.4797\n",
      "Epoch 453/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.4067 - mse: 88.4067 - mae: 6.5695 - val_loss: 82.3287 - val_mse: 82.3287 - val_mae: 6.4535\n",
      "Epoch 454/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.3102 - mse: 88.3102 - mae: 6.5656 - val_loss: 82.2787 - val_mse: 82.2787 - val_mae: 6.4785\n",
      "Epoch 455/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.4545 - mse: 88.4545 - mae: 6.5710 - val_loss: 82.3369 - val_mse: 82.3369 - val_mae: 6.4429\n",
      "Epoch 456/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.1968 - mse: 88.1969 - mae: 6.5556 - val_loss: 82.9613 - val_mse: 82.9613 - val_mae: 6.4322\n",
      "Epoch 457/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.5078 - mse: 88.5077 - mae: 6.5667 - val_loss: 82.0678 - val_mse: 82.0678 - val_mae: 6.5571\n",
      "Epoch 458/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.5768 - mse: 88.5769 - mae: 6.5671 - val_loss: 83.1866 - val_mse: 83.1866 - val_mae: 6.4035\n",
      "Epoch 459/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.4823 - mse: 88.4823 - mae: 6.5621 - val_loss: 83.1510 - val_mse: 83.1510 - val_mae: 6.5582\n",
      "Epoch 460/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.0996 - mse: 88.0996 - mae: 6.5541 - val_loss: 82.3686 - val_mse: 82.3686 - val_mae: 6.4311\n",
      "Epoch 461/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.1778 - mse: 88.1778 - mae: 6.5622 - val_loss: 83.0811 - val_mse: 83.0811 - val_mae: 6.6683\n",
      "Epoch 462/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.4217 - mse: 88.4217 - mae: 6.5678 - val_loss: 82.9395 - val_mse: 82.9395 - val_mae: 6.4067\n",
      "Epoch 463/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.3879 - mse: 88.3879 - mae: 6.5634 - val_loss: 82.8761 - val_mse: 82.8761 - val_mae: 6.6467\n",
      "Epoch 464/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4682 - mse: 88.4682 - mae: 6.5630 - val_loss: 83.5441 - val_mse: 83.5441 - val_mae: 6.4284\n",
      "Epoch 465/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.3427 - mse: 88.3427 - mae: 6.5652 - val_loss: 82.6184 - val_mse: 82.6184 - val_mae: 6.4610\n",
      "Epoch 466/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 88.2606 - mse: 88.2605 - mae: 6.5568 - val_loss: 82.7936 - val_mse: 82.7935 - val_mae: 6.6507\n",
      "Epoch 467/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.3815 - mse: 88.3815 - mae: 6.5716 - val_loss: 81.8477 - val_mse: 81.8477 - val_mae: 6.4975\n",
      "Epoch 468/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.3990 - mse: 88.3991 - mae: 6.5612 - val_loss: 82.5732 - val_mse: 82.5732 - val_mae: 6.6394\n",
      "Epoch 469/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.1715 - mse: 88.1715 - mae: 6.5606 - val_loss: 82.5446 - val_mse: 82.5446 - val_mae: 6.5365\n",
      "Epoch 470/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.4523 - mse: 88.4523 - mae: 6.5662 - val_loss: 82.1845 - val_mse: 82.1845 - val_mae: 6.5701\n",
      "Epoch 471/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.3547 - mse: 88.3547 - mae: 6.5584 - val_loss: 82.4063 - val_mse: 82.4063 - val_mae: 6.4724\n",
      "Epoch 472/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 88.3343 - mse: 88.3343 - mae: 6.5654 - val_loss: 82.7613 - val_mse: 82.7613 - val_mae: 6.5252\n",
      "Epoch 473/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.2554 - mse: 88.2555 - mae: 6.5576 - val_loss: 82.4054 - val_mse: 82.4054 - val_mae: 6.4957\n",
      "Epoch 474/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.3346 - mse: 88.3346 - mae: 6.5664 - val_loss: 82.2631 - val_mse: 82.2632 - val_mae: 6.4378\n",
      "Epoch 475/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.4307 - mse: 88.4307 - mae: 6.5615 - val_loss: 84.1199 - val_mse: 84.1199 - val_mae: 6.7288\n",
      "Epoch 476/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.6409 - mse: 88.6408 - mae: 6.5695 - val_loss: 82.4118 - val_mse: 82.4118 - val_mae: 6.4791\n",
      "Epoch 477/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4126 - mse: 88.4125 - mae: 6.5510 - val_loss: 83.3497 - val_mse: 83.3497 - val_mae: 6.4033\n",
      "Epoch 478/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.2720 - mse: 88.2719 - mae: 6.5584 - val_loss: 82.2086 - val_mse: 82.2086 - val_mae: 6.4637\n",
      "Epoch 479/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.2660 - mse: 88.2660 - mae: 6.5608 - val_loss: 84.8654 - val_mse: 84.8654 - val_mae: 6.3990\n",
      "Epoch 480/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.2382 - mse: 88.2382 - mae: 6.5554 - val_loss: 86.3596 - val_mse: 86.3596 - val_mae: 6.9134\n",
      "Epoch 481/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.2426 - mse: 88.2426 - mae: 6.5588 - val_loss: 83.5778 - val_mse: 83.5778 - val_mae: 6.4346\n",
      "Epoch 482/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.2040 - mse: 88.2040 - mae: 6.5592 - val_loss: 85.3366 - val_mse: 85.3366 - val_mae: 6.8637\n",
      "Epoch 483/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.3795 - mse: 88.3795 - mae: 6.5685 - val_loss: 83.2444 - val_mse: 83.2443 - val_mae: 6.6649\n",
      "Epoch 484/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.1563 - mse: 88.1563 - mae: 6.5577 - val_loss: 86.3195 - val_mse: 86.3195 - val_mae: 6.8478\n",
      "Epoch 485/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1729 - mse: 88.1730 - mae: 6.5688 - val_loss: 83.6460 - val_mse: 83.6460 - val_mae: 6.4098\n",
      "Epoch 486/1000\n",
      "14194/14194 [==============================] - 2s 141us/step - loss: 88.4549 - mse: 88.4548 - mae: 6.5668 - val_loss: 82.5266 - val_mse: 82.5266 - val_mae: 6.5395\n",
      "Epoch 487/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 88.3984 - mse: 88.3983 - mae: 6.5562 - val_loss: 82.5902 - val_mse: 82.5902 - val_mae: 6.4513\n",
      "Epoch 488/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 88.1512 - mse: 88.1513 - mae: 6.5385 - val_loss: 85.5483 - val_mse: 85.5482 - val_mae: 6.5069\n",
      "Epoch 489/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.3243 - mse: 88.3243 - mae: 6.5580 - val_loss: 82.1180 - val_mse: 82.1180 - val_mae: 6.4786\n",
      "Epoch 490/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 88.3873 - mse: 88.3873 - mae: 6.5597 - val_loss: 82.3024 - val_mse: 82.3024 - val_mae: 6.4287\n",
      "Epoch 491/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.3295 - mse: 88.3294 - mae: 6.5622 - val_loss: 85.2956 - val_mse: 85.2956 - val_mae: 6.8015\n",
      "Epoch 492/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.2433 - mse: 88.2433 - mae: 6.5607 - val_loss: 82.2148 - val_mse: 82.2148 - val_mae: 6.5428\n",
      "Epoch 493/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.6138 - mse: 88.6138 - mae: 6.5767 - val_loss: 82.3516 - val_mse: 82.3516 - val_mae: 6.4725\n",
      "Epoch 494/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.2065 - mse: 88.2065 - mae: 6.5644 - val_loss: 82.2808 - val_mse: 82.2808 - val_mae: 6.4509\n",
      "Epoch 495/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.4459 - mse: 88.4459 - mae: 6.5651 - val_loss: 81.9526 - val_mse: 81.9526 - val_mae: 6.4871\n",
      "Epoch 496/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 88.4391 - mse: 88.4392 - mae: 6.5785 - val_loss: 82.3104 - val_mse: 82.3104 - val_mae: 6.4336\n",
      "Epoch 497/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.3101 - mse: 88.3101 - mae: 6.5529 - val_loss: 83.5075 - val_mse: 83.5075 - val_mae: 6.4017\n",
      "Epoch 498/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.3192 - mse: 88.3192 - mae: 6.5616 - val_loss: 82.4280 - val_mse: 82.4281 - val_mae: 6.4880\n",
      "Epoch 499/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1209 - mse: 88.1209 - mae: 6.5660 - val_loss: 82.9988 - val_mse: 82.9988 - val_mae: 6.4040\n",
      "Epoch 500/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.0938 - mse: 88.0938 - mae: 6.5500 - val_loss: 82.1823 - val_mse: 82.1823 - val_mae: 6.5077\n",
      "Epoch 501/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.4331 - mse: 88.4331 - mae: 6.5625 - val_loss: 82.0075 - val_mse: 82.0075 - val_mae: 6.5017\n",
      "Epoch 502/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4399 - mse: 88.4399 - mae: 6.5670 - val_loss: 81.8748 - val_mse: 81.8747 - val_mae: 6.4883\n",
      "Epoch 503/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.1911 - mse: 88.1911 - mae: 6.5607 - val_loss: 82.8389 - val_mse: 82.8389 - val_mae: 6.5554\n",
      "Epoch 504/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.4598 - mse: 88.4599 - mae: 6.5610 - val_loss: 82.8032 - val_mse: 82.8032 - val_mae: 6.4267\n",
      "Epoch 505/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 88.2406 - mse: 88.2405 - mae: 6.5683 - val_loss: 83.5504 - val_mse: 83.5504 - val_mae: 6.4310\n",
      "Epoch 506/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 88.3502 - mse: 88.3501 - mae: 6.5676 - val_loss: 84.7711 - val_mse: 84.7711 - val_mae: 6.4067\n",
      "Epoch 507/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.3196 - mse: 88.3196 - mae: 6.5613 - val_loss: 81.9451 - val_mse: 81.9451 - val_mae: 6.5213\n",
      "Epoch 508/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 88.2265 - mse: 88.2265 - mae: 6.5673 - val_loss: 82.9920 - val_mse: 82.9920 - val_mae: 6.4458\n",
      "Epoch 509/1000\n",
      "14194/14194 [==============================] - 2s 140us/step - loss: 88.2869 - mse: 88.2869 - mae: 6.5704 - val_loss: 82.0381 - val_mse: 82.0382 - val_mae: 6.4762\n",
      "Epoch 510/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.3002 - mse: 88.3001 - mae: 6.5652 - val_loss: 82.9325 - val_mse: 82.9324 - val_mae: 6.4327\n",
      "Epoch 511/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 88.2158 - mse: 88.2159 - mae: 6.5651 - val_loss: 84.4765 - val_mse: 84.4766 - val_mae: 6.4013\n",
      "Epoch 512/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 88.1805 - mse: 88.1806 - mae: 6.5577 - val_loss: 83.3664 - val_mse: 83.3663 - val_mae: 6.4713\n",
      "Epoch 513/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.3953 - mse: 88.3953 - mae: 6.5569 - val_loss: 82.5799 - val_mse: 82.5799 - val_mae: 6.6439\n",
      "Epoch 514/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.5690 - mse: 88.5690 - mae: 6.5702 - val_loss: 82.3357 - val_mse: 82.3357 - val_mae: 6.5873\n",
      "Epoch 515/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.1665 - mse: 88.1665 - mae: 6.5581 - val_loss: 82.8477 - val_mse: 82.8477 - val_mae: 6.4254\n",
      "Epoch 516/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 88.3269 - mse: 88.3269 - mae: 6.5626 - val_loss: 83.5197 - val_mse: 83.5197 - val_mae: 6.4210\n",
      "Epoch 517/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 88.0987 - mse: 88.0988 - mae: 6.5525 - val_loss: 82.1634 - val_mse: 82.1634 - val_mae: 6.4561\n",
      "Epoch 518/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 88.2234 - mse: 88.2233 - mae: 6.5671 - val_loss: 84.9954 - val_mse: 84.9954 - val_mae: 6.7275\n",
      "Epoch 519/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.1850 - mse: 88.1850 - mae: 6.5593 - val_loss: 82.5844 - val_mse: 82.5844 - val_mae: 6.4410\n",
      "Epoch 520/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 88.1503 - mse: 88.1502 - mae: 6.5522 - val_loss: 83.0601 - val_mse: 83.0601 - val_mae: 6.4404\n",
      "Epoch 521/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.3675 - mse: 88.3675 - mae: 6.5648 - val_loss: 82.1054 - val_mse: 82.1054 - val_mae: 6.5401\n",
      "Epoch 522/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 88.2721 - mse: 88.2721 - mae: 6.5748 - val_loss: 83.5790 - val_mse: 83.5789 - val_mae: 6.4531\n",
      "Epoch 523/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.2309 - mse: 88.2309 - mae: 6.5598 - val_loss: 83.0766 - val_mse: 83.0766 - val_mae: 6.4594\n",
      "Epoch 524/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.2406 - mse: 88.2406 - mae: 6.5637 - val_loss: 82.5532 - val_mse: 82.5533 - val_mae: 6.4041\n",
      "Epoch 525/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.3226 - mse: 88.3227 - mae: 6.5673 - val_loss: 81.8794 - val_mse: 81.8795 - val_mae: 6.4636\n",
      "Epoch 526/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 88.1265 - mse: 88.1265 - mae: 6.5515 - val_loss: 82.1063 - val_mse: 82.1063 - val_mae: 6.4576\n",
      "Epoch 527/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.2582 - mse: 88.2583 - mae: 6.5585 - val_loss: 82.3188 - val_mse: 82.3188 - val_mae: 6.4190\n",
      "Epoch 528/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.2629 - mse: 88.2629 - mae: 6.5527 - val_loss: 83.0426 - val_mse: 83.0427 - val_mae: 6.4015\n",
      "Epoch 529/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.2418 - mse: 88.2418 - mae: 6.5430 - val_loss: 82.0541 - val_mse: 82.0541 - val_mae: 6.5137\n",
      "Epoch 530/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.5282 - mse: 88.5282 - mae: 6.5643 - val_loss: 82.0368 - val_mse: 82.0368 - val_mae: 6.5165\n",
      "Epoch 531/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.2310 - mse: 88.2311 - mae: 6.5650 - val_loss: 83.6321 - val_mse: 83.6321 - val_mae: 6.4124\n",
      "Epoch 532/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.3762 - mse: 88.3762 - mae: 6.5593 - val_loss: 82.8597 - val_mse: 82.8597 - val_mae: 6.6737\n",
      "Epoch 533/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.1350 - mse: 88.1350 - mae: 6.5612 - val_loss: 82.5738 - val_mse: 82.5738 - val_mae: 6.4081\n",
      "Epoch 534/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.1119 - mse: 88.1119 - mae: 6.5477 - val_loss: 85.0922 - val_mse: 85.0922 - val_mae: 6.8113\n",
      "Epoch 535/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 88.1818 - mse: 88.1817 - mae: 6.5627 - val_loss: 84.1212 - val_mse: 84.1212 - val_mae: 6.8162\n",
      "Epoch 536/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 88.3117 - mse: 88.3117 - mae: 6.5668 - val_loss: 82.1004 - val_mse: 82.1004 - val_mae: 6.4583\n",
      "Epoch 537/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 88.2257 - mse: 88.2257 - mae: 6.5534 - val_loss: 83.7647 - val_mse: 83.7647 - val_mae: 6.7290\n",
      "Epoch 538/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 88.1169 - mse: 88.1169 - mae: 6.5550 - val_loss: 82.6553 - val_mse: 82.6553 - val_mae: 6.3995\n",
      "Epoch 539/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 88.2712 - mse: 88.2711 - mae: 6.5544 - val_loss: 82.1103 - val_mse: 82.1103 - val_mae: 6.5415\n",
      "Epoch 540/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.1912 - mse: 88.1912 - mae: 6.5578 - val_loss: 82.1410 - val_mse: 82.1410 - val_mae: 6.4525\n",
      "Epoch 541/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 88.1962 - mse: 88.1962 - mae: 6.5539 - val_loss: 85.5890 - val_mse: 85.5890 - val_mae: 6.9302\n",
      "Epoch 542/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.4038 - mse: 88.4037 - mae: 6.5704 - val_loss: 83.1132 - val_mse: 83.1132 - val_mae: 6.6249\n",
      "Epoch 543/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.3554 - mse: 88.3554 - mae: 6.5554 - val_loss: 82.7663 - val_mse: 82.7662 - val_mae: 6.4013\n",
      "Epoch 544/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.1105 - mse: 88.1105 - mae: 6.5509 - val_loss: 83.9227 - val_mse: 83.9228 - val_mae: 6.4028\n",
      "Epoch 545/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.9704 - mse: 87.9704 - mae: 6.5452 - val_loss: 84.0556 - val_mse: 84.0556 - val_mae: 6.7933\n",
      "Epoch 546/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.5212 - mse: 88.5211 - mae: 6.5660 - val_loss: 81.9062 - val_mse: 81.9062 - val_mae: 6.5123\n",
      "Epoch 547/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.2014 - mse: 88.2013 - mae: 6.5573 - val_loss: 82.6870 - val_mse: 82.6870 - val_mae: 6.4092\n",
      "Epoch 548/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.1261 - mse: 88.1261 - mae: 6.5588 - val_loss: 81.9593 - val_mse: 81.9592 - val_mae: 6.5003\n",
      "Epoch 549/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 88.4316 - mse: 88.4316 - mae: 6.5639 - val_loss: 82.2376 - val_mse: 82.2376 - val_mae: 6.5825\n",
      "Epoch 550/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 88.3147 - mse: 88.3147 - mae: 6.5610 - val_loss: 81.9877 - val_mse: 81.9877 - val_mae: 6.4392\n",
      "Epoch 551/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.0911 - mse: 88.0910 - mae: 6.5445 - val_loss: 82.5229 - val_mse: 82.5229 - val_mae: 6.6202\n",
      "Epoch 552/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.0413 - mse: 88.0413 - mae: 6.5605 - val_loss: 82.5029 - val_mse: 82.5029 - val_mae: 6.4172\n",
      "Epoch 553/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.2377 - mse: 88.2377 - mae: 6.5645 - val_loss: 82.1976 - val_mse: 82.1976 - val_mae: 6.5817\n",
      "Epoch 554/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.2634 - mse: 88.2634 - mae: 6.5636 - val_loss: 83.2501 - val_mse: 83.2501 - val_mae: 6.5923\n",
      "Epoch 555/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.0389 - mse: 88.0388 - mae: 6.5535 - val_loss: 82.0320 - val_mse: 82.0320 - val_mae: 6.4098\n",
      "Epoch 556/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 88.0520 - mse: 88.0520 - mae: 6.5447 - val_loss: 82.6502 - val_mse: 82.6502 - val_mae: 6.5238\n",
      "Epoch 557/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.0809 - mse: 88.0809 - mae: 6.5424 - val_loss: 83.1444 - val_mse: 83.1444 - val_mae: 6.4857\n",
      "Epoch 558/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.3277 - mse: 88.3277 - mae: 6.5518 - val_loss: 84.1148 - val_mse: 84.1148 - val_mae: 6.3958\n",
      "Epoch 559/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.0252 - mse: 88.0252 - mae: 6.5471 - val_loss: 82.2025 - val_mse: 82.2025 - val_mae: 6.5529\n",
      "Epoch 560/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.6627 - mse: 88.6628 - mae: 6.5696 - val_loss: 83.1501 - val_mse: 83.1501 - val_mae: 6.4035\n",
      "Epoch 561/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1832 - mse: 88.1832 - mae: 6.5521 - val_loss: 82.2467 - val_mse: 82.2467 - val_mae: 6.4914\n",
      "Epoch 562/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.0680 - mse: 88.0679 - mae: 6.5524 - val_loss: 83.2137 - val_mse: 83.2137 - val_mae: 6.4560\n",
      "Epoch 563/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.0110 - mse: 88.0110 - mae: 6.5526 - val_loss: 82.2945 - val_mse: 82.2944 - val_mae: 6.4791\n",
      "Epoch 564/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 88.0981 - mse: 88.0981 - mae: 6.5608 - val_loss: 81.8430 - val_mse: 81.8430 - val_mae: 6.4683\n",
      "Epoch 565/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.3371 - mse: 88.3371 - mae: 6.5467 - val_loss: 82.8226 - val_mse: 82.8226 - val_mae: 6.5670\n",
      "Epoch 566/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.1652 - mse: 88.1653 - mae: 6.5542 - val_loss: 82.0432 - val_mse: 82.0432 - val_mae: 6.5638\n",
      "Epoch 567/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 88.1122 - mse: 88.1122 - mae: 6.5529 - val_loss: 82.1311 - val_mse: 82.1311 - val_mae: 6.4174\n",
      "Epoch 568/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.0658 - mse: 88.0658 - mae: 6.5451 - val_loss: 83.3888 - val_mse: 83.3888 - val_mae: 6.4126\n",
      "Epoch 569/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.1850 - mse: 88.1849 - mae: 6.5582 - val_loss: 81.9033 - val_mse: 81.9033 - val_mae: 6.5239\n",
      "Epoch 570/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.4380 - mse: 88.4381 - mae: 6.5598 - val_loss: 86.0404 - val_mse: 86.0404 - val_mae: 6.4395\n",
      "Epoch 571/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 88.3404 - mse: 88.3404 - mae: 6.5575 - val_loss: 83.1869 - val_mse: 83.1869 - val_mae: 6.6750\n",
      "Epoch 572/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1164 - mse: 88.1163 - mae: 6.5526 - val_loss: 82.5912 - val_mse: 82.5912 - val_mae: 6.5524\n",
      "Epoch 573/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.2329 - mse: 88.2328 - mae: 6.5588 - val_loss: 82.7966 - val_mse: 82.7966 - val_mae: 6.6080\n",
      "Epoch 574/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.0742 - mse: 88.0742 - mae: 6.5524 - val_loss: 82.0757 - val_mse: 82.0756 - val_mae: 6.5153\n",
      "Epoch 575/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 88.0831 - mse: 88.0831 - mae: 6.5526 - val_loss: 82.4281 - val_mse: 82.4281 - val_mae: 6.6310\n",
      "Epoch 576/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.9151 - mse: 87.9151 - mae: 6.5548 - val_loss: 83.2184 - val_mse: 83.2184 - val_mae: 6.3948\n",
      "Epoch 577/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.3171 - mse: 88.3170 - mae: 6.5537 - val_loss: 83.4602 - val_mse: 83.4602 - val_mae: 6.4257\n",
      "Epoch 578/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 87.9079 - mse: 87.9080 - mae: 6.5498 - val_loss: 82.0662 - val_mse: 82.0663 - val_mae: 6.4133\n",
      "Epoch 579/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.3101 - mse: 88.3101 - mae: 6.5530 - val_loss: 81.7669 - val_mse: 81.7669 - val_mae: 6.4636\n",
      "Epoch 580/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.9018 - mse: 87.9017 - mae: 6.5430 - val_loss: 82.2178 - val_mse: 82.2178 - val_mae: 6.4515\n",
      "Epoch 581/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1474 - mse: 88.1473 - mae: 6.5521 - val_loss: 81.9435 - val_mse: 81.9435 - val_mae: 6.4595\n",
      "Epoch 582/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 88.1295 - mse: 88.1295 - mae: 6.5593 - val_loss: 82.7382 - val_mse: 82.7382 - val_mae: 6.4224\n",
      "Epoch 583/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.9688 - mse: 87.9688 - mae: 6.5469 - val_loss: 83.8721 - val_mse: 83.8721 - val_mae: 6.6957\n",
      "Epoch 584/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.9728 - mse: 87.9728 - mae: 6.5450 - val_loss: 82.1820 - val_mse: 82.1820 - val_mae: 6.4232\n",
      "Epoch 585/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 88.0096 - mse: 88.0096 - mae: 6.5416 - val_loss: 84.7404 - val_mse: 84.7404 - val_mae: 6.8121\n",
      "Epoch 586/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.1486 - mse: 88.1486 - mae: 6.5517 - val_loss: 82.3707 - val_mse: 82.3707 - val_mae: 6.4179\n",
      "Epoch 587/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 88.2016 - mse: 88.2016 - mae: 6.5504 - val_loss: 82.3570 - val_mse: 82.3570 - val_mae: 6.4169\n",
      "Epoch 588/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.9604 - mse: 87.9604 - mae: 6.5448 - val_loss: 84.9156 - val_mse: 84.9156 - val_mae: 6.8042\n",
      "Epoch 589/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.0611 - mse: 88.0611 - mae: 6.5406 - val_loss: 82.4384 - val_mse: 82.4385 - val_mae: 6.4208\n",
      "Epoch 590/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.9786 - mse: 87.9786 - mae: 6.5466 - val_loss: 83.6009 - val_mse: 83.6009 - val_mae: 6.3852\n",
      "Epoch 591/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 88.1839 - mse: 88.1839 - mae: 6.5438 - val_loss: 81.9356 - val_mse: 81.9356 - val_mae: 6.4675\n",
      "Epoch 592/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.9667 - mse: 87.9667 - mae: 6.5385 - val_loss: 82.6557 - val_mse: 82.6557 - val_mae: 6.4097\n",
      "Epoch 593/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.2003 - mse: 88.2004 - mae: 6.5502 - val_loss: 82.8906 - val_mse: 82.8906 - val_mae: 6.4273\n",
      "Epoch 594/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 88.0875 - mse: 88.0874 - mae: 6.5582 - val_loss: 81.9489 - val_mse: 81.9490 - val_mae: 6.4606\n",
      "Epoch 595/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 88.3033 - mse: 88.3033 - mae: 6.5467 - val_loss: 82.2721 - val_mse: 82.2722 - val_mae: 6.5923\n",
      "Epoch 596/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.0970 - mse: 88.0971 - mae: 6.5369 - val_loss: 82.2371 - val_mse: 82.2371 - val_mae: 6.4402\n",
      "Epoch 597/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.2413 - mse: 88.2414 - mae: 6.5546 - val_loss: 81.5872 - val_mse: 81.5872 - val_mae: 6.4625\n",
      "Epoch 598/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 88.2200 - mse: 88.2199 - mae: 6.5454 - val_loss: 82.1069 - val_mse: 82.1069 - val_mae: 6.5475\n",
      "Epoch 599/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.9732 - mse: 87.9732 - mae: 6.5277 - val_loss: 83.1749 - val_mse: 83.1749 - val_mae: 6.6496\n",
      "Epoch 600/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.8588 - mse: 87.8588 - mae: 6.5359 - val_loss: 84.7689 - val_mse: 84.7689 - val_mae: 6.4648\n",
      "Epoch 601/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 88.0236 - mse: 88.0235 - mae: 6.5364 - val_loss: 82.0233 - val_mse: 82.0233 - val_mae: 6.4630\n",
      "Epoch 602/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 88.0222 - mse: 88.0222 - mae: 6.5354 - val_loss: 81.8541 - val_mse: 81.8542 - val_mae: 6.4932\n",
      "Epoch 603/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 88.0600 - mse: 88.0600 - mae: 6.5443 - val_loss: 82.2058 - val_mse: 82.2058 - val_mae: 6.4031\n",
      "Epoch 604/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 88.1875 - mse: 88.1875 - mae: 6.5514 - val_loss: 83.6585 - val_mse: 83.6585 - val_mae: 6.4253\n",
      "Epoch 605/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.1780 - mse: 88.1780 - mae: 6.5418 - val_loss: 82.2627 - val_mse: 82.2626 - val_mae: 6.4476\n",
      "Epoch 606/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 88.1157 - mse: 88.1157 - mae: 6.5458 - val_loss: 83.1518 - val_mse: 83.1517 - val_mae: 6.3882\n",
      "Epoch 607/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 88.0648 - mse: 88.0648 - mae: 6.5382 - val_loss: 81.9423 - val_mse: 81.9423 - val_mae: 6.5143\n",
      "Epoch 608/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.7001 - mse: 87.7000 - mae: 6.5346 - val_loss: 82.4337 - val_mse: 82.4337 - val_mae: 6.5412\n",
      "Epoch 609/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 88.1685 - mse: 88.1685 - mae: 6.5454 - val_loss: 81.8964 - val_mse: 81.8964 - val_mae: 6.4830\n",
      "Epoch 610/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 88.1356 - mse: 88.1357 - mae: 6.5398 - val_loss: 82.3148 - val_mse: 82.3148 - val_mae: 6.6207\n",
      "Epoch 611/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.0534 - mse: 88.0534 - mae: 6.5428 - val_loss: 82.8871 - val_mse: 82.8870 - val_mae: 6.4602\n",
      "Epoch 612/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 88.1060 - mse: 88.1060 - mae: 6.5380 - val_loss: 82.4979 - val_mse: 82.4979 - val_mae: 6.6169\n",
      "Epoch 613/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 88.1468 - mse: 88.1468 - mae: 6.5416 - val_loss: 82.7785 - val_mse: 82.7785 - val_mae: 6.4078\n",
      "Epoch 614/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 88.1123 - mse: 88.1123 - mae: 6.5430 - val_loss: 82.0434 - val_mse: 82.0434 - val_mae: 6.5333\n",
      "Epoch 615/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.9154 - mse: 87.9154 - mae: 6.5364 - val_loss: 81.9628 - val_mse: 81.9628 - val_mae: 6.5506\n",
      "Epoch 616/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.2831 - mse: 88.2831 - mae: 6.5608 - val_loss: 82.0483 - val_mse: 82.0483 - val_mae: 6.4641\n",
      "Epoch 617/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.9351 - mse: 87.9351 - mae: 6.5346 - val_loss: 83.0222 - val_mse: 83.0222 - val_mae: 6.4025\n",
      "Epoch 618/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.0347 - mse: 88.0347 - mae: 6.5399 - val_loss: 82.6050 - val_mse: 82.6050 - val_mae: 6.4095\n",
      "Epoch 619/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.9326 - mse: 87.9326 - mae: 6.5286 - val_loss: 82.3347 - val_mse: 82.3348 - val_mae: 6.4655\n",
      "Epoch 620/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 88.0820 - mse: 88.0820 - mae: 6.5368 - val_loss: 83.2535 - val_mse: 83.2535 - val_mae: 6.3694\n",
      "Epoch 621/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.0509 - mse: 88.0509 - mae: 6.5381 - val_loss: 82.3073 - val_mse: 82.3073 - val_mae: 6.4713\n",
      "Epoch 622/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 88.0183 - mse: 88.0183 - mae: 6.5357 - val_loss: 81.8713 - val_mse: 81.8713 - val_mae: 6.5166\n",
      "Epoch 623/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 88.0850 - mse: 88.0849 - mae: 6.5406 - val_loss: 81.9925 - val_mse: 81.9925 - val_mae: 6.5615\n",
      "Epoch 624/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.9221 - mse: 87.9221 - mae: 6.5401 - val_loss: 83.4954 - val_mse: 83.4954 - val_mae: 6.4159\n",
      "Epoch 625/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.0091 - mse: 88.0090 - mae: 6.5414 - val_loss: 82.4555 - val_mse: 82.4555 - val_mae: 6.6320\n",
      "Epoch 626/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 88.4283 - mse: 88.4282 - mae: 6.5637 - val_loss: 81.7718 - val_mse: 81.7718 - val_mae: 6.4231\n",
      "Epoch 627/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.0457 - mse: 88.0456 - mae: 6.5379 - val_loss: 84.0036 - val_mse: 84.0036 - val_mae: 6.4299\n",
      "Epoch 628/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.9480 - mse: 87.9479 - mae: 6.5401 - val_loss: 82.0605 - val_mse: 82.0605 - val_mae: 6.5221\n",
      "Epoch 629/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 87.9462 - mse: 87.9462 - mae: 6.5422 - val_loss: 82.1460 - val_mse: 82.1461 - val_mae: 6.6124\n",
      "Epoch 630/1000\n",
      "14194/14194 [==============================] - 2s 136us/step - loss: 87.9111 - mse: 87.9111 - mae: 6.5400 - val_loss: 81.5960 - val_mse: 81.5960 - val_mae: 6.4603\n",
      "Epoch 631/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.9471 - mse: 87.9471 - mae: 6.5415 - val_loss: 81.6635 - val_mse: 81.6636 - val_mae: 6.4646\n",
      "Epoch 632/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 88.0130 - mse: 88.0130 - mae: 6.5366 - val_loss: 83.8713 - val_mse: 83.8713 - val_mae: 6.4044\n",
      "Epoch 633/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 88.0515 - mse: 88.0516 - mae: 6.5365 - val_loss: 83.4811 - val_mse: 83.4811 - val_mae: 6.7625\n",
      "Epoch 634/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 88.2142 - mse: 88.2142 - mae: 6.5498 - val_loss: 82.7917 - val_mse: 82.7917 - val_mae: 6.3897\n",
      "Epoch 635/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 88.0769 - mse: 88.0769 - mae: 6.5421 - val_loss: 82.1323 - val_mse: 82.1323 - val_mae: 6.4512\n",
      "Epoch 636/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.9046 - mse: 87.9045 - mae: 6.5375 - val_loss: 82.9301 - val_mse: 82.9301 - val_mae: 6.4107\n",
      "Epoch 637/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 88.1481 - mse: 88.1481 - mae: 6.5410 - val_loss: 83.0018 - val_mse: 83.0018 - val_mae: 6.4211\n",
      "Epoch 638/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.8452 - mse: 87.8451 - mae: 6.5375 - val_loss: 83.9657 - val_mse: 83.9657 - val_mae: 6.7594\n",
      "Epoch 639/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.9549 - mse: 87.9550 - mae: 6.5447 - val_loss: 81.9526 - val_mse: 81.9526 - val_mae: 6.5379\n",
      "Epoch 640/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 88.0285 - mse: 88.0284 - mae: 6.5521 - val_loss: 83.2646 - val_mse: 83.2646 - val_mae: 6.4052\n",
      "Epoch 641/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.9230 - mse: 87.9231 - mae: 6.5403 - val_loss: 82.0228 - val_mse: 82.0228 - val_mae: 6.4692\n",
      "Epoch 642/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.8994 - mse: 87.8994 - mae: 6.5407 - val_loss: 83.8991 - val_mse: 83.8991 - val_mae: 6.4217\n",
      "Epoch 643/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 88.0596 - mse: 88.0596 - mae: 6.5434 - val_loss: 84.1611 - val_mse: 84.1611 - val_mae: 6.3703\n",
      "Epoch 644/1000\n",
      "14194/14194 [==============================] - 2s 133us/step - loss: 87.8078 - mse: 87.8078 - mae: 6.5215 - val_loss: 82.8540 - val_mse: 82.8540 - val_mae: 6.3974\n",
      "Epoch 645/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.8584 - mse: 87.8583 - mae: 6.5400 - val_loss: 82.3277 - val_mse: 82.3277 - val_mae: 6.4355\n",
      "Epoch 646/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.9916 - mse: 87.9915 - mae: 6.5414 - val_loss: 82.6963 - val_mse: 82.6962 - val_mae: 6.6255\n",
      "Epoch 647/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 88.0568 - mse: 88.0568 - mae: 6.5488 - val_loss: 83.1250 - val_mse: 83.1250 - val_mae: 6.3741\n",
      "Epoch 648/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 88.0187 - mse: 88.0187 - mae: 6.5327 - val_loss: 81.6841 - val_mse: 81.6841 - val_mae: 6.4944\n",
      "Epoch 649/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.9907 - mse: 87.9908 - mae: 6.5419 - val_loss: 81.7956 - val_mse: 81.7957 - val_mae: 6.4846\n",
      "Epoch 650/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.7191 - mse: 87.7191 - mae: 6.5350 - val_loss: 85.2998 - val_mse: 85.2998 - val_mae: 6.4559\n",
      "Epoch 651/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 88.1162 - mse: 88.1163 - mae: 6.5334 - val_loss: 81.7958 - val_mse: 81.7958 - val_mae: 6.5316\n",
      "Epoch 652/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.9996 - mse: 87.9995 - mae: 6.5350 - val_loss: 81.9250 - val_mse: 81.9250 - val_mae: 6.3951\n",
      "Epoch 653/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.7639 - mse: 87.7639 - mae: 6.5325 - val_loss: 82.9492 - val_mse: 82.9492 - val_mae: 6.6017\n",
      "Epoch 654/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.9247 - mse: 87.9246 - mae: 6.5502 - val_loss: 84.4401 - val_mse: 84.4401 - val_mae: 6.7162\n",
      "Epoch 655/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 88.0295 - mse: 88.0296 - mae: 6.5436 - val_loss: 82.0156 - val_mse: 82.0156 - val_mae: 6.5236\n",
      "Epoch 656/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.8540 - mse: 87.8540 - mae: 6.5352 - val_loss: 82.7285 - val_mse: 82.7285 - val_mae: 6.4447\n",
      "Epoch 657/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.9175 - mse: 87.9174 - mae: 6.5368 - val_loss: 81.9210 - val_mse: 81.9210 - val_mae: 6.4640\n",
      "Epoch 658/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.7464 - mse: 87.7465 - mae: 6.5391 - val_loss: 82.0566 - val_mse: 82.0566 - val_mae: 6.4208\n",
      "Epoch 659/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.8124 - mse: 87.8126 - mae: 6.5279 - val_loss: 81.9219 - val_mse: 81.9219 - val_mae: 6.4112\n",
      "Epoch 660/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.8656 - mse: 87.8656 - mae: 6.5322 - val_loss: 82.0120 - val_mse: 82.0120 - val_mae: 6.4314\n",
      "Epoch 661/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.9778 - mse: 87.9777 - mae: 6.5296 - val_loss: 81.9325 - val_mse: 81.9325 - val_mae: 6.5620\n",
      "Epoch 662/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.7186 - mse: 87.7185 - mae: 6.5397 - val_loss: 83.0652 - val_mse: 83.0652 - val_mae: 6.6485\n",
      "Epoch 663/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 88.0670 - mse: 88.0669 - mae: 6.5380 - val_loss: 82.8358 - val_mse: 82.8358 - val_mae: 6.4204\n",
      "Epoch 664/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.7401 - mse: 87.7400 - mae: 6.5254 - val_loss: 84.7148 - val_mse: 84.7148 - val_mae: 6.7391\n",
      "Epoch 665/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 87.8698 - mse: 87.8697 - mae: 6.5323 - val_loss: 83.7726 - val_mse: 83.7726 - val_mae: 6.7438\n",
      "Epoch 666/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 87.9656 - mse: 87.9656 - mae: 6.5305 - val_loss: 82.6925 - val_mse: 82.6926 - val_mae: 6.4069\n",
      "Epoch 667/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.8122 - mse: 87.8122 - mae: 6.5278 - val_loss: 82.4865 - val_mse: 82.4865 - val_mae: 6.4495\n",
      "Epoch 668/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.8657 - mse: 87.8656 - mae: 6.5386 - val_loss: 81.7152 - val_mse: 81.7152 - val_mae: 6.4890\n",
      "Epoch 669/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.9773 - mse: 87.9773 - mae: 6.5411 - val_loss: 81.7193 - val_mse: 81.7193 - val_mae: 6.5220\n",
      "Epoch 670/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.1346 - mse: 88.1346 - mae: 6.5456 - val_loss: 82.0057 - val_mse: 82.0058 - val_mae: 6.4151\n",
      "Epoch 671/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.6693 - mse: 87.6693 - mae: 6.5391 - val_loss: 82.6170 - val_mse: 82.6170 - val_mae: 6.4314\n",
      "Epoch 672/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.9016 - mse: 87.9016 - mae: 6.5327 - val_loss: 82.7730 - val_mse: 82.7730 - val_mae: 6.4409\n",
      "Epoch 673/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.8008 - mse: 87.8008 - mae: 6.5425 - val_loss: 82.2301 - val_mse: 82.2301 - val_mae: 6.4148\n",
      "Epoch 674/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 88.0728 - mse: 88.0729 - mae: 6.5290 - val_loss: 81.6832 - val_mse: 81.6831 - val_mae: 6.5159\n",
      "Epoch 675/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.6466 - mse: 87.6467 - mae: 6.5253 - val_loss: 82.8518 - val_mse: 82.8518 - val_mae: 6.3928\n",
      "Epoch 676/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 88.0384 - mse: 88.0384 - mae: 6.5376 - val_loss: 82.2913 - val_mse: 82.2913 - val_mae: 6.5542\n",
      "Epoch 677/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.8754 - mse: 87.8754 - mae: 6.5341 - val_loss: 82.0740 - val_mse: 82.0741 - val_mae: 6.3952\n",
      "Epoch 678/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.7292 - mse: 87.7292 - mae: 6.5314 - val_loss: 82.5992 - val_mse: 82.5992 - val_mae: 6.3837\n",
      "Epoch 679/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 88.0276 - mse: 88.0276 - mae: 6.5379 - val_loss: 82.5274 - val_mse: 82.5274 - val_mae: 6.3857\n",
      "Epoch 680/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.7398 - mse: 87.7397 - mae: 6.5255 - val_loss: 81.6716 - val_mse: 81.6716 - val_mae: 6.4348\n",
      "Epoch 681/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.8730 - mse: 87.8731 - mae: 6.5299 - val_loss: 81.5958 - val_mse: 81.5958 - val_mae: 6.4834\n",
      "Epoch 682/1000\n",
      "14194/14194 [==============================] - ETA: 0s - loss: 87.8095 - mse: 87.8094 - mae: 6.532 - 2s 108us/step - loss: 87.7647 - mse: 87.7647 - mae: 6.5319 - val_loss: 83.0236 - val_mse: 83.0236 - val_mae: 6.4127\n",
      "Epoch 683/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.9138 - mse: 87.9138 - mae: 6.5502 - val_loss: 81.6207 - val_mse: 81.6207 - val_mae: 6.4469\n",
      "Epoch 684/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.8357 - mse: 87.8357 - mae: 6.5273 - val_loss: 82.5983 - val_mse: 82.5983 - val_mae: 6.4337\n",
      "Epoch 685/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.8270 - mse: 87.8269 - mae: 6.5345 - val_loss: 81.9278 - val_mse: 81.9278 - val_mae: 6.5044\n",
      "Epoch 686/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 87.7518 - mse: 87.7518 - mae: 6.5316 - val_loss: 81.6833 - val_mse: 81.6833 - val_mae: 6.4058\n",
      "Epoch 687/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.6997 - mse: 87.6998 - mae: 6.5299 - val_loss: 82.5047 - val_mse: 82.5047 - val_mae: 6.3915\n",
      "Epoch 688/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 88.2120 - mse: 88.2120 - mae: 6.5438 - val_loss: 82.8772 - val_mse: 82.8772 - val_mae: 6.3805\n",
      "Epoch 689/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.8445 - mse: 87.8445 - mae: 6.5284 - val_loss: 81.9918 - val_mse: 81.9918 - val_mae: 6.5456\n",
      "Epoch 690/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.6906 - mse: 87.6906 - mae: 6.5332 - val_loss: 82.5799 - val_mse: 82.5799 - val_mae: 6.4401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 691/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.8474 - mse: 87.8474 - mae: 6.5281 - val_loss: 82.0374 - val_mse: 82.0373 - val_mae: 6.4970\n",
      "Epoch 692/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.7889 - mse: 87.7889 - mae: 6.5302 - val_loss: 81.9126 - val_mse: 81.9126 - val_mae: 6.4976\n",
      "Epoch 693/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.7057 - mse: 87.7058 - mae: 6.5218 - val_loss: 82.7868 - val_mse: 82.7868 - val_mae: 6.6132\n",
      "Epoch 694/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.8459 - mse: 87.8458 - mae: 6.5390 - val_loss: 81.4417 - val_mse: 81.4416 - val_mae: 6.4194\n",
      "Epoch 695/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.9643 - mse: 87.9643 - mae: 6.5336 - val_loss: 81.7904 - val_mse: 81.7904 - val_mae: 6.4403\n",
      "Epoch 696/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 88.0024 - mse: 88.0024 - mae: 6.5349 - val_loss: 82.8450 - val_mse: 82.8450 - val_mae: 6.4250\n",
      "Epoch 697/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.9351 - mse: 87.9351 - mae: 6.5378 - val_loss: 81.7608 - val_mse: 81.7608 - val_mae: 6.4794\n",
      "Epoch 698/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.9673 - mse: 87.9673 - mae: 6.5400 - val_loss: 81.8081 - val_mse: 81.8080 - val_mae: 6.4501\n",
      "Epoch 699/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.7655 - mse: 87.7655 - mae: 6.5371 - val_loss: 81.9653 - val_mse: 81.9653 - val_mae: 6.4703\n",
      "Epoch 700/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.9718 - mse: 87.9718 - mae: 6.5439 - val_loss: 82.6118 - val_mse: 82.6118 - val_mae: 6.3888\n",
      "Epoch 701/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 87.9752 - mse: 87.9752 - mae: 6.5322 - val_loss: 83.6071 - val_mse: 83.6071 - val_mae: 6.3763\n",
      "Epoch 702/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.8393 - mse: 87.8392 - mae: 6.5309 - val_loss: 81.7955 - val_mse: 81.7955 - val_mae: 6.4963\n",
      "Epoch 703/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.7984 - mse: 87.7985 - mae: 6.5362 - val_loss: 82.2634 - val_mse: 82.2634 - val_mae: 6.4428\n",
      "Epoch 704/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.8468 - mse: 87.8468 - mae: 6.5331 - val_loss: 81.9433 - val_mse: 81.9433 - val_mae: 6.5919\n",
      "Epoch 705/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 87.8249 - mse: 87.8248 - mae: 6.5352 - val_loss: 81.8604 - val_mse: 81.8604 - val_mae: 6.4380\n",
      "Epoch 706/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 87.8437 - mse: 87.8437 - mae: 6.5279 - val_loss: 82.0991 - val_mse: 82.0991 - val_mae: 6.4119\n",
      "Epoch 707/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.8714 - mse: 87.8715 - mae: 6.5355 - val_loss: 82.3404 - val_mse: 82.3404 - val_mae: 6.4864\n",
      "Epoch 708/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.7400 - mse: 87.7399 - mae: 6.5315 - val_loss: 82.2111 - val_mse: 82.2111 - val_mae: 6.6197\n",
      "Epoch 709/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.7911 - mse: 87.7912 - mae: 6.5227 - val_loss: 81.9006 - val_mse: 81.9006 - val_mae: 6.5375\n",
      "Epoch 710/1000\n",
      "14194/14194 [==============================] - 1s 106us/step - loss: 88.0766 - mse: 88.0766 - mae: 6.5430 - val_loss: 81.6798 - val_mse: 81.6798 - val_mae: 6.4537\n",
      "Epoch 711/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.7172 - mse: 87.7172 - mae: 6.5430 - val_loss: 81.7816 - val_mse: 81.7816 - val_mae: 6.5698\n",
      "Epoch 712/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.9167 - mse: 87.9166 - mae: 6.5357 - val_loss: 81.9953 - val_mse: 81.9953 - val_mae: 6.4368\n",
      "Epoch 713/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.8153 - mse: 87.8153 - mae: 6.5288 - val_loss: 81.8821 - val_mse: 81.8821 - val_mae: 6.4571\n",
      "Epoch 714/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.8842 - mse: 87.8842 - mae: 6.5290 - val_loss: 81.6873 - val_mse: 81.6873 - val_mae: 6.4856\n",
      "Epoch 715/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.7756 - mse: 87.7757 - mae: 6.5332 - val_loss: 82.5868 - val_mse: 82.5868 - val_mae: 6.5359\n",
      "Epoch 716/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.6650 - mse: 87.6650 - mae: 6.5245 - val_loss: 82.5352 - val_mse: 82.5352 - val_mae: 6.6583\n",
      "Epoch 717/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.8034 - mse: 87.8035 - mae: 6.5276 - val_loss: 81.9527 - val_mse: 81.9527 - val_mae: 6.4798\n",
      "Epoch 718/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.8098 - mse: 87.8098 - mae: 6.5347 - val_loss: 82.2415 - val_mse: 82.2415 - val_mae: 6.4502\n",
      "Epoch 719/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 87.7430 - mse: 87.7431 - mae: 6.5317 - val_loss: 82.3131 - val_mse: 82.3131 - val_mae: 6.5189\n",
      "Epoch 720/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.6215 - mse: 87.6216 - mae: 6.5356 - val_loss: 82.0641 - val_mse: 82.0641 - val_mae: 6.5534\n",
      "Epoch 721/1000\n",
      "14194/14194 [==============================] - 2s 138us/step - loss: 87.6676 - mse: 87.6676 - mae: 6.5257 - val_loss: 82.2875 - val_mse: 82.2875 - val_mae: 6.5553\n",
      "Epoch 722/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 88.0489 - mse: 88.0488 - mae: 6.5403 - val_loss: 82.4161 - val_mse: 82.4161 - val_mae: 6.3879\n",
      "Epoch 723/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.7605 - mse: 87.7604 - mae: 6.5392 - val_loss: 81.9739 - val_mse: 81.9738 - val_mae: 6.5497\n",
      "Epoch 724/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 87.9196 - mse: 87.9196 - mae: 6.5421 - val_loss: 82.1045 - val_mse: 82.1046 - val_mae: 6.4020\n",
      "Epoch 725/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 87.8844 - mse: 87.8845 - mae: 6.5307 - val_loss: 82.4769 - val_mse: 82.4768 - val_mae: 6.4078\n",
      "Epoch 726/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.9063 - mse: 87.9064 - mae: 6.5294 - val_loss: 82.1030 - val_mse: 82.1030 - val_mae: 6.3936\n",
      "Epoch 727/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.8420 - mse: 87.8420 - mae: 6.5337 - val_loss: 82.1192 - val_mse: 82.1192 - val_mae: 6.4114\n",
      "Epoch 728/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 87.7962 - mse: 87.7962 - mae: 6.5330 - val_loss: 82.5548 - val_mse: 82.5549 - val_mae: 6.3967\n",
      "Epoch 729/1000\n",
      "14194/14194 [==============================] - 2s 132us/step - loss: 87.6135 - mse: 87.6134 - mae: 6.5285 - val_loss: 82.2121 - val_mse: 82.2121 - val_mae: 6.4701\n",
      "Epoch 730/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.5569 - mse: 87.5568 - mae: 6.5267 - val_loss: 82.3606 - val_mse: 82.3606 - val_mae: 6.4394\n",
      "Epoch 731/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.7829 - mse: 87.7829 - mae: 6.5354 - val_loss: 81.7966 - val_mse: 81.7966 - val_mae: 6.5425\n",
      "Epoch 732/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.7842 - mse: 87.7843 - mae: 6.5202 - val_loss: 82.2290 - val_mse: 82.2291 - val_mae: 6.3824\n",
      "Epoch 733/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 87.7826 - mse: 87.7826 - mae: 6.5271 - val_loss: 81.8010 - val_mse: 81.8010 - val_mae: 6.4292\n",
      "Epoch 734/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 87.8100 - mse: 87.8100 - mae: 6.5315 - val_loss: 82.0406 - val_mse: 82.0406 - val_mae: 6.4076\n",
      "Epoch 735/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.6950 - mse: 87.6950 - mae: 6.5412 - val_loss: 82.6770 - val_mse: 82.6770 - val_mae: 6.4050\n",
      "Epoch 736/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 87.8244 - mse: 87.8245 - mae: 6.5307 - val_loss: 83.6610 - val_mse: 83.6609 - val_mae: 6.7369\n",
      "Epoch 737/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 126us/step - loss: 87.8103 - mse: 87.8103 - mae: 6.5341 - val_loss: 81.9207 - val_mse: 81.9208 - val_mae: 6.4444\n",
      "Epoch 738/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.6670 - mse: 87.6670 - mae: 6.5294 - val_loss: 82.4751 - val_mse: 82.4752 - val_mae: 6.3904\n",
      "Epoch 739/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.7075 - mse: 87.7075 - mae: 6.5303 - val_loss: 81.7125 - val_mse: 81.7125 - val_mae: 6.4491\n",
      "Epoch 740/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.7457 - mse: 87.7456 - mae: 6.5185 - val_loss: 81.9663 - val_mse: 81.9663 - val_mae: 6.4382\n",
      "Epoch 741/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.7320 - mse: 87.7320 - mae: 6.5162 - val_loss: 81.8355 - val_mse: 81.8356 - val_mae: 6.4754\n",
      "Epoch 742/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 87.5658 - mse: 87.5657 - mae: 6.5244 - val_loss: 82.3326 - val_mse: 82.3326 - val_mae: 6.6406\n",
      "Epoch 743/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.7281 - mse: 87.7281 - mae: 6.5278 - val_loss: 81.7662 - val_mse: 81.7662 - val_mae: 6.4509\n",
      "Epoch 744/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 87.8408 - mse: 87.8407 - mae: 6.5240 - val_loss: 82.3139 - val_mse: 82.3139 - val_mae: 6.4307\n",
      "Epoch 745/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 87.5915 - mse: 87.5915 - mae: 6.5249 - val_loss: 81.6335 - val_mse: 81.6335 - val_mae: 6.4404\n",
      "Epoch 746/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 87.5748 - mse: 87.5747 - mae: 6.5261 - val_loss: 81.7587 - val_mse: 81.7587 - val_mae: 6.4582\n",
      "Epoch 747/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.7678 - mse: 87.7678 - mae: 6.5351 - val_loss: 82.5459 - val_mse: 82.5459 - val_mae: 6.3812\n",
      "Epoch 748/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.7028 - mse: 87.7028 - mae: 6.5288 - val_loss: 82.9125 - val_mse: 82.9125 - val_mae: 6.4725\n",
      "Epoch 749/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.9372 - mse: 87.9372 - mae: 6.5429 - val_loss: 82.1480 - val_mse: 82.1479 - val_mae: 6.3722\n",
      "Epoch 750/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 87.6617 - mse: 87.6617 - mae: 6.5185 - val_loss: 82.3517 - val_mse: 82.3517 - val_mae: 6.6096\n",
      "Epoch 751/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 87.8942 - mse: 87.8942 - mae: 6.5283 - val_loss: 81.6742 - val_mse: 81.6742 - val_mae: 6.5038\n",
      "Epoch 752/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.6462 - mse: 87.6462 - mae: 6.5287 - val_loss: 82.1635 - val_mse: 82.1636 - val_mae: 6.5129\n",
      "Epoch 753/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 87.8121 - mse: 87.8121 - mae: 6.5279 - val_loss: 82.0451 - val_mse: 82.0451 - val_mae: 6.5741\n",
      "Epoch 754/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 87.7845 - mse: 87.7845 - mae: 6.5204 - val_loss: 81.8508 - val_mse: 81.8508 - val_mae: 6.5394\n",
      "Epoch 755/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.6544 - mse: 87.6544 - mae: 6.5230 - val_loss: 81.6410 - val_mse: 81.6410 - val_mae: 6.4839\n",
      "Epoch 756/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.6703 - mse: 87.6702 - mae: 6.5303 - val_loss: 83.0797 - val_mse: 83.0797 - val_mae: 6.3894\n",
      "Epoch 757/1000\n",
      "14194/14194 [==============================] - 2s 146us/step - loss: 87.6428 - mse: 87.6428 - mae: 6.5174 - val_loss: 82.4132 - val_mse: 82.4132 - val_mae: 6.4379\n",
      "Epoch 758/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 87.7816 - mse: 87.7814 - mae: 6.5323 - val_loss: 81.5036 - val_mse: 81.5036 - val_mae: 6.4830\n",
      "Epoch 759/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.6971 - mse: 87.6971 - mae: 6.5238 - val_loss: 81.7008 - val_mse: 81.7008 - val_mae: 6.4637\n",
      "Epoch 760/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.7676 - mse: 87.7675 - mae: 6.5300 - val_loss: 81.9769 - val_mse: 81.9769 - val_mae: 6.4359\n",
      "Epoch 761/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.7848 - mse: 87.7849 - mae: 6.5277 - val_loss: 81.6314 - val_mse: 81.6314 - val_mae: 6.4672\n",
      "Epoch 762/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.8393 - mse: 87.8392 - mae: 6.5398 - val_loss: 81.8031 - val_mse: 81.8031 - val_mae: 6.4145\n",
      "Epoch 763/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.5963 - mse: 87.5962 - mae: 6.5226 - val_loss: 82.2525 - val_mse: 82.2525 - val_mae: 6.5556\n",
      "Epoch 764/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.6326 - mse: 87.6327 - mae: 6.5151 - val_loss: 81.7817 - val_mse: 81.7817 - val_mae: 6.4802\n",
      "Epoch 765/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 87.7988 - mse: 87.7989 - mae: 6.5311 - val_loss: 82.0336 - val_mse: 82.0335 - val_mae: 6.5871\n",
      "Epoch 766/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 87.6931 - mse: 87.6932 - mae: 6.5297 - val_loss: 82.6401 - val_mse: 82.6401 - val_mae: 6.4118\n",
      "Epoch 767/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 87.6298 - mse: 87.6298 - mae: 6.5195 - val_loss: 82.1257 - val_mse: 82.1257 - val_mae: 6.4640\n",
      "Epoch 768/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.5937 - mse: 87.5937 - mae: 6.5149 - val_loss: 81.8386 - val_mse: 81.8386 - val_mae: 6.4571\n",
      "Epoch 769/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.5722 - mse: 87.5722 - mae: 6.5169 - val_loss: 83.4582 - val_mse: 83.4582 - val_mae: 6.7347\n",
      "Epoch 770/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 87.7588 - mse: 87.7588 - mae: 6.5294 - val_loss: 81.9506 - val_mse: 81.9506 - val_mae: 6.5754\n",
      "Epoch 771/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.7262 - mse: 87.7262 - mae: 6.5251 - val_loss: 81.5265 - val_mse: 81.5265 - val_mae: 6.4712\n",
      "Epoch 772/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.6101 - mse: 87.6101 - mae: 6.5244 - val_loss: 82.7768 - val_mse: 82.7768 - val_mae: 6.3822\n",
      "Epoch 773/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.6137 - mse: 87.6136 - mae: 6.5239 - val_loss: 81.4111 - val_mse: 81.4111 - val_mae: 6.4635\n",
      "Epoch 774/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.6491 - mse: 87.6491 - mae: 6.5241 - val_loss: 83.8152 - val_mse: 83.8153 - val_mae: 6.3911\n",
      "Epoch 775/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 87.9344 - mse: 87.9343 - mae: 6.5232 - val_loss: 81.8015 - val_mse: 81.8015 - val_mae: 6.5174\n",
      "Epoch 776/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.5820 - mse: 87.5820 - mae: 6.5151 - val_loss: 81.5794 - val_mse: 81.5794 - val_mae: 6.4625\n",
      "Epoch 777/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.5722 - mse: 87.5721 - mae: 6.5292 - val_loss: 82.7030 - val_mse: 82.7031 - val_mae: 6.3912\n",
      "Epoch 778/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.7234 - mse: 87.7234 - mae: 6.5236 - val_loss: 81.9423 - val_mse: 81.9422 - val_mae: 6.5387\n",
      "Epoch 779/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.7032 - mse: 87.7032 - mae: 6.5222 - val_loss: 82.5566 - val_mse: 82.5566 - val_mae: 6.5432\n",
      "Epoch 780/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.8044 - mse: 87.8045 - mae: 6.5315 - val_loss: 82.8924 - val_mse: 82.8924 - val_mae: 6.5447\n",
      "Epoch 781/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.6363 - mse: 87.6363 - mae: 6.5252 - val_loss: 82.2508 - val_mse: 82.2508 - val_mae: 6.4106\n",
      "Epoch 782/1000\n",
      "14194/14194 [==============================] - 2s 109us/step - loss: 87.5604 - mse: 87.5603 - mae: 6.5163 - val_loss: 82.8021 - val_mse: 82.8021 - val_mae: 6.6303\n",
      "Epoch 783/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.7279 - mse: 87.7280 - mae: 6.5294 - val_loss: 82.2941 - val_mse: 82.2941 - val_mae: 6.4056\n",
      "Epoch 784/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.6518 - mse: 87.6518 - mae: 6.5232 - val_loss: 81.5974 - val_mse: 81.5974 - val_mae: 6.4439\n",
      "Epoch 785/1000\n",
      "14194/14194 [==============================] - 2s 142us/step - loss: 87.7647 - mse: 87.7647 - mae: 6.5385 - val_loss: 82.6373 - val_mse: 82.6373 - val_mae: 6.3814\n",
      "Epoch 786/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.3546 - mse: 87.3547 - mae: 6.5129 - val_loss: 82.2419 - val_mse: 82.2420 - val_mae: 6.4311\n",
      "Epoch 787/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.7367 - mse: 87.7367 - mae: 6.5249 - val_loss: 82.7496 - val_mse: 82.7496 - val_mae: 6.3974\n",
      "Epoch 788/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.5949 - mse: 87.5950 - mae: 6.5178 - val_loss: 81.7550 - val_mse: 81.7550 - val_mae: 6.4148\n",
      "Epoch 789/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 87.6644 - mse: 87.6644 - mae: 6.5175 - val_loss: 82.0272 - val_mse: 82.0271 - val_mae: 6.4137\n",
      "Epoch 790/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.6094 - mse: 87.6094 - mae: 6.5355 - val_loss: 81.8997 - val_mse: 81.8997 - val_mae: 6.4551\n",
      "Epoch 791/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.4440 - mse: 87.4440 - mae: 6.5143 - val_loss: 81.5523 - val_mse: 81.5523 - val_mae: 6.5122\n",
      "Epoch 792/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.7865 - mse: 87.7865 - mae: 6.5373 - val_loss: 81.9521 - val_mse: 81.9521 - val_mae: 6.4228\n",
      "Epoch 793/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.5186 - mse: 87.5186 - mae: 6.5164 - val_loss: 81.7540 - val_mse: 81.7540 - val_mae: 6.4834\n",
      "Epoch 794/1000\n",
      "14194/14194 [==============================] - 1s 92us/step - loss: 87.4912 - mse: 87.4912 - mae: 6.5181 - val_loss: 82.3274 - val_mse: 82.3274 - val_mae: 6.3910\n",
      "Epoch 795/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.4768 - mse: 87.4769 - mae: 6.5226 - val_loss: 81.8491 - val_mse: 81.8491 - val_mae: 6.4639\n",
      "Epoch 796/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.7774 - mse: 87.7775 - mae: 6.5297 - val_loss: 82.0518 - val_mse: 82.0518 - val_mae: 6.4690\n",
      "Epoch 797/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.5640 - mse: 87.5640 - mae: 6.5245 - val_loss: 81.8429 - val_mse: 81.8428 - val_mae: 6.4807\n",
      "Epoch 798/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.4343 - mse: 87.4343 - mae: 6.5247 - val_loss: 81.6079 - val_mse: 81.6079 - val_mae: 6.4444\n",
      "Epoch 799/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.7398 - mse: 87.7396 - mae: 6.5282 - val_loss: 82.4366 - val_mse: 82.4366 - val_mae: 6.3916\n",
      "Epoch 800/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.5214 - mse: 87.5213 - mae: 6.5111 - val_loss: 81.6099 - val_mse: 81.6099 - val_mae: 6.4460\n",
      "Epoch 801/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.5519 - mse: 87.5518 - mae: 6.5208 - val_loss: 84.0850 - val_mse: 84.0850 - val_mae: 6.3632\n",
      "Epoch 802/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.5390 - mse: 87.5390 - mae: 6.5138 - val_loss: 82.8253 - val_mse: 82.8253 - val_mae: 6.6535\n",
      "Epoch 803/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.5781 - mse: 87.5780 - mae: 6.5256 - val_loss: 81.5478 - val_mse: 81.5478 - val_mae: 6.5196\n",
      "Epoch 804/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.6126 - mse: 87.6125 - mae: 6.5219 - val_loss: 81.8229 - val_mse: 81.8229 - val_mae: 6.5551\n",
      "Epoch 805/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.5254 - mse: 87.5253 - mae: 6.5219 - val_loss: 82.3677 - val_mse: 82.3677 - val_mae: 6.4090\n",
      "Epoch 806/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.4785 - mse: 87.4785 - mae: 6.5188 - val_loss: 82.2037 - val_mse: 82.2038 - val_mae: 6.5030\n",
      "Epoch 807/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.7530 - mse: 87.7530 - mae: 6.5274 - val_loss: 82.2829 - val_mse: 82.2829 - val_mae: 6.4320\n",
      "Epoch 808/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.5884 - mse: 87.5885 - mae: 6.5196 - val_loss: 81.8502 - val_mse: 81.8502 - val_mae: 6.4859\n",
      "Epoch 809/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.6329 - mse: 87.6329 - mae: 6.5182 - val_loss: 83.0787 - val_mse: 83.0788 - val_mae: 6.3832\n",
      "Epoch 810/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.4704 - mse: 87.4705 - mae: 6.5089 - val_loss: 82.3405 - val_mse: 82.3405 - val_mae: 6.5455\n",
      "Epoch 811/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.5502 - mse: 87.5502 - mae: 6.5261 - val_loss: 83.0000 - val_mse: 83.0000 - val_mae: 6.3789\n",
      "Epoch 812/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.6439 - mse: 87.6440 - mae: 6.5175 - val_loss: 81.8495 - val_mse: 81.8495 - val_mae: 6.4285\n",
      "Epoch 813/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.6228 - mse: 87.6228 - mae: 6.5178 - val_loss: 82.5423 - val_mse: 82.5423 - val_mae: 6.4351\n",
      "Epoch 814/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.4976 - mse: 87.4976 - mae: 6.5157 - val_loss: 81.8553 - val_mse: 81.8554 - val_mae: 6.4286\n",
      "Epoch 815/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.4860 - mse: 87.4859 - mae: 6.5181 - val_loss: 81.6413 - val_mse: 81.6413 - val_mae: 6.4805\n",
      "Epoch 816/1000\n",
      "14194/14194 [==============================] - 2s 111us/step - loss: 87.4815 - mse: 87.4816 - mae: 6.5276 - val_loss: 83.6707 - val_mse: 83.6707 - val_mae: 6.3953\n",
      "Epoch 817/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.8637 - mse: 87.8638 - mae: 6.5292 - val_loss: 81.6885 - val_mse: 81.6885 - val_mae: 6.4725\n",
      "Epoch 818/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.5476 - mse: 87.5476 - mae: 6.5223 - val_loss: 82.0635 - val_mse: 82.0635 - val_mae: 6.4538\n",
      "Epoch 819/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.6302 - mse: 87.6303 - mae: 6.5173 - val_loss: 83.1523 - val_mse: 83.1523 - val_mae: 6.3976\n",
      "Epoch 820/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 87.6501 - mse: 87.6501 - mae: 6.5156 - val_loss: 82.1328 - val_mse: 82.1328 - val_mae: 6.5740\n",
      "Epoch 821/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.4615 - mse: 87.4615 - mae: 6.5213 - val_loss: 81.7915 - val_mse: 81.7915 - val_mae: 6.5204\n",
      "Epoch 822/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 87.8223 - mse: 87.8223 - mae: 6.5249 - val_loss: 82.4033 - val_mse: 82.4033 - val_mae: 6.6013\n",
      "Epoch 823/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.4857 - mse: 87.4856 - mae: 6.5249 - val_loss: 82.2975 - val_mse: 82.2975 - val_mae: 6.4192\n",
      "Epoch 824/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 87.6435 - mse: 87.6435 - mae: 6.5227 - val_loss: 81.6852 - val_mse: 81.6852 - val_mae: 6.5358\n",
      "Epoch 825/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.4537 - mse: 87.4537 - mae: 6.5296 - val_loss: 82.3867 - val_mse: 82.3867 - val_mae: 6.4127\n",
      "Epoch 826/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.6106 - mse: 87.6106 - mae: 6.5179 - val_loss: 81.7315 - val_mse: 81.7315 - val_mae: 6.4216\n",
      "Epoch 827/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.4529 - mse: 87.4528 - mae: 6.5166 - val_loss: 81.6301 - val_mse: 81.6301 - val_mae: 6.4606\n",
      "Epoch 828/1000\n",
      "14194/14194 [==============================] - 2s 149us/step - loss: 87.7859 - mse: 87.7859 - mae: 6.5218 - val_loss: 82.7665 - val_mse: 82.7665 - val_mae: 6.3932\n",
      "Epoch 829/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 167us/step - loss: 87.5290 - mse: 87.5290 - mae: 6.5138 - val_loss: 82.1764 - val_mse: 82.1764 - val_mae: 6.5734\n",
      "Epoch 830/1000\n",
      "14194/14194 [==============================] - 3s 189us/step - loss: 87.5778 - mse: 87.5777 - mae: 6.5249 - val_loss: 82.4144 - val_mse: 82.4144 - val_mae: 6.6021\n",
      "Epoch 831/1000\n",
      "14194/14194 [==============================] - 3s 179us/step - loss: 87.4032 - mse: 87.4032 - mae: 6.5226 - val_loss: 81.8998 - val_mse: 81.8998 - val_mae: 6.4184\n",
      "Epoch 832/1000\n",
      "14194/14194 [==============================] - 2s 156us/step - loss: 87.5944 - mse: 87.5943 - mae: 6.5210 - val_loss: 81.8007 - val_mse: 81.8006 - val_mae: 6.4492\n",
      "Epoch 833/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.6343 - mse: 87.6343 - mae: 6.5261 - val_loss: 82.3313 - val_mse: 82.3313 - val_mae: 6.5155\n",
      "Epoch 834/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.5488 - mse: 87.5487 - mae: 6.5265 - val_loss: 82.6387 - val_mse: 82.6388 - val_mae: 6.4082\n",
      "Epoch 835/1000\n",
      "14194/14194 [==============================] - 2s 130us/step - loss: 87.5499 - mse: 87.5498 - mae: 6.5127 - val_loss: 81.9548 - val_mse: 81.9548 - val_mae: 6.4759\n",
      "Epoch 836/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.3064 - mse: 87.3064 - mae: 6.5224 - val_loss: 82.6215 - val_mse: 82.6215 - val_mae: 6.5257\n",
      "Epoch 837/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.9259 - mse: 87.9259 - mae: 6.5250 - val_loss: 82.0614 - val_mse: 82.0614 - val_mae: 6.4600\n",
      "Epoch 838/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.6925 - mse: 87.6925 - mae: 6.5181 - val_loss: 82.4999 - val_mse: 82.4998 - val_mae: 6.6339\n",
      "Epoch 839/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 87.6529 - mse: 87.6528 - mae: 6.5245 - val_loss: 81.6049 - val_mse: 81.6050 - val_mae: 6.4275\n",
      "Epoch 840/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.4998 - mse: 87.4997 - mae: 6.5124 - val_loss: 81.9726 - val_mse: 81.9726 - val_mae: 6.5325\n",
      "Epoch 841/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.5690 - mse: 87.5690 - mae: 6.5198 - val_loss: 81.7276 - val_mse: 81.7276 - val_mae: 6.4402\n",
      "Epoch 842/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 87.6179 - mse: 87.6179 - mae: 6.5151 - val_loss: 81.9062 - val_mse: 81.9062 - val_mae: 6.4374\n",
      "Epoch 843/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.7118 - mse: 87.7118 - mae: 6.5212 - val_loss: 82.0129 - val_mse: 82.0129 - val_mae: 6.3967\n",
      "Epoch 844/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.6810 - mse: 87.6810 - mae: 6.5214 - val_loss: 81.8234 - val_mse: 81.8234 - val_mae: 6.5404\n",
      "Epoch 845/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.3120 - mse: 87.3121 - mae: 6.5216 - val_loss: 82.6799 - val_mse: 82.6799 - val_mae: 6.3762\n",
      "Epoch 846/1000\n",
      "14194/14194 [==============================] - 2s 123us/step - loss: 87.5967 - mse: 87.5967 - mae: 6.5161 - val_loss: 82.6406 - val_mse: 82.6405 - val_mae: 6.4145\n",
      "Epoch 847/1000\n",
      "14194/14194 [==============================] - 2s 126us/step - loss: 87.5939 - mse: 87.5938 - mae: 6.5218 - val_loss: 82.5160 - val_mse: 82.5160 - val_mae: 6.6523\n",
      "Epoch 848/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.5688 - mse: 87.5687 - mae: 6.5170 - val_loss: 82.1950 - val_mse: 82.1950 - val_mae: 6.4310\n",
      "Epoch 849/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.6166 - mse: 87.6166 - mae: 6.5122 - val_loss: 81.9065 - val_mse: 81.9065 - val_mae: 6.3960\n",
      "Epoch 850/1000\n",
      "14194/14194 [==============================] - 2s 122us/step - loss: 87.6139 - mse: 87.6139 - mae: 6.5143 - val_loss: 81.7376 - val_mse: 81.7376 - val_mae: 6.4922\n",
      "Epoch 851/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.5978 - mse: 87.5977 - mae: 6.5216 - val_loss: 82.4516 - val_mse: 82.4515 - val_mae: 6.3800\n",
      "Epoch 852/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.5883 - mse: 87.5883 - mae: 6.5097 - val_loss: 81.5532 - val_mse: 81.5532 - val_mae: 6.5037\n",
      "Epoch 853/1000\n",
      "14194/14194 [==============================] - 2s 129us/step - loss: 87.4809 - mse: 87.4810 - mae: 6.5076 - val_loss: 82.6671 - val_mse: 82.6671 - val_mae: 6.3855\n",
      "Epoch 854/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.6359 - mse: 87.6359 - mae: 6.5153 - val_loss: 81.7488 - val_mse: 81.7489 - val_mae: 6.5267\n",
      "Epoch 855/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.5882 - mse: 87.5882 - mae: 6.5176 - val_loss: 81.7057 - val_mse: 81.7057 - val_mae: 6.4350\n",
      "Epoch 856/1000\n",
      "14194/14194 [==============================] - 2s 118us/step - loss: 87.4574 - mse: 87.4574 - mae: 6.5177 - val_loss: 81.7226 - val_mse: 81.7226 - val_mae: 6.4375\n",
      "Epoch 857/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.4524 - mse: 87.4525 - mae: 6.5158 - val_loss: 82.2050 - val_mse: 82.2050 - val_mae: 6.3901\n",
      "Epoch 858/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 87.5267 - mse: 87.5268 - mae: 6.5071 - val_loss: 82.1466 - val_mse: 82.1466 - val_mae: 6.5087\n",
      "Epoch 859/1000\n",
      "14194/14194 [==============================] - 2s 143us/step - loss: 87.3385 - mse: 87.3386 - mae: 6.5160 - val_loss: 82.8710 - val_mse: 82.8710 - val_mae: 6.4971\n",
      "Epoch 860/1000\n",
      "14194/14194 [==============================] - 2s 144us/step - loss: 87.4565 - mse: 87.4566 - mae: 6.5144 - val_loss: 82.0140 - val_mse: 82.0140 - val_mae: 6.4844\n",
      "Epoch 861/1000\n",
      "14194/14194 [==============================] - 2s 137us/step - loss: 87.5826 - mse: 87.5825 - mae: 6.5145 - val_loss: 83.1003 - val_mse: 83.1003 - val_mae: 6.6489\n",
      "Epoch 862/1000\n",
      "14194/14194 [==============================] - 2s 128us/step - loss: 87.6415 - mse: 87.6414 - mae: 6.5304 - val_loss: 82.7446 - val_mse: 82.7446 - val_mae: 6.3965\n",
      "Epoch 863/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.6502 - mse: 87.6502 - mae: 6.5191 - val_loss: 82.5833 - val_mse: 82.5833 - val_mae: 6.3862\n",
      "Epoch 864/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 87.3754 - mse: 87.3754 - mae: 6.5060 - val_loss: 82.4232 - val_mse: 82.4232 - val_mae: 6.6251\n",
      "Epoch 865/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.6994 - mse: 87.6993 - mae: 6.5247 - val_loss: 81.5032 - val_mse: 81.5031 - val_mae: 6.4392\n",
      "Epoch 866/1000\n",
      "14194/14194 [==============================] - 2s 127us/step - loss: 87.3615 - mse: 87.3615 - mae: 6.5046 - val_loss: 82.5987 - val_mse: 82.5987 - val_mae: 6.4399\n",
      "Epoch 867/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.5634 - mse: 87.5635 - mae: 6.5165 - val_loss: 82.6023 - val_mse: 82.6023 - val_mae: 6.3724\n",
      "Epoch 868/1000\n",
      "14194/14194 [==============================] - 2s 131us/step - loss: 87.5996 - mse: 87.5995 - mae: 6.5198 - val_loss: 81.7647 - val_mse: 81.7648 - val_mae: 6.5313\n",
      "Epoch 869/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.4347 - mse: 87.4347 - mae: 6.5143 - val_loss: 82.5773 - val_mse: 82.5774 - val_mae: 6.4004\n",
      "Epoch 870/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.4653 - mse: 87.4652 - mae: 6.5269 - val_loss: 81.7327 - val_mse: 81.7327 - val_mae: 6.3967\n",
      "Epoch 871/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.5198 - mse: 87.5197 - mae: 6.5158 - val_loss: 82.3340 - val_mse: 82.3340 - val_mae: 6.6147\n",
      "Epoch 872/1000\n",
      "14194/14194 [==============================] - 2s 121us/step - loss: 87.4048 - mse: 87.4048 - mae: 6.5149 - val_loss: 81.7248 - val_mse: 81.7248 - val_mae: 6.4222\n",
      "Epoch 873/1000\n",
      "14194/14194 [==============================] - 2s 120us/step - loss: 87.3136 - mse: 87.3136 - mae: 6.5140 - val_loss: 82.0352 - val_mse: 82.0353 - val_mae: 6.5292\n",
      "Epoch 874/1000\n",
      "14194/14194 [==============================] - 2s 125us/step - loss: 87.4961 - mse: 87.4961 - mae: 6.5248 - val_loss: 81.4535 - val_mse: 81.4535 - val_mae: 6.4142\n",
      "Epoch 875/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 2s 129us/step - loss: 87.6663 - mse: 87.6662 - mae: 6.5141 - val_loss: 82.4225 - val_mse: 82.4225 - val_mae: 6.6172\n",
      "Epoch 876/1000\n",
      "14194/14194 [==============================] - 2s 117us/step - loss: 87.5505 - mse: 87.5505 - mae: 6.5214 - val_loss: 81.5883 - val_mse: 81.5883 - val_mae: 6.5153\n",
      "Epoch 877/1000\n",
      "14194/14194 [==============================] - 2s 115us/step - loss: 87.2586 - mse: 87.2586 - mae: 6.5120 - val_loss: 82.3820 - val_mse: 82.3820 - val_mae: 6.4106\n",
      "Epoch 878/1000\n",
      "14194/14194 [==============================] - 2s 119us/step - loss: 87.3899 - mse: 87.3898 - mae: 6.5167 - val_loss: 81.5374 - val_mse: 81.5374 - val_mae: 6.4692\n",
      "Epoch 879/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.5347 - mse: 87.5347 - mae: 6.5119 - val_loss: 82.3016 - val_mse: 82.3016 - val_mae: 6.4840\n",
      "Epoch 880/1000\n",
      "14194/14194 [==============================] - 2s 116us/step - loss: 87.3564 - mse: 87.3565 - mae: 6.5133 - val_loss: 81.7528 - val_mse: 81.7528 - val_mae: 6.4130\n",
      "Epoch 881/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.6244 - mse: 87.6244 - mae: 6.5138 - val_loss: 81.3895 - val_mse: 81.3895 - val_mae: 6.4225\n",
      "Epoch 882/1000\n",
      "14194/14194 [==============================] - 8s 576us/step - loss: 87.3881 - mse: 87.3881 - mae: 6.5021 - val_loss: 81.8182 - val_mse: 81.8182 - val_mae: 6.4512\n",
      "Epoch 883/1000\n",
      "14194/14194 [==============================] - 6s 426us/step - loss: 87.4116 - mse: 87.4116 - mae: 6.5149 - val_loss: 81.9448 - val_mse: 81.9448 - val_mae: 6.4155\n",
      "Epoch 884/1000\n",
      "14194/14194 [==============================] - 3s 243us/step - loss: 87.3838 - mse: 87.3839 - mae: 6.5093 - val_loss: 81.5949 - val_mse: 81.5949 - val_mae: 6.4135\n",
      "Epoch 885/1000\n",
      "14194/14194 [==============================] - 3s 206us/step - loss: 87.4901 - mse: 87.4903 - mae: 6.5066 - val_loss: 81.7543 - val_mse: 81.7542 - val_mae: 6.5536\n",
      "Epoch 886/1000\n",
      "14194/14194 [==============================] - 4580s 323ms/step - loss: 87.5266 - mse: 87.5266 - mae: 6.5124 - val_loss: 81.5613 - val_mse: 81.5613 - val_mae: 6.5266\n",
      "Epoch 887/1000\n",
      "14194/14194 [==============================] - 3s 235us/step - loss: 87.4170 - mse: 87.4170 - mae: 6.5171 - val_loss: 81.5014 - val_mse: 81.5014 - val_mae: 6.4028\n",
      "Epoch 888/1000\n",
      "14194/14194 [==============================] - 2s 153us/step - loss: 87.3310 - mse: 87.3310 - mae: 6.5091 - val_loss: 81.5169 - val_mse: 81.5169 - val_mae: 6.4798\n",
      "Epoch 889/1000\n",
      "14194/14194 [==============================] - 2s 135us/step - loss: 87.2979 - mse: 87.2980 - mae: 6.5164 - val_loss: 81.6029 - val_mse: 81.6029 - val_mae: 6.5172\n",
      "Epoch 890/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 87.4676 - mse: 87.4676 - mae: 6.5207 - val_loss: 82.5414 - val_mse: 82.5414 - val_mae: 6.3743\n",
      "Epoch 891/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.2505 - mse: 87.2505 - mae: 6.5016 - val_loss: 82.1071 - val_mse: 82.1071 - val_mae: 6.3996\n",
      "Epoch 892/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.5297 - mse: 87.5298 - mae: 6.5174 - val_loss: 81.2352 - val_mse: 81.2352 - val_mae: 6.4403\n",
      "Epoch 893/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.2021 - mse: 87.2022 - mae: 6.5004 - val_loss: 82.3312 - val_mse: 82.3313 - val_mae: 6.3882\n",
      "Epoch 894/1000\n",
      "14194/14194 [==============================] - 1s 91us/step - loss: 87.2053 - mse: 87.2053 - mae: 6.4940 - val_loss: 81.5359 - val_mse: 81.5360 - val_mae: 6.5262\n",
      "Epoch 895/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.3374 - mse: 87.3374 - mae: 6.5055 - val_loss: 82.7722 - val_mse: 82.7722 - val_mae: 6.6436\n",
      "Epoch 896/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.4844 - mse: 87.4844 - mae: 6.5159 - val_loss: 81.7366 - val_mse: 81.7366 - val_mae: 6.5544\n",
      "Epoch 897/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.5552 - mse: 87.5553 - mae: 6.5269 - val_loss: 81.4353 - val_mse: 81.4352 - val_mae: 6.4427\n",
      "Epoch 898/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.4957 - mse: 87.4956 - mae: 6.5177 - val_loss: 81.2898 - val_mse: 81.2898 - val_mae: 6.5108\n",
      "Epoch 899/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.1885 - mse: 87.1885 - mae: 6.5074 - val_loss: 81.7685 - val_mse: 81.7685 - val_mae: 6.4375\n",
      "Epoch 900/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.3375 - mse: 87.3375 - mae: 6.5097 - val_loss: 82.7176 - val_mse: 82.7177 - val_mae: 6.6621\n",
      "Epoch 901/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.4717 - mse: 87.4718 - mae: 6.5148 - val_loss: 81.7373 - val_mse: 81.7373 - val_mae: 6.5412\n",
      "Epoch 902/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.3706 - mse: 87.3706 - mae: 6.5201 - val_loss: 82.5837 - val_mse: 82.5837 - val_mae: 6.4021\n",
      "Epoch 903/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.4196 - mse: 87.4197 - mae: 6.5146 - val_loss: 82.1937 - val_mse: 82.1937 - val_mae: 6.3740\n",
      "Epoch 904/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.2549 - mse: 87.2550 - mae: 6.4962 - val_loss: 81.4219 - val_mse: 81.4219 - val_mae: 6.4504\n",
      "Epoch 905/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.4333 - mse: 87.4332 - mae: 6.5110 - val_loss: 81.3302 - val_mse: 81.3302 - val_mae: 6.4648\n",
      "Epoch 906/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.3822 - mse: 87.3823 - mae: 6.5119 - val_loss: 81.4587 - val_mse: 81.4588 - val_mae: 6.4491\n",
      "Epoch 907/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4726 - mse: 87.4725 - mae: 6.5191 - val_loss: 82.3494 - val_mse: 82.3493 - val_mae: 6.4033\n",
      "Epoch 908/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.4274 - mse: 87.4273 - mae: 6.5169 - val_loss: 81.3954 - val_mse: 81.3953 - val_mae: 6.4401\n",
      "Epoch 909/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.2509 - mse: 87.2510 - mae: 6.4974 - val_loss: 81.3700 - val_mse: 81.3700 - val_mae: 6.4302\n",
      "Epoch 910/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 87.3860 - mse: 87.3859 - mae: 6.5151 - val_loss: 81.4400 - val_mse: 81.4400 - val_mae: 6.4267\n",
      "Epoch 911/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.4845 - mse: 87.4845 - mae: 6.5071 - val_loss: 82.3208 - val_mse: 82.3208 - val_mae: 6.4233\n",
      "Epoch 912/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.5886 - mse: 87.5885 - mae: 6.5102 - val_loss: 81.1779 - val_mse: 81.1779 - val_mae: 6.4832\n",
      "Epoch 913/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4412 - mse: 87.4412 - mae: 6.5131 - val_loss: 81.5323 - val_mse: 81.5322 - val_mae: 6.4680\n",
      "Epoch 914/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.2435 - mse: 87.2434 - mae: 6.4996 - val_loss: 81.6132 - val_mse: 81.6133 - val_mae: 6.4465\n",
      "Epoch 915/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.3512 - mse: 87.3513 - mae: 6.5098 - val_loss: 81.4900 - val_mse: 81.4901 - val_mae: 6.4787\n",
      "Epoch 916/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 87.5835 - mse: 87.5835 - mae: 6.5129 - val_loss: 81.9970 - val_mse: 81.9970 - val_mae: 6.5843\n",
      "Epoch 917/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.2380 - mse: 87.2380 - mae: 6.5038 - val_loss: 82.2818 - val_mse: 82.2818 - val_mae: 6.5482\n",
      "Epoch 918/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.2846 - mse: 87.2847 - mae: 6.5079 - val_loss: 81.8503 - val_mse: 81.8502 - val_mae: 6.5332\n",
      "Epoch 919/1000\n",
      "14194/14194 [==============================] - 2s 124us/step - loss: 87.2840 - mse: 87.2841 - mae: 6.5053 - val_loss: 82.2653 - val_mse: 82.2653 - val_mae: 6.3769\n",
      "Epoch 920/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.3613 - mse: 87.3614 - mae: 6.5105 - val_loss: 81.9135 - val_mse: 81.9135 - val_mae: 6.4070\n",
      "Epoch 921/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.5364 - mse: 87.5363 - mae: 6.5219 - val_loss: 82.0188 - val_mse: 82.0188 - val_mae: 6.5457\n",
      "Epoch 922/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.2221 - mse: 87.2221 - mae: 6.5007 - val_loss: 82.0432 - val_mse: 82.0432 - val_mae: 6.6103\n",
      "Epoch 923/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.2859 - mse: 87.2859 - mae: 6.5136 - val_loss: 81.6715 - val_mse: 81.6715 - val_mae: 6.3742\n",
      "Epoch 924/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.2673 - mse: 87.2674 - mae: 6.5255 - val_loss: 81.2882 - val_mse: 81.2882 - val_mae: 6.4789\n",
      "Epoch 925/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.4640 - mse: 87.4639 - mae: 6.5163 - val_loss: 81.5651 - val_mse: 81.5651 - val_mae: 6.4510\n",
      "Epoch 926/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.2303 - mse: 87.2304 - mae: 6.4976 - val_loss: 81.8082 - val_mse: 81.8082 - val_mae: 6.5402\n",
      "Epoch 927/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.1372 - mse: 87.1373 - mae: 6.5080 - val_loss: 82.6633 - val_mse: 82.6633 - val_mae: 6.3676\n",
      "Epoch 928/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4037 - mse: 87.4037 - mae: 6.5157 - val_loss: 83.1216 - val_mse: 83.1216 - val_mae: 6.7007\n",
      "Epoch 929/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.5174 - mse: 87.5175 - mae: 6.5186 - val_loss: 82.4145 - val_mse: 82.4145 - val_mae: 6.3750\n",
      "Epoch 930/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.2725 - mse: 87.2726 - mae: 6.5041 - val_loss: 82.3428 - val_mse: 82.3428 - val_mae: 6.5816\n",
      "Epoch 931/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.5053 - mse: 87.5054 - mae: 6.5097 - val_loss: 81.7548 - val_mse: 81.7548 - val_mae: 6.5682\n",
      "Epoch 932/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4196 - mse: 87.4196 - mae: 6.5165 - val_loss: 81.3298 - val_mse: 81.3298 - val_mae: 6.4775\n",
      "Epoch 933/1000\n",
      "14194/14194 [==============================] - 2s 110us/step - loss: 87.3607 - mse: 87.3606 - mae: 6.5110 - val_loss: 81.4338 - val_mse: 81.4338 - val_mae: 6.3852\n",
      "Epoch 934/1000\n",
      "14194/14194 [==============================] - 1s 89us/step - loss: 87.4011 - mse: 87.4010 - mae: 6.5060 - val_loss: 81.2770 - val_mse: 81.2770 - val_mae: 6.4451\n",
      "Epoch 935/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.2967 - mse: 87.2968 - mae: 6.5107 - val_loss: 81.3775 - val_mse: 81.3775 - val_mae: 6.4196\n",
      "Epoch 936/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.3568 - mse: 87.3569 - mae: 6.5067 - val_loss: 81.5458 - val_mse: 81.5458 - val_mae: 6.4138\n",
      "Epoch 937/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.2588 - mse: 87.2588 - mae: 6.5040 - val_loss: 82.2194 - val_mse: 82.2194 - val_mae: 6.6016\n",
      "Epoch 938/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.4903 - mse: 87.4903 - mae: 6.5124 - val_loss: 81.3805 - val_mse: 81.3806 - val_mae: 6.4348\n",
      "Epoch 939/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.2565 - mse: 87.2565 - mae: 6.5030 - val_loss: 81.5730 - val_mse: 81.5730 - val_mae: 6.4669\n",
      "Epoch 940/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.3143 - mse: 87.3143 - mae: 6.5145 - val_loss: 81.5473 - val_mse: 81.5473 - val_mae: 6.5434\n",
      "Epoch 941/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.1799 - mse: 87.1799 - mae: 6.5212 - val_loss: 81.5674 - val_mse: 81.5675 - val_mae: 6.4748\n",
      "Epoch 942/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.2568 - mse: 87.2568 - mae: 6.5183 - val_loss: 81.4386 - val_mse: 81.4386 - val_mae: 6.5030\n",
      "Epoch 943/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.4797 - mse: 87.4797 - mae: 6.5047 - val_loss: 81.6031 - val_mse: 81.6031 - val_mae: 6.5540\n",
      "Epoch 944/1000\n",
      "14194/14194 [==============================] - 1s 93us/step - loss: 87.3782 - mse: 87.3783 - mae: 6.5106 - val_loss: 81.6952 - val_mse: 81.6952 - val_mae: 6.5212\n",
      "Epoch 945/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.3482 - mse: 87.3482 - mae: 6.4989 - val_loss: 82.1646 - val_mse: 82.1646 - val_mae: 6.3818\n",
      "Epoch 946/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.1903 - mse: 87.1903 - mae: 6.5135 - val_loss: 81.2558 - val_mse: 81.2558 - val_mae: 6.4210\n",
      "Epoch 947/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.6250 - mse: 87.6251 - mae: 6.5154 - val_loss: 81.7876 - val_mse: 81.7877 - val_mae: 6.4107\n",
      "Epoch 948/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.5013 - mse: 87.5012 - mae: 6.5118 - val_loss: 82.5158 - val_mse: 82.5158 - val_mae: 6.6261\n",
      "Epoch 949/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.4977 - mse: 87.4977 - mae: 6.5138 - val_loss: 81.4410 - val_mse: 81.4410 - val_mae: 6.4152\n",
      "Epoch 950/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.4394 - mse: 87.4394 - mae: 6.5076 - val_loss: 81.6085 - val_mse: 81.6084 - val_mae: 6.4186\n",
      "Epoch 951/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.3614 - mse: 87.3614 - mae: 6.5061 - val_loss: 81.9907 - val_mse: 81.9907 - val_mae: 6.5174\n",
      "Epoch 952/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.4575 - mse: 87.4574 - mae: 6.5158 - val_loss: 81.2638 - val_mse: 81.2638 - val_mae: 6.4532\n",
      "Epoch 953/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.1872 - mse: 87.1872 - mae: 6.5049 - val_loss: 81.5164 - val_mse: 81.5164 - val_mae: 6.4238\n",
      "Epoch 954/1000\n",
      "14194/14194 [==============================] - 2s 108us/step - loss: 87.2907 - mse: 87.2907 - mae: 6.5122 - val_loss: 81.4984 - val_mse: 81.4985 - val_mae: 6.4740\n",
      "Epoch 955/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.4969 - mse: 87.4969 - mae: 6.5146 - val_loss: 82.1726 - val_mse: 82.1726 - val_mae: 6.4276\n",
      "Epoch 956/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.3350 - mse: 87.3350 - mae: 6.4984 - val_loss: 81.3334 - val_mse: 81.3334 - val_mae: 6.4840\n",
      "Epoch 957/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.3031 - mse: 87.3031 - mae: 6.5024 - val_loss: 81.9727 - val_mse: 81.9727 - val_mae: 6.4939\n",
      "Epoch 958/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.3753 - mse: 87.3752 - mae: 6.5068 - val_loss: 84.2926 - val_mse: 84.2926 - val_mae: 6.6110\n",
      "Epoch 959/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.3694 - mse: 87.3694 - mae: 6.5192 - val_loss: 82.6312 - val_mse: 82.6312 - val_mae: 6.4155\n",
      "Epoch 960/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.4033 - mse: 87.4033 - mae: 6.5034 - val_loss: 81.4046 - val_mse: 81.4045 - val_mae: 6.5158\n",
      "Epoch 961/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.1682 - mse: 87.1683 - mae: 6.5098 - val_loss: 83.7703 - val_mse: 83.7703 - val_mae: 6.3804\n",
      "Epoch 962/1000\n",
      "14194/14194 [==============================] - 2s 107us/step - loss: 87.4522 - mse: 87.4522 - mae: 6.5209 - val_loss: 82.1692 - val_mse: 82.1692 - val_mae: 6.3701\n",
      "Epoch 963/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.3339 - mse: 87.3339 - mae: 6.5077 - val_loss: 81.9829 - val_mse: 81.9829 - val_mae: 6.4758\n",
      "Epoch 964/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.6901 - mse: 87.6901 - mae: 6.5192 - val_loss: 81.5857 - val_mse: 81.5856 - val_mae: 6.5163\n",
      "Epoch 965/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.0779 - mse: 87.0778 - mae: 6.5178 - val_loss: 81.4896 - val_mse: 81.4896 - val_mae: 6.4519\n",
      "Epoch 966/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.2295 - mse: 87.2295 - mae: 6.4999 - val_loss: 81.8769 - val_mse: 81.8768 - val_mae: 6.5330\n",
      "Epoch 967/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.4442 - mse: 87.4441 - mae: 6.5079 - val_loss: 81.6573 - val_mse: 81.6573 - val_mae: 6.5302\n",
      "Epoch 968/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.3453 - mse: 87.3453 - mae: 6.5074 - val_loss: 81.4596 - val_mse: 81.4596 - val_mae: 6.4160\n",
      "Epoch 969/1000\n",
      "14194/14194 [==============================] - 2s 114us/step - loss: 87.2701 - mse: 87.2700 - mae: 6.5017 - val_loss: 81.8649 - val_mse: 81.8650 - val_mae: 6.3801\n",
      "Epoch 970/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.2109 - mse: 87.2110 - mae: 6.5014 - val_loss: 82.4450 - val_mse: 82.4450 - val_mae: 6.3631\n",
      "Epoch 971/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.2194 - mse: 87.2195 - mae: 6.5020 - val_loss: 81.3500 - val_mse: 81.3500 - val_mae: 6.4715\n",
      "Epoch 972/1000\n",
      "14194/14194 [==============================] - 1s 95us/step - loss: 87.1573 - mse: 87.1574 - mae: 6.5093 - val_loss: 81.3640 - val_mse: 81.3640 - val_mae: 6.4546\n",
      "Epoch 973/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.2338 - mse: 87.2338 - mae: 6.4960 - val_loss: 82.7612 - val_mse: 82.7612 - val_mae: 6.3821\n",
      "Epoch 974/1000\n",
      "14194/14194 [==============================] - 2s 113us/step - loss: 87.2039 - mse: 87.2040 - mae: 6.5095 - val_loss: 81.9220 - val_mse: 81.9220 - val_mae: 6.4515\n",
      "Epoch 975/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.3287 - mse: 87.3287 - mae: 6.5104 - val_loss: 81.6934 - val_mse: 81.6934 - val_mae: 6.4090\n",
      "Epoch 976/1000\n",
      "14194/14194 [==============================] - 2s 112us/step - loss: 87.3119 - mse: 87.3118 - mae: 6.5051 - val_loss: 86.3303 - val_mse: 86.3303 - val_mae: 6.9193\n",
      "Epoch 977/1000\n",
      "14194/14194 [==============================] - 2s 106us/step - loss: 87.4109 - mse: 87.4109 - mae: 6.5157 - val_loss: 81.4892 - val_mse: 81.4892 - val_mae: 6.3832\n",
      "Epoch 978/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.3596 - mse: 87.3595 - mae: 6.5090 - val_loss: 81.7602 - val_mse: 81.7603 - val_mae: 6.5739\n",
      "Epoch 979/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.2107 - mse: 87.2107 - mae: 6.5009 - val_loss: 81.6211 - val_mse: 81.6210 - val_mae: 6.5166\n",
      "Epoch 980/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.3176 - mse: 87.3176 - mae: 6.5093 - val_loss: 81.9081 - val_mse: 81.9081 - val_mae: 6.5813\n",
      "Epoch 981/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.3981 - mse: 87.3981 - mae: 6.5017 - val_loss: 81.4866 - val_mse: 81.4866 - val_mae: 6.4066\n",
      "Epoch 982/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.1451 - mse: 87.1452 - mae: 6.5008 - val_loss: 81.3870 - val_mse: 81.3869 - val_mae: 6.4512\n",
      "Epoch 983/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.2486 - mse: 87.2485 - mae: 6.5009 - val_loss: 81.6627 - val_mse: 81.6628 - val_mae: 6.4632\n",
      "Epoch 984/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.4859 - mse: 87.4858 - mae: 6.5162 - val_loss: 82.7184 - val_mse: 82.7183 - val_mae: 6.3828\n",
      "Epoch 985/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.4723 - mse: 87.4723 - mae: 6.5057 - val_loss: 82.3857 - val_mse: 82.3858 - val_mae: 6.5981\n",
      "Epoch 986/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.1912 - mse: 87.1913 - mae: 6.5018 - val_loss: 82.5558 - val_mse: 82.5558 - val_mae: 6.3815\n",
      "Epoch 987/1000\n",
      "14194/14194 [==============================] - 1s 96us/step - loss: 87.2833 - mse: 87.2832 - mae: 6.5074 - val_loss: 81.8016 - val_mse: 81.8016 - val_mae: 6.3784\n",
      "Epoch 988/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.1985 - mse: 87.1984 - mae: 6.4998 - val_loss: 81.2985 - val_mse: 81.2985 - val_mae: 6.4682\n",
      "Epoch 989/1000\n",
      "14194/14194 [==============================] - 1s 99us/step - loss: 87.3201 - mse: 87.3201 - mae: 6.5112 - val_loss: 84.9279 - val_mse: 84.9279 - val_mae: 6.3984\n",
      "Epoch 990/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.3133 - mse: 87.3132 - mae: 6.5080 - val_loss: 82.0537 - val_mse: 82.0537 - val_mae: 6.3766\n",
      "Epoch 991/1000\n",
      "14194/14194 [==============================] - 1s 98us/step - loss: 87.2772 - mse: 87.2771 - mae: 6.4988 - val_loss: 81.8104 - val_mse: 81.8105 - val_mae: 6.3947\n",
      "Epoch 992/1000\n",
      "14194/14194 [==============================] - 1s 94us/step - loss: 87.3833 - mse: 87.3833 - mae: 6.5143 - val_loss: 82.3378 - val_mse: 82.3378 - val_mae: 6.3872\n",
      "Epoch 993/1000\n",
      "14194/14194 [==============================] - 1s 101us/step - loss: 87.2977 - mse: 87.2977 - mae: 6.5007 - val_loss: 81.5703 - val_mse: 81.5703 - val_mae: 6.4758\n",
      "Epoch 994/1000\n",
      "14194/14194 [==============================] - 1s 100us/step - loss: 87.4529 - mse: 87.4529 - mae: 6.5147 - val_loss: 82.0992 - val_mse: 82.0992 - val_mae: 6.3979\n",
      "Epoch 995/1000\n",
      "14194/14194 [==============================] - 1s 102us/step - loss: 87.2463 - mse: 87.2462 - mae: 6.5000 - val_loss: 81.5221 - val_mse: 81.5221 - val_mae: 6.4514\n",
      "Epoch 996/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.3770 - mse: 87.3770 - mae: 6.5068 - val_loss: 82.8995 - val_mse: 82.8995 - val_mae: 6.3702\n",
      "Epoch 997/1000\n",
      "14194/14194 [==============================] - 1s 104us/step - loss: 87.4263 - mse: 87.4262 - mae: 6.5058 - val_loss: 81.6249 - val_mse: 81.6249 - val_mae: 6.3933\n",
      "Epoch 998/1000\n",
      "14194/14194 [==============================] - 1s 105us/step - loss: 87.3368 - mse: 87.3368 - mae: 6.5057 - val_loss: 81.6004 - val_mse: 81.6004 - val_mae: 6.4616\n",
      "Epoch 999/1000\n",
      "14194/14194 [==============================] - 1s 97us/step - loss: 87.1444 - mse: 87.1444 - mae: 6.5055 - val_loss: 84.7678 - val_mse: 84.7678 - val_mae: 6.7474\n",
      "Epoch 1000/1000\n",
      "14194/14194 [==============================] - 1s 103us/step - loss: 87.4642 - mse: 87.4641 - mae: 6.5082 - val_loss: 82.0363 - val_mse: 82.0363 - val_mae: 6.4268\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt,asarray\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=10,  verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82.80110663497035, 82.80111694335938, 6.4374799728393555] [9.09951134 9.09951191 2.53721894]\n"
     ]
    }
   ],
   "source": [
    "error = model.evaluate(X_test, y_test, verbose=2)\n",
    "# print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
    "print(error, sqrt(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelNoS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'loss', 'mse', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# d = defaultdict(LabelEncoder)\n",
    "\n",
    "# labeled_df = X_df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# labeled_df\n",
    "# labeled_df.apply(lambda x: d[x.name].inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.expand_dims(X_test[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: [[34.837612]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted price: {model.predict(test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
