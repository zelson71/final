{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQL database in Postgres\n",
    "wine_db = \"postgresql://postgres:postgres@localhost:5432/Final\"\n",
    "engine = sa.create_engine(wine_db, echo=False)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select table and create df\n",
    "df = pd.read_sql(\"SELECT * FROM us_french_cab_bords\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subregion a str\n",
    "df['subregion'] = df['subregion'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>price</th>\n",
       "      <th>price_bin</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_bin</th>\n",
       "      <th>region</th>\n",
       "      <th>subregion</th>\n",
       "      <th>subsubregion</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>varietal</th>\n",
       "      <th>vintage</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>This is among winemaker Cathy Corison's finest...</td>\n",
       "      <td>Kronos Vineyard</td>\n",
       "      <td>185.0</td>\n",
       "      <td>$100+</td>\n",
       "      <td>95</td>\n",
       "      <td>95-100</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa</td>\n",
       "      <td>St. Helena</td>\n",
       "      <td>Corison 2015 Kronos Vineyard Cabernet Sauvigno...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/corison-2...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2015</td>\n",
       "      <td>Corison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>14.2</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>This bottling by industry veterans John and He...</td>\n",
       "      <td>None</td>\n",
       "      <td>35.0</td>\n",
       "      <td>$25-$50</td>\n",
       "      <td>94</td>\n",
       "      <td>90-94</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Falcone 2016 Cabernet Sauvignon (Paso Robles)</td>\n",
       "      <td>https://www.winemag.com/buying-guide/falcone-2...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2016</td>\n",
       "      <td>Falcone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>14.5</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>Inky in the glass, this rich and structured bo...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>56.0</td>\n",
       "      <td>$51-$100</td>\n",
       "      <td>94</td>\n",
       "      <td>90-94</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Daou 2016 Reserve Cabernet Sauvignon (Paso Rob...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/daou-2016...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2016</td>\n",
       "      <td>Daou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>14.5</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>This wine is pretty, floral and compact in red...</td>\n",
       "      <td>None</td>\n",
       "      <td>102.0</td>\n",
       "      <td>$100+</td>\n",
       "      <td>95</td>\n",
       "      <td>95-100</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Alpha Omega 2015 Cabernet Sauvignon (Napa Valley)</td>\n",
       "      <td>https://www.winemag.com/buying-guide/alpha-ome...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2015</td>\n",
       "      <td>Alpha Omega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>161</td>\n",
       "      <td>14.7</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>The producer's new Nomad collection includes b...</td>\n",
       "      <td>Nomad Beckstoffer Missouri Hopper Vineyard</td>\n",
       "      <td>175.0</td>\n",
       "      <td>$100+</td>\n",
       "      <td>95</td>\n",
       "      <td>95-100</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Stewart 2016 Nomad Beckstoffer Missouri Hopper...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/stewart-2...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2016</td>\n",
       "      <td>Stewart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18051</td>\n",
       "      <td>18050</td>\n",
       "      <td>237901</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Red</td>\n",
       "      <td>France</td>\n",
       "      <td>A lean, rather attenuated wine. It has some pl...</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "      <td>$0-$25</td>\n",
       "      <td>81</td>\n",
       "      <td>80-84</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>None</td>\n",
       "      <td>Caves Fleury 2005  Bordeaux</td>\n",
       "      <td>https://www.winemag.com/buying-guide/caves-fle...</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>2005</td>\n",
       "      <td>Caves Fleury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18052</td>\n",
       "      <td>18051</td>\n",
       "      <td>237904</td>\n",
       "      <td>14.6</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>Hard to explain why the winery is only now rel...</td>\n",
       "      <td>None</td>\n",
       "      <td>28.0</td>\n",
       "      <td>$25-$50</td>\n",
       "      <td>82</td>\n",
       "      <td>80-84</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Santa Barbara County</td>\n",
       "      <td>Tin House 2003 Cabernet Sauvignon (Santa Barba...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/tin-house...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2003</td>\n",
       "      <td>Tin House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18053</td>\n",
       "      <td>18052</td>\n",
       "      <td>237909</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Red</td>\n",
       "      <td>France</td>\n",
       "      <td>A raw-edged wine, with dusty tannins and black...</td>\n",
       "      <td>Mouton Cadet</td>\n",
       "      <td>9.0</td>\n",
       "      <td>$0-$25</td>\n",
       "      <td>82</td>\n",
       "      <td>80-84</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>None</td>\n",
       "      <td>Baron Philippe de Rothschild 2005 Mouton Cadet...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/baron-phi...</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>2005</td>\n",
       "      <td>Baron Philippe de Rothschild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18054</td>\n",
       "      <td>18053</td>\n",
       "      <td>237910</td>\n",
       "      <td>13.8</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>The blazing vintage took its toll on this high...</td>\n",
       "      <td>None</td>\n",
       "      <td>42.0</td>\n",
       "      <td>$25-$50</td>\n",
       "      <td>82</td>\n",
       "      <td>80-84</td>\n",
       "      <td>California</td>\n",
       "      <td>Sierra Foothills</td>\n",
       "      <td>Sierra Foothills</td>\n",
       "      <td>Peters Family 2004 Cabernet Sauvignon (Sierra ...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/peters-fa...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2004</td>\n",
       "      <td>Peters Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18055</td>\n",
       "      <td>18055</td>\n",
       "      <td>225077</td>\n",
       "      <td>13.5</td>\n",
       "      <td>Red</td>\n",
       "      <td>US</td>\n",
       "      <td>A little harsh and green, with mint, black cur...</td>\n",
       "      <td>Seven Oaks</td>\n",
       "      <td>17.0</td>\n",
       "      <td>$0-$25</td>\n",
       "      <td>85</td>\n",
       "      <td>85-89</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>J. Lohr 2006 Seven Oaks Cabernet Sauvignon (Pa...</td>\n",
       "      <td>https://www.winemag.com/buying-guide/j-lohr-20...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>2006</td>\n",
       "      <td>J. Lohr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18056 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      ID  alcohol category country  \\\n",
       "0          0       1     13.2      Red      US   \n",
       "1          1      52     14.2      Red      US   \n",
       "2          2     128     14.5      Red      US   \n",
       "3          3     136     14.5      Red      US   \n",
       "4          4     161     14.7      Red      US   \n",
       "...      ...     ...      ...      ...     ...   \n",
       "18051  18050  237901     12.0      Red  France   \n",
       "18052  18051  237904     14.6      Red      US   \n",
       "18053  18052  237909     13.0      Red  France   \n",
       "18054  18053  237910     13.8      Red      US   \n",
       "18055  18055  225077     13.5      Red      US   \n",
       "\n",
       "                                             description  \\\n",
       "0      This is among winemaker Cathy Corison's finest...   \n",
       "1      This bottling by industry veterans John and He...   \n",
       "2      Inky in the glass, this rich and structured bo...   \n",
       "3      This wine is pretty, floral and compact in red...   \n",
       "4      The producer's new Nomad collection includes b...   \n",
       "...                                                  ...   \n",
       "18051  A lean, rather attenuated wine. It has some pl...   \n",
       "18052  Hard to explain why the winery is only now rel...   \n",
       "18053  A raw-edged wine, with dusty tannins and black...   \n",
       "18054  The blazing vintage took its toll on this high...   \n",
       "18055  A little harsh and green, with mint, black cur...   \n",
       "\n",
       "                                      designation  price price_bin  rating  \\\n",
       "0                                 Kronos Vineyard  185.0     $100+      95   \n",
       "1                                            None   35.0   $25-$50      94   \n",
       "2                                         Reserve   56.0  $51-$100      94   \n",
       "3                                            None  102.0     $100+      95   \n",
       "4      Nomad Beckstoffer Missouri Hopper Vineyard  175.0     $100+      95   \n",
       "...                                           ...    ...       ...     ...   \n",
       "18051                                        None   10.0    $0-$25      81   \n",
       "18052                                        None   28.0   $25-$50      82   \n",
       "18053                                Mouton Cadet    9.0    $0-$25      82   \n",
       "18054                                        None   42.0   $25-$50      82   \n",
       "18055                                  Seven Oaks   17.0    $0-$25      85   \n",
       "\n",
       "      rating_bin      region         subregion          subsubregion  \\\n",
       "0         95-100  California              Napa            St. Helena   \n",
       "1          90-94  California     Central Coast           Paso Robles   \n",
       "2          90-94  California     Central Coast           Paso Robles   \n",
       "3         95-100  California              Napa           Napa Valley   \n",
       "4         95-100  California              Napa           Napa Valley   \n",
       "...          ...         ...               ...                   ...   \n",
       "18051      80-84    Bordeaux          Bordeaux                  None   \n",
       "18052      80-84  California     Central Coast  Santa Barbara County   \n",
       "18053      80-84    Bordeaux          Bordeaux                  None   \n",
       "18054      80-84  California  Sierra Foothills      Sierra Foothills   \n",
       "18055      85-89  California     Central Coast           Paso Robles   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Corison 2015 Kronos Vineyard Cabernet Sauvigno...   \n",
       "1          Falcone 2016 Cabernet Sauvignon (Paso Robles)   \n",
       "2      Daou 2016 Reserve Cabernet Sauvignon (Paso Rob...   \n",
       "3      Alpha Omega 2015 Cabernet Sauvignon (Napa Valley)   \n",
       "4      Stewart 2016 Nomad Beckstoffer Missouri Hopper...   \n",
       "...                                                  ...   \n",
       "18051                        Caves Fleury 2005  Bordeaux   \n",
       "18052  Tin House 2003 Cabernet Sauvignon (Santa Barba...   \n",
       "18053  Baron Philippe de Rothschild 2005 Mouton Cadet...   \n",
       "18054  Peters Family 2004 Cabernet Sauvignon (Sierra ...   \n",
       "18055  J. Lohr 2006 Seven Oaks Cabernet Sauvignon (Pa...   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://www.winemag.com/buying-guide/corison-2...   \n",
       "1      https://www.winemag.com/buying-guide/falcone-2...   \n",
       "2      https://www.winemag.com/buying-guide/daou-2016...   \n",
       "3      https://www.winemag.com/buying-guide/alpha-ome...   \n",
       "4      https://www.winemag.com/buying-guide/stewart-2...   \n",
       "...                                                  ...   \n",
       "18051  https://www.winemag.com/buying-guide/caves-fle...   \n",
       "18052  https://www.winemag.com/buying-guide/tin-house...   \n",
       "18053  https://www.winemag.com/buying-guide/baron-phi...   \n",
       "18054  https://www.winemag.com/buying-guide/peters-fa...   \n",
       "18055  https://www.winemag.com/buying-guide/j-lohr-20...   \n",
       "\n",
       "                       varietal  vintage                        winery  \n",
       "0            Cabernet Sauvignon     2015                       Corison  \n",
       "1            Cabernet Sauvignon     2016                       Falcone  \n",
       "2            Cabernet Sauvignon     2016                          Daou  \n",
       "3            Cabernet Sauvignon     2015                   Alpha Omega  \n",
       "4            Cabernet Sauvignon     2016                       Stewart  \n",
       "...                         ...      ...                           ...  \n",
       "18051  Bordeaux-style Red Blend     2005                  Caves Fleury  \n",
       "18052        Cabernet Sauvignon     2003                     Tin House  \n",
       "18053  Bordeaux-style Red Blend     2005  Baron Philippe de Rothschild  \n",
       "18054        Cabernet Sauvignon     2004                 Peters Family  \n",
       "18055        Cabernet Sauvignon     2006                       J. Lohr  \n",
       "\n",
       "[18056 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>subregion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Central Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>56.0</td>\n",
       "      <td>94</td>\n",
       "      <td>Central Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>102.0</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.0</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  rating      subregion\n",
       "0  185.0      95           Napa\n",
       "1   35.0      94  Central Coast\n",
       "2   56.0      94  Central Coast\n",
       "3  102.0      95           Napa\n",
       "4  175.0      95           Napa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unneccessary columns\n",
    "df = df.drop(['ID', 'index','category', 'country', 'region', 'subsubregion','varietal', 'alcohol', 'description', 'price_bin', 'rating_bin', 'designation', 'title', 'url', 'vintage','winery'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "df = df.dropna(subset=['subregion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>Anjou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>Anjou Villages Brissac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>Atlantique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>Augusta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>Bergerac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2179</td>\n",
       "      <td>100</td>\n",
       "      <td>Vin de Pays des Coteaux de Peyriac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>100</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2181</td>\n",
       "      <td>100</td>\n",
       "      <td>Virginia's Eastern Shore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2182</td>\n",
       "      <td>100</td>\n",
       "      <td>Washington Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2183</td>\n",
       "      <td>100</td>\n",
       "      <td>Yadkin Valley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2184 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                                   1\n",
       "0      80                               Anjou\n",
       "1      80              Anjou Villages Brissac\n",
       "2      80                          Atlantique\n",
       "3      80                             Augusta\n",
       "4      80                            Bergerac\n",
       "...   ...                                 ...\n",
       "2179  100  Vin de Pays des Coteaux de Peyriac\n",
       "2180  100                            Virginia\n",
       "2181  100            Virginia's Eastern Shore\n",
       "2182  100                    Washington Other\n",
       "2183  100                       Yadkin Valley\n",
       "\n",
       "[2184 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather unique lists of categories\n",
    "encoded_rating = le.fit(df.rating)\n",
    "rating_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "rating_name_array = le.classes_\n",
    "rating_le_array = le.transform(le.classes_)\n",
    "\n",
    "encoded_subregion = le.fit(df.subregion)\n",
    "subregion_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "subregion_name_array = le.classes_\n",
    "subregion_le_array = le.transform(le.classes_)\n",
    "\n",
    "# Create dataframe of all possible iterations of rating and subregion pairings\n",
    "predict_list = list(itertools.product(rating_name_array, subregion_name_array))\n",
    "predict_df = pd.DataFrame(predict_list)\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False,categories=\"auto\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply OneHotEncoder to X\n",
    "X_ohe = ohe.fit_transform(X) # It returns an numpy array\n",
    "categories = ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# apply OneHotEncoder to predict_df\n",
    "predict_X_ohe = ohe.fit_transform(predict_df)\n",
    "print(predict_X_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4514,)\n",
      "(4514, 125)\n"
     ]
    }
   ],
   "source": [
    "# Prepare for train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ohe, y, random_state=42)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capture # inputs in a variable\n",
    "input_dims = X_ohe.shape[1]\n",
    "input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               16128     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,449\n",
      "Trainable params: 24,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Sequential Keras model to train data\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation = 'relu', name='dense_1', kernel_initializer='glorot_uniform', input_dim=(input_dims)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu', name='dense_2', kernel_initializer='glorot_uniform'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(64, activation='relu', name='dense_3', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(1,name='predictions'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt,asarray\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10833 samples, validate on 2709 samples\n",
      "Epoch 1/125\n",
      "10833/10833 [==============================] - 1s 102us/step - loss: 7818.4594 - mse: 7818.4604 - mae: 29.2897 - val_loss: 14410.8158 - val_mse: 14410.8184 - val_mae: 22.2759\n",
      "Epoch 2/125\n",
      "10833/10833 [==============================] - 1s 62us/step - loss: 6743.2707 - mse: 6743.2695 - mae: 21.5969 - val_loss: 14171.4858 - val_mse: 14171.4834 - val_mae: 21.1476\n",
      "Epoch 3/125\n",
      "10833/10833 [==============================] - 1s 60us/step - loss: 6591.1742 - mse: 6591.1753 - mae: 21.3145 - val_loss: 13961.0567 - val_mse: 13961.0527 - val_mae: 21.8689\n",
      "Epoch 4/125\n",
      "10833/10833 [==============================] - 1s 61us/step - loss: 6480.4263 - mse: 6480.4268 - mae: 21.4197 - val_loss: 13797.8346 - val_mse: 13797.8311 - val_mae: 22.5625\n",
      "Epoch 5/125\n",
      "10833/10833 [==============================] - 1s 61us/step - loss: 6418.1101 - mse: 6418.1104 - mae: 21.5085 - val_loss: 13639.2072 - val_mse: 13639.2041 - val_mae: 21.4692\n",
      "Epoch 6/125\n",
      "10833/10833 [==============================] - 1s 61us/step - loss: 6367.6312 - mse: 6367.6294 - mae: 21.5518 - val_loss: 13443.4643 - val_mse: 13443.4629 - val_mae: 22.2980\n",
      "Epoch 7/125\n",
      "10833/10833 [==============================] - 1s 62us/step - loss: 6324.0878 - mse: 6324.0874 - mae: 21.4625 - val_loss: 13283.0150 - val_mse: 13283.0137 - val_mae: 22.1835\n",
      "Epoch 8/125\n",
      "10833/10833 [==============================] - 1s 60us/step - loss: 6274.0635 - mse: 6274.0635 - mae: 21.5383 - val_loss: 13116.7917 - val_mse: 13116.7959 - val_mae: 22.5239\n",
      "Epoch 9/125\n",
      "10833/10833 [==============================] - 1s 64us/step - loss: 6238.2351 - mse: 6238.2349 - mae: 21.5341 - val_loss: 12915.3170 - val_mse: 12915.3174 - val_mae: 22.3785\n",
      "Epoch 10/125\n",
      "10833/10833 [==============================] - 1s 60us/step - loss: 6189.7431 - mse: 6189.7456 - mae: 21.6997 - val_loss: 12750.2135 - val_mse: 12750.2119 - val_mae: 21.9305\n",
      "Epoch 11/125\n",
      "10833/10833 [==============================] - 1s 62us/step - loss: 6152.5368 - mse: 6152.5371 - mae: 21.2814 - val_loss: 12585.7632 - val_mse: 12585.7627 - val_mae: 23.8946\n",
      "Epoch 12/125\n",
      "10833/10833 [==============================] - 1s 59us/step - loss: 6109.0974 - mse: 6109.0977 - mae: 21.5317 - val_loss: 12360.0655 - val_mse: 12360.0664 - val_mae: 22.9006\n",
      "Epoch 13/125\n",
      "10833/10833 [==============================] - 1s 60us/step - loss: 6075.9327 - mse: 6075.9316 - mae: 21.6545 - val_loss: 12160.3263 - val_mse: 12160.3271 - val_mae: 22.3785\n",
      "Epoch 14/125\n",
      "10833/10833 [==============================] - 1s 66us/step - loss: 5997.6813 - mse: 5997.6807 - mae: 21.4564 - val_loss: 11961.2525 - val_mse: 11961.2549 - val_mae: 22.4907\n",
      "Epoch 15/125\n",
      "10833/10833 [==============================] - 1s 67us/step - loss: 5991.2916 - mse: 5991.2900 - mae: 21.4331 - val_loss: 11753.8647 - val_mse: 11753.8633 - val_mae: 22.0853\n",
      "Epoch 16/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 5935.9933 - mse: 5935.9937 - mae: 21.3618 - val_loss: 11706.8608 - val_mse: 11706.8643 - val_mae: 22.6318\n",
      "Epoch 17/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 5903.0315 - mse: 5903.0312 - mae: 21.4958 - val_loss: 11372.6541 - val_mse: 11372.6523 - val_mae: 22.2287\n",
      "Epoch 18/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 5857.3462 - mse: 5857.3438 - mae: 21.4787 - val_loss: 11197.5727 - val_mse: 11197.5723 - val_mae: 22.7854\n",
      "Epoch 19/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 5797.6349 - mse: 5797.6357 - mae: 21.5658 - val_loss: 10991.9715 - val_mse: 10991.9736 - val_mae: 21.4336\n",
      "Epoch 20/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 5796.4037 - mse: 5796.4033 - mae: 21.0869 - val_loss: 10849.0985 - val_mse: 10849.1006 - val_mae: 23.6853\n",
      "Epoch 21/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 5717.1986 - mse: 5717.2002 - mae: 21.5227 - val_loss: 10562.7611 - val_mse: 10562.7607 - val_mae: 21.4272\n",
      "Epoch 22/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 5639.8817 - mse: 5639.8813 - mae: 21.2353 - val_loss: 10322.8730 - val_mse: 10322.8770 - val_mae: 21.8604\n",
      "Epoch 23/125\n",
      "10833/10833 [==============================] - 1s 92us/step - loss: 5636.3604 - mse: 5636.3618 - mae: 21.4164 - val_loss: 10148.8117 - val_mse: 10148.8096 - val_mae: 21.4888\n",
      "Epoch 24/125\n",
      "10833/10833 [==============================] - 1s 99us/step - loss: 5609.0580 - mse: 5609.0586 - mae: 21.1112 - val_loss: 9955.8031 - val_mse: 9955.8018 - val_mae: 23.3193\n",
      "Epoch 25/125\n",
      "10833/10833 [==============================] - 1s 91us/step - loss: 5580.9148 - mse: 5580.9165 - mae: 21.6503 - val_loss: 9807.6957 - val_mse: 9807.6973 - val_mae: 21.2001\n",
      "Epoch 26/125\n",
      "10833/10833 [==============================] - 1s 105us/step - loss: 5541.3454 - mse: 5541.3452 - mae: 21.2729 - val_loss: 9654.3622 - val_mse: 9654.3613 - val_mae: 21.4046\n",
      "Epoch 27/125\n",
      "10833/10833 [==============================] - 1s 106us/step - loss: 5531.1400 - mse: 5531.1396 - mae: 21.1770 - val_loss: 9425.0039 - val_mse: 9425.0059 - val_mae: 21.6275\n",
      "Epoch 28/125\n",
      "10833/10833 [==============================] - 1s 99us/step - loss: 5486.3732 - mse: 5486.3726 - mae: 21.1056 - val_loss: 9241.3434 - val_mse: 9241.3428 - val_mae: 21.9292\n",
      "Epoch 29/125\n",
      "10833/10833 [==============================] - 1s 81us/step - loss: 5479.6742 - mse: 5479.6729 - mae: 21.0862 - val_loss: 9034.9090 - val_mse: 9034.9121 - val_mae: 22.1098\n",
      "Epoch 30/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 5432.3656 - mse: 5432.3643 - mae: 21.1846 - val_loss: 8884.6665 - val_mse: 8884.6641 - val_mae: 22.0290\n",
      "Epoch 31/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 5407.5161 - mse: 5407.5146 - mae: 21.1956 - val_loss: 8731.2531 - val_mse: 8731.2539 - val_mae: 21.6889\n",
      "Epoch 32/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 5373.6402 - mse: 5373.6416 - mae: 21.2598 - val_loss: 8661.4819 - val_mse: 8661.4814 - val_mae: 21.1450\n",
      "Epoch 33/125\n",
      "10833/10833 [==============================] - 1s 68us/step - loss: 5298.0374 - mse: 5298.0352 - mae: 21.0268 - val_loss: 8475.9887 - val_mse: 8475.9893 - val_mae: 22.0714\n",
      "Epoch 34/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 5319.3880 - mse: 5319.3887 - mae: 21.1529 - val_loss: 8351.5769 - val_mse: 8351.5752 - val_mae: 21.5913\n",
      "Epoch 35/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 5286.4138 - mse: 5286.4136 - mae: 21.0662 - val_loss: 8192.6358 - val_mse: 8192.6367 - val_mae: 21.6966\n",
      "Epoch 36/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 5286.4508 - mse: 5286.4487 - mae: 21.1023 - val_loss: 8120.6743 - val_mse: 8120.6748 - val_mae: 21.7216\n",
      "Epoch 37/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 5217.3479 - mse: 5217.3462 - mae: 20.9112 - val_loss: 7949.5852 - val_mse: 7949.5830 - val_mae: 21.8488\n",
      "Epoch 38/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 5157.9345 - mse: 5157.9341 - mae: 21.0309 - val_loss: 7880.1965 - val_mse: 7880.2002 - val_mae: 21.8684\n",
      "Epoch 39/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 5206.2351 - mse: 5206.2363 - mae: 20.8952 - val_loss: 7779.4618 - val_mse: 7779.4595 - val_mae: 21.3729\n",
      "Epoch 40/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 5183.1600 - mse: 5183.1616 - mae: 20.8681 - val_loss: 7598.6420 - val_mse: 7598.6431 - val_mae: 21.4697\n",
      "Epoch 41/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 5144.0528 - mse: 5144.0522 - mae: 21.3629 - val_loss: 7488.5253 - val_mse: 7488.5259 - val_mae: 21.5358\n",
      "Epoch 42/125\n",
      "10833/10833 [==============================] - 1s 79us/step - loss: 5083.2698 - mse: 5083.2690 - mae: 20.8695 - val_loss: 7350.7767 - val_mse: 7350.7744 - val_mae: 21.8300\n",
      "Epoch 43/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 5069.0759 - mse: 5069.0757 - mae: 20.9473 - val_loss: 7256.8918 - val_mse: 7256.8916 - val_mae: 21.8061\n",
      "Epoch 44/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 5028.9381 - mse: 5028.9385 - mae: 20.9183 - val_loss: 7208.8856 - val_mse: 7208.8867 - val_mae: 22.1810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 5036.1399 - mse: 5036.1396 - mae: 20.9235 - val_loss: 7095.0802 - val_mse: 7095.0786 - val_mae: 21.3975\n",
      "Epoch 46/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 5057.6088 - mse: 5057.6094 - mae: 20.8368 - val_loss: 7017.4752 - val_mse: 7017.4727 - val_mae: 21.5375\n",
      "Epoch 47/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4952.8047 - mse: 4952.8042 - mae: 20.7749 - val_loss: 6904.6964 - val_mse: 6904.6953 - val_mae: 21.6330\n",
      "Epoch 48/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 5002.6392 - mse: 5002.6421 - mae: 20.8165 - val_loss: 6827.8982 - val_mse: 6827.9004 - val_mae: 21.1530\n",
      "Epoch 49/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4939.6612 - mse: 4939.6631 - mae: 20.6171 - val_loss: 6701.3615 - val_mse: 6701.3623 - val_mae: 21.5013\n",
      "Epoch 50/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4979.0520 - mse: 4979.0522 - mae: 20.7152 - val_loss: 6594.6745 - val_mse: 6594.6733 - val_mae: 21.5690\n",
      "Epoch 51/125\n",
      "10833/10833 [==============================] - 1s 75us/step - loss: 4894.3499 - mse: 4894.3496 - mae: 20.7152 - val_loss: 6510.5780 - val_mse: 6510.5767 - val_mae: 21.8567\n",
      "Epoch 52/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4936.8323 - mse: 4936.8340 - mae: 20.7379 - val_loss: 6465.9353 - val_mse: 6465.9370 - val_mae: 21.1364\n",
      "Epoch 53/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4877.9387 - mse: 4877.9395 - mae: 20.5160 - val_loss: 6394.5481 - val_mse: 6394.5469 - val_mae: 21.2959\n",
      "Epoch 54/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4879.3962 - mse: 4879.3960 - mae: 20.6319 - val_loss: 6312.9272 - val_mse: 6312.9268 - val_mae: 21.3959\n",
      "Epoch 55/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4842.4898 - mse: 4842.4893 - mae: 20.4783 - val_loss: 6311.5652 - val_mse: 6311.5664 - val_mae: 21.9876\n",
      "Epoch 56/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4813.2906 - mse: 4813.2925 - mae: 20.6292 - val_loss: 6154.5756 - val_mse: 6154.5771 - val_mae: 21.5926\n",
      "Epoch 57/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4793.1039 - mse: 4793.1021 - mae: 20.5345 - val_loss: 6096.7215 - val_mse: 6096.7227 - val_mae: 21.1785\n",
      "Epoch 58/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4835.5950 - mse: 4835.5962 - mae: 20.4803 - val_loss: 5989.2795 - val_mse: 5989.2798 - val_mae: 21.5046\n",
      "Epoch 59/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4780.7462 - mse: 4780.7451 - mae: 20.5033 - val_loss: 5968.0845 - val_mse: 5968.0845 - val_mae: 21.1420\n",
      "Epoch 60/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4821.8236 - mse: 4821.8237 - mae: 20.4790 - val_loss: 5944.3906 - val_mse: 5944.3896 - val_mae: 21.0761\n",
      "Epoch 61/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4791.2026 - mse: 4791.2026 - mae: 20.3589 - val_loss: 5841.8101 - val_mse: 5841.8101 - val_mae: 21.3460\n",
      "Epoch 62/125\n",
      "10833/10833 [==============================] - 1s 80us/step - loss: 4736.1896 - mse: 4736.1899 - mae: 20.3237 - val_loss: 5821.7256 - val_mse: 5821.7241 - val_mae: 21.8893\n",
      "Epoch 63/125\n",
      "10833/10833 [==============================] - 1s 76us/step - loss: 4742.8843 - mse: 4742.8853 - mae: 20.4659 - val_loss: 5796.5780 - val_mse: 5796.5771 - val_mae: 21.3767\n",
      "Epoch 64/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4714.9874 - mse: 4714.9883 - mae: 20.3683 - val_loss: 5738.8219 - val_mse: 5738.8232 - val_mae: 21.3922\n",
      "Epoch 65/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4680.0169 - mse: 4680.0161 - mae: 20.3697 - val_loss: 5660.6612 - val_mse: 5660.6611 - val_mae: 21.4936\n",
      "Epoch 66/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4674.6775 - mse: 4674.6758 - mae: 20.4838 - val_loss: 5648.2476 - val_mse: 5648.2476 - val_mae: 21.0781\n",
      "Epoch 67/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4716.6732 - mse: 4716.6738 - mae: 20.2306 - val_loss: 5559.5574 - val_mse: 5559.5562 - val_mae: 21.6155\n",
      "Epoch 68/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4691.2624 - mse: 4691.2603 - mae: 20.2104 - val_loss: 5557.0864 - val_mse: 5557.0859 - val_mae: 21.3647\n",
      "Epoch 69/125\n",
      "10833/10833 [==============================] - 1s 76us/step - loss: 4635.1612 - mse: 4635.1616 - mae: 20.3751 - val_loss: 5459.7562 - val_mse: 5459.7559 - val_mae: 21.1389\n",
      "Epoch 70/125\n",
      "10833/10833 [==============================] - 1s 78us/step - loss: 4709.2055 - mse: 4709.2056 - mae: 20.2700 - val_loss: 5440.0441 - val_mse: 5440.0439 - val_mae: 21.5049\n",
      "Epoch 71/125\n",
      "10833/10833 [==============================] - 1s 77us/step - loss: 4655.1245 - mse: 4655.1245 - mae: 20.2966 - val_loss: 5380.2395 - val_mse: 5380.2397 - val_mae: 21.5643\n",
      "Epoch 72/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4657.8570 - mse: 4657.8589 - mae: 20.3270 - val_loss: 5280.3314 - val_mse: 5280.3320 - val_mae: 21.3251\n",
      "Epoch 73/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4631.6642 - mse: 4631.6646 - mae: 20.2616 - val_loss: 5230.2264 - val_mse: 5230.2271 - val_mae: 21.5572\n",
      "Epoch 74/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4605.5830 - mse: 4605.5859 - mae: 20.1936 - val_loss: 5258.6269 - val_mse: 5258.6255 - val_mae: 20.8771\n",
      "Epoch 75/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4636.0710 - mse: 4636.0713 - mae: 20.1794 - val_loss: 5236.9868 - val_mse: 5236.9858 - val_mae: 21.5391\n",
      "Epoch 76/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4631.4586 - mse: 4631.4570 - mae: 20.3616 - val_loss: 5172.4987 - val_mse: 5172.4995 - val_mae: 21.0416\n",
      "Epoch 77/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4636.5744 - mse: 4636.5762 - mae: 20.3166 - val_loss: 5119.3689 - val_mse: 5119.3682 - val_mae: 21.0699\n",
      "Epoch 78/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4578.4403 - mse: 4578.4399 - mae: 20.1101 - val_loss: 5087.4386 - val_mse: 5087.4380 - val_mae: 21.2403\n",
      "Epoch 79/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4574.9155 - mse: 4574.9165 - mae: 20.0905 - val_loss: 5068.2519 - val_mse: 5068.2520 - val_mae: 21.2928\n",
      "Epoch 80/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4571.9089 - mse: 4571.9072 - mae: 20.1063 - val_loss: 5069.5291 - val_mse: 5069.5293 - val_mae: 21.8110\n",
      "Epoch 81/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4602.9133 - mse: 4602.9126 - mae: 20.1622 - val_loss: 5047.7379 - val_mse: 5047.7388 - val_mae: 21.1142\n",
      "Epoch 82/125\n",
      "10833/10833 [==============================] - 1s 81us/step - loss: 4549.5425 - mse: 4549.5420 - mae: 20.1871 - val_loss: 5017.5746 - val_mse: 5017.5752 - val_mae: 21.4507\n",
      "Epoch 83/125\n",
      "10833/10833 [==============================] - 1s 74us/step - loss: 4613.8337 - mse: 4613.8335 - mae: 20.0754 - val_loss: 4980.7197 - val_mse: 4980.7192 - val_mae: 21.6704\n",
      "Epoch 84/125\n",
      "10833/10833 [==============================] - 1s 76us/step - loss: 4605.1985 - mse: 4605.1987 - mae: 20.1317 - val_loss: 5002.7696 - val_mse: 5002.7705 - val_mae: 21.6581\n",
      "Epoch 85/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4560.9195 - mse: 4560.9185 - mae: 20.1305 - val_loss: 5045.3419 - val_mse: 5045.3413 - val_mae: 21.4835\n",
      "Epoch 86/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4552.4558 - mse: 4552.4561 - mae: 20.0056 - val_loss: 4999.9835 - val_mse: 4999.9834 - val_mae: 21.4676\n",
      "Epoch 87/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4546.3572 - mse: 4546.3564 - mae: 20.0905 - val_loss: 4983.6672 - val_mse: 4983.6660 - val_mae: 21.5101\n",
      "Epoch 88/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4549.6562 - mse: 4549.6572 - mae: 20.1817 - val_loss: 4977.6570 - val_mse: 4977.6562 - val_mae: 21.3726\n",
      "Epoch 89/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10833/10833 [==============================] - 1s 71us/step - loss: 4536.7076 - mse: 4536.7080 - mae: 20.0410 - val_loss: 4921.5814 - val_mse: 4921.5820 - val_mae: 21.8496\n",
      "Epoch 90/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4535.7677 - mse: 4535.7661 - mae: 20.1446 - val_loss: 4892.3545 - val_mse: 4892.3540 - val_mae: 20.8881\n",
      "Epoch 91/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4527.5443 - mse: 4527.5444 - mae: 20.0313 - val_loss: 5031.1109 - val_mse: 5031.1099 - val_mae: 20.9176\n",
      "Epoch 92/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4575.1433 - mse: 4575.1436 - mae: 19.9857 - val_loss: 5001.1733 - val_mse: 5001.1748 - val_mae: 21.3540\n",
      "Epoch 93/125\n",
      "10833/10833 [==============================] - 1s 68us/step - loss: 4524.4375 - mse: 4524.4355 - mae: 20.1160 - val_loss: 4954.0024 - val_mse: 4954.0039 - val_mae: 21.3058\n",
      "Epoch 94/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4545.9166 - mse: 4545.9165 - mae: 20.0791 - val_loss: 4987.2353 - val_mse: 4987.2354 - val_mae: 21.0238\n",
      "Epoch 95/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4556.1414 - mse: 4556.1426 - mae: 20.0449 - val_loss: 4983.6762 - val_mse: 4983.6753 - val_mae: 21.2661\n",
      "Epoch 96/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4541.7447 - mse: 4541.7451 - mae: 20.0513 - val_loss: 4973.2332 - val_mse: 4973.2329 - val_mae: 21.3470\n",
      "Epoch 97/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4529.0007 - mse: 4528.9990 - mae: 20.0546 - val_loss: 4889.4439 - val_mse: 4889.4448 - val_mae: 21.2153\n",
      "Epoch 98/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4533.0352 - mse: 4533.0366 - mae: 20.0224 - val_loss: 4916.7668 - val_mse: 4916.7666 - val_mae: 21.5420\n",
      "Epoch 99/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4536.1963 - mse: 4536.1958 - mae: 19.9789 - val_loss: 4911.9582 - val_mse: 4911.9575 - val_mae: 22.2388\n",
      "Epoch 100/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4512.6395 - mse: 4512.6382 - mae: 20.1086 - val_loss: 4899.1940 - val_mse: 4899.1938 - val_mae: 21.1731\n",
      "Epoch 101/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4543.0650 - mse: 4543.0654 - mae: 19.9310 - val_loss: 4981.9020 - val_mse: 4981.9023 - val_mae: 21.8409\n",
      "Epoch 102/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4531.2876 - mse: 4531.2886 - mae: 19.9800 - val_loss: 5052.6880 - val_mse: 5052.6880 - val_mae: 21.7174\n",
      "Epoch 103/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4527.4016 - mse: 4527.4009 - mae: 20.0691 - val_loss: 4980.1561 - val_mse: 4980.1562 - val_mae: 22.0763\n",
      "Epoch 104/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4576.5600 - mse: 4576.5596 - mae: 20.0347 - val_loss: 4940.8170 - val_mse: 4940.8179 - val_mae: 21.1781\n",
      "Epoch 105/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4539.7838 - mse: 4539.7842 - mae: 19.8972 - val_loss: 4936.3156 - val_mse: 4936.3154 - val_mae: 21.4196\n",
      "Epoch 106/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4520.7463 - mse: 4520.7422 - mae: 20.1221 - val_loss: 4936.6581 - val_mse: 4936.6572 - val_mae: 21.0782\n",
      "Epoch 107/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4523.9056 - mse: 4523.9058 - mae: 19.8978 - val_loss: 4992.5148 - val_mse: 4992.5151 - val_mae: 22.1292\n",
      "Epoch 108/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4524.2797 - mse: 4524.2817 - mae: 20.0605 - val_loss: 5047.7991 - val_mse: 5047.7998 - val_mae: 21.4613\n",
      "Epoch 109/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4512.1814 - mse: 4512.1831 - mae: 19.9230 - val_loss: 4997.9591 - val_mse: 4997.9585 - val_mae: 21.6100\n",
      "Epoch 110/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4496.0072 - mse: 4496.0063 - mae: 20.0384 - val_loss: 5031.6385 - val_mse: 5031.6377 - val_mae: 21.3597\n",
      "Epoch 111/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4533.7911 - mse: 4533.7905 - mae: 20.0099 - val_loss: 5031.2328 - val_mse: 5031.2329 - val_mae: 21.2588\n",
      "Epoch 112/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4563.7056 - mse: 4563.7065 - mae: 19.9476 - val_loss: 5057.5176 - val_mse: 5057.5166 - val_mae: 20.9406\n",
      "Epoch 113/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4522.8767 - mse: 4522.8760 - mae: 20.1371 - val_loss: 4945.4315 - val_mse: 4945.4316 - val_mae: 21.5437\n",
      "Epoch 114/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4538.1294 - mse: 4538.1289 - mae: 19.9328 - val_loss: 5001.2060 - val_mse: 5001.2056 - val_mae: 21.7453\n",
      "Epoch 115/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4535.8764 - mse: 4535.8774 - mae: 20.0263 - val_loss: 4958.1285 - val_mse: 4958.1299 - val_mae: 21.8766\n",
      "Epoch 116/125\n",
      "10833/10833 [==============================] - 1s 73us/step - loss: 4514.6511 - mse: 4514.6499 - mae: 19.9114 - val_loss: 4993.6996 - val_mse: 4993.6997 - val_mae: 21.8199\n",
      "Epoch 117/125\n",
      "10833/10833 [==============================] - 1s 80us/step - loss: 4506.4742 - mse: 4506.4746 - mae: 19.9263 - val_loss: 5003.6785 - val_mse: 5003.6782 - val_mae: 21.2491\n",
      "Epoch 118/125\n",
      "10833/10833 [==============================] - 1s 78us/step - loss: 4527.5342 - mse: 4527.5342 - mae: 19.9143 - val_loss: 5107.3770 - val_mse: 5107.3760 - val_mae: 21.2359\n",
      "Epoch 119/125\n",
      "10833/10833 [==============================] - 1s 68us/step - loss: 4529.5224 - mse: 4529.5225 - mae: 20.0624 - val_loss: 5003.2992 - val_mse: 5003.2993 - val_mae: 21.6712\n",
      "Epoch 120/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4518.3609 - mse: 4518.3584 - mae: 19.8968 - val_loss: 5045.2978 - val_mse: 5045.2983 - val_mae: 21.7024\n",
      "Epoch 121/125\n",
      "10833/10833 [==============================] - 1s 70us/step - loss: 4529.3698 - mse: 4529.3701 - mae: 19.9304 - val_loss: 5072.9992 - val_mse: 5072.9990 - val_mae: 21.8648\n",
      "Epoch 122/125\n",
      "10833/10833 [==============================] - 1s 69us/step - loss: 4526.1700 - mse: 4526.1714 - mae: 20.0464 - val_loss: 5076.2211 - val_mse: 5076.2207 - val_mae: 20.9962\n",
      "Epoch 123/125\n",
      "10833/10833 [==============================] - 1s 72us/step - loss: 4516.3131 - mse: 4516.3130 - mae: 19.8380 - val_loss: 5048.1500 - val_mse: 5048.1504 - val_mae: 21.0366\n",
      "Epoch 124/125\n",
      "10833/10833 [==============================] - 1s 71us/step - loss: 4540.8997 - mse: 4540.8989 - mae: 19.8413 - val_loss: 5049.3375 - val_mse: 5049.3364 - val_mae: 21.4456\n",
      "Epoch 125/125\n",
      "10833/10833 [==============================] - 1s 77us/step - loss: 4521.6181 - mse: 4521.6172 - mae: 19.9481 - val_loss: 5092.5396 - val_mse: 5092.5396 - val_mae: 21.6100\n"
     ]
    }
   ],
   "source": [
    "# Fit model with training data\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=30, callbacks=[EarlyStopping(patience=5)])\n",
    "history = model.fit(X_train, y_train, epochs=125, batch_size=30,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3141.0208433689563\n",
      "MAE: 3141.019775390625\n",
      "Mean Absolute Percentage Error: 20.200368881225586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate error and print MSE, MAE and Mean Absolute Percentage Error\n",
    "error = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(f\"MSE: {error[0]}\")\n",
    "print(f\"MAE: {error[1]}\")\n",
    "print(f\"Mean Absolute Percentage Error: {error[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xV9f3H8dcnN3uREBLIAIIyZMiMiLhQHIgDa6libcVJq7bVVq1a29r9a3+21fpztM5aiwNRKw5UVFxVkKDsIZuEMLL3ujef3x/ngAECJCE3J+PzfDzy4N7vGfdz7iV533O+53yPqCrGGGNMa4R4XYAxxpjOy0LEGGNMq1mIGGOMaTULEWOMMa1mIWKMMabVLESMMca0moWIMe1ERP4pIr9r5rxbReSso12PMcFmIWKMMabVLESMMca0moWIMY24h5FuF5EVIlIpIk+ISG8RmS8i5SLyrogkNpr/IhFZLSIlIvKBiAxtNG2MiHzhLvcCEHnAa10gIsvcZT8VkZGtrPl6EdkoIkUiMk9E0tx2EZH7RGSPiJS62zTCnTZVRNa4te0Qkdta9YaZbs9CxJiDfRM4GxgMXAjMB34G9ML5nfkRgIgMBp4DbgGSgTeB10QkXETCgf8AzwA9gRfd9eIuOxZ4EvgekAT8A5gnIhEtKVREzgT+B7gUSAW2Ac+7k88BTnO3IwG4DCh0pz0BfE9V44ARwPsteV1j9rIQMeZg/6equ1V1B/AxsFhVv1TVWuAVYIw732XAG6q6QFXrgT8DUcBEYAIQBtyvqvWqOhdY0ug1rgf+oaqLVTWgqk8Dte5yLXEF8KSqfuHWdxdwkohkAvVAHHAcIKq6VlV3usvVA8NEJF5Vi1X1ixa+rjGAhYgxTdnd6HF1E89j3cdpON/8AVDVBiAHSHen7dD9Rzjd1uhxf+BW91BWiYiUAH3d5VriwBoqcPY20lX1feBB4CFgt4g8KiLx7qzfBKYC20TkQxE5qYWvawxgIWLM0cjDCQPA6YPACYIdwE4g3W3bq1+jxznA71U1odFPtKo+d5Q1xOAcHtsBoKoPqOo4YDjOYa3b3fYlqjoNSME57Danha9rDGAhYszRmAOcLyKTRSQMuBXnkNSnwGeAH/iRiISKyCXA+EbLPgZ8X0ROdDvAY0TkfBGJa2ENzwJXi8hotz/lDziH37aKyAnu+sOASqAGCLh9NleISA/3MFwZEDiK98F0YxYixrSSqq4HvgP8H1CA0wl/oarWqWodcAlwFVCM03/ycqNls3H6RR50p290521pDe8BvwBewtn7ORaY4U6OxwmrYpxDXoU4/TYA3wW2ikgZ8H13O4xpMbGbUhljjGkt2xMxxhjTahYixhhjWs1CxBhjTKtZiBhjjGm1UK8LaG+9evXSzMxMr8swxphOZenSpQWqmnxge7cLkczMTLKzs70uwxhjOhUR2dZUux3OMsYY02oWIsYYY1rNQsQYY0yrdbs+kabU19eTm5tLTU2N16V0CZGRkWRkZBAWFuZ1KcaYILMQAXJzc4mLiyMzM5P9B101LaWqFBYWkpuby4ABA7wuxxgTZHY4C6ipqSEpKckCpA2ICElJSbZXZ0w3YSHisgBpO/ZeGtN9WIg0hypUFkB1ideVGGNMh2Ih0lxVBVC2A7ShzVddUlLCww8/3OLlpk6dSkmJBZsxxjsWIs0hAnFpEKiDqsI2X/2hQiQQOPzN5t58800SEhLavB5jjGkuOzuruSLiICwGyndDVBKEtF3+3nnnnWzatInRo0cTFhZGbGwsqampLFu2jDVr1nDxxReTk5NDTU0NN998M7NmzQK+HsKloqKC8847j1NOOYVPP/2U9PR0Xn31VaKiotqsRmOMaYqFyAF+/dpq1uSVNT1RA1BfDb4i8DX/GohhafHcc+HwQ07/4x//yKpVq1i2bBkffPAB559/PqtWrdp3iuyTTz5Jz549qa6u5oQTTuCb3/wmSUlJ+61jw4YNPPfcczz22GNceumlvPTSS3znO3bHU2NMcFmItIT4nJ9AHfhCgeCchTR+/Pj9rrF44IEHeOWVVwDIyclhw4YNB4XIgAEDGD16NADjxo1j69atQanNGGMaC1qIiMiTwAXAHlUdccC024B7gWRVLRDnnNC/AVOBKuAqVf3CnXcm8HN30d+p6tNu+zjgn0AU8CZws7bBDeMPt8cAQF0lFHwFsb0hPu1oX65JMTEx+x5/8MEHvPvuu3z22WdER0czadKkJq/BiIiI2PfY5/NRXV0dlNqMMaaxYHas/xOYcmCjiPQFzga2N2o+Dxjk/swCHnHn7QncA5wIjAfuEZFEd5lH3Hn3LnfQawVFeAxE9YSKPeBvmwvq4uLiKC8vb3JaaWkpiYmJREdHs27dOhYtWtQmr2mMMW0haCGiqh8BRU1Mug/4KdB4r2Ea8C91LAISRCQVOBdYoKpFqloMLACmuNPiVfUzd+/jX8DFwdqWg8SnOWdsle5ok9UlJSVx8sknM2LECG6//fb9pk2ZMgW/38/IkSP5xS9+wYQJE9rkNY0xpi20a5+IiFwE7FDV5Qdc1ZwO5DR6nuu2Ha49t4n2Q73uLJy9Fvr163cUW+DyhUFsHyjPg5oyiIw/6lU+++yzTbZHREQwf/78Jqft7ffo1asXq1at2td+2223HXU9xhjTHO12nYiIRAN3A79sanITbdqK9iap6qOqmqWqWcnJB93dsXVikyE0Akq2Q6C+bdZpjDGdTHtebHgsMABYLiJbgQzgCxHpg7Mn0bfRvBlA3hHaM5pobz8SAomZ0OCH4q3O0CjGGNPNtFuIqOpKVU1R1UxVzcQJgrGquguYB1wpjglAqaruBN4GzhGRRLdD/RzgbXdauYhMcM/suhJ4tb22ZZ+waOiRAXUVULGr3V/eGGO8FrQQEZHngM+AISKSKyLXHmb2N4HNwEbgMeBGAFUtAn4LLHF/fuO2AdwAPO4uswlouuMg2KKTnLO1ync5/SPGGNONBK1jXVUvP8L0zEaPFbjpEPM9CTzZRHs2MOLgJdqZiLM3Ul/lHNZKPg5Cw72uyhhj2oUNwNgWQnyQOABQKN4SlJF+jTGmI7IQaSthkZDQ39kjKdsZ1JeKjY0FIC8vj+nTpzc5z6RJk8jOzj7seu6//36qqqr2Pbeh5Y0xLWUh0paiEpw+kso9UFd15PmPUlpaGnPnzm318geGiA0tb4xpKQuRthafBiGhUJrT7NN+77jjjv3uJ/KrX/2KX//610yePJmxY8dy/PHH8+qrB598tnXrVkaMcLqFqqurmTFjBiNHjuSyyy7bb+ysG264gaysLIYPH84999wDOIM65uXlccYZZ3DGGWcAztDyBQUFAPz1r39lxIgRjBgxgvvvv3/f6w0dOpTrr7+e4cOHc84559gYXcZ0czaK74Hm3wm7Vh7dOhrqnXG1fBHgC4c+x8N5fzzk7DNmzOCWW27hxhtvBGDOnDm89dZb/PjHPyY+Pp6CggImTJjARRdddMj7lz/yyCNER0ezYsUKVqxYwdixY/dN+/3vf0/Pnj0JBAJMnjyZFStW8KMf/Yi//vWvLFy4kF69eu23rqVLl/LUU0+xePFiVJUTTzyR008/ncTERBty3hizH9sTCYaQUOcnUOfcg+QIxowZw549e8jLy2P58uUkJiaSmprKz372M0aOHMlZZ53Fjh072L179yHX8dFHH+37Yz5y5EhGjhy5b9qcOXMYO3YsY8aMYfXq1axZs+aw9XzyySd84xvfICYmhtjYWC655BI+/vhjwIacN8bsz/ZEDnSYPYYWCdRB/nonTHoNPuLs06dPZ+7cuezatYsZM2Ywe/Zs8vPzWbp0KWFhYWRmZjY5BHxjTe2lbNmyhT//+c8sWbKExMRErrrqqiOu53Aj6tuQ88aYxmxPJFh84c6wKP4aKDly/8iMGTN4/vnnmTt3LtOnT6e0tJSUlBTCwsJYuHAh27ZtO+zyp512GrNnzwZg1apVrFixAoCysjJiYmLo0aMHu3fv3m8wx0MNQX/aaafxn//8h6qqKiorK3nllVc49dRTW/gGGGO6A9sTCaaIOIhLhfKdUBUDMYce/HH48OGUl5eTnp5OamoqV1xxBRdeeCFZWVmMHj2a44477rAvdcMNN3D11VczcuRIRo8ezfjx4wEYNWoUY8aMYfjw4RxzzDGcfPLJ+5aZNWsW5513HqmpqSxcuHBf+9ixY7nqqqv2reO6665jzJgxdujKGHMQaYObAXYqWVlZeuD1E2vXrmXo0KHBeUFVKNoEtRWQPNgZb6sbCOp7aoxpdyKyVFWzDmy3w1nBJuJchBgSCkVboeHIHe3GGNNZWIi0B18YJPaHQC2U5h55fmOM6SQsRFxBP6wXEQexvaG6CGqbvp96V9HdDpEa051ZiACRkZEUFhYG/49fbB/nrK2SnC47SKOqUlhYSGRkpNelGGPagZ2dBWRkZJCbm0t+fn7wX6y+zhlbK6+8Te7N3hFFRkaSkZFx5BmNMZ2ehQgQFhbGgAED2u8F51wJX70NN34GPY9pv9c1xpg2ZoezvHDu/0BoBMy9Bvy1XldjjDGtZiHihR7pMO1hyPsSFtzjdTXGGNNqFiJeGXoBnHgDLH4E1r7udTXGGNMqFiJeOvs3kDYGXr0Rig8/NpYxxnREFiJeCg2H6U85Q6PMvRr8dV5XZIwxLWIh4rWeA2Dag7BjKbz3a6+rMcaYFrEQ6QiGTYPx34PPHoStn3hdjTHGNJuFSEdx9q8hPh3e+UWz781ujDFesxDpKMKi4MyfQ94XsPoVr6sxxphmsRDpSEZeBinDnb4R62Q3xnQCFiIdSYjPOe23eCtkP+l1NcYYc0QWIh3NwMlwzBmw8A9Qscfraowx5rAsRDoaEZh6L/ir4e27va7GGGMOy0KkI+o1CE75MaycA5s/8LoaY4w5JAuRjuqUn0DiAHj9J1Bf43U1xhjTpKCFiIg8KSJ7RGRVo7Z7RWSdiKwQkVdEJKHRtLtEZKOIrBeRcxu1T3HbNorInY3aB4jIYhHZICIviEh4sLbFE2GRcP5foGgT/PdvXldjjDFNCuaeyD+BKQe0LQBGqOpI4CvgLgARGQbMAIa7yzwsIj4R8QEPAecBw4DL3XkB/gTcp6qDgGLg2iBuizcGToYR34SP/wKFm7yuxhhjDhK0EFHVj4CiA9reUVW/+3QRsPceqtOA51W1VlW3ABuB8e7PRlXdrKp1wPPANBER4Exgrrv808DFwdoWT537B+cGVm/8xK5kN8Z0OF72iVwDzHcfpwM5jablum2Hak8CShoF0t72JonILBHJFpHsdrmPeluK6wOTf+l0sK+ce8TZjTGmPXkSIiJyN+AHZu9tamI2bUV7k1T1UVXNUtWs5OTklpbrvaxrnPuOvPNzqC33uhpjjNmn3UNERGYCFwBXqO47PpML9G00WwaQd5j2AiBBREIPaO+aQnww9S9QsQs+/F+vqzHGmH3aNUREZApwB3CRqlY1mjQPmCEiESIyABgEfA4sAQa5Z2KF43S+z3PDZyEw3V1+JvBqe22HJzLGwZjvwqKHIf8rr6sxxhgguKf4Pgd8BgwRkVwRuRZ4EIgDFojIMhH5O4CqrgbmAGuAt4CbVDXg9nn8AHgbWAvMcecFJ4x+IiIbcfpIngjWtnQYZ/0KwmPgtZud8bWMMcZjot3sjJ+srCzNzs72uozW+/Lf8OoPAIXMU+GC+6HXQK+rMsZ0cSKyVFWzDmy3K9Y7mzHfgR+vcu49snM5vP9brysyxnRjoUeexXQ4PTLgtNuhsgCyn4LqYohK9LoqY0w3ZHsindmoGRCohdX/8boSY0w3ZSHSmaWOhuTjYPnzXldijOmmLEQ6MxEYdTnkLIKizV5XY4zphixEOruRlwJieyPGGE9YiHR28WlwzCT4cjbUVXpdjTGmm7EQ6QpOvRXKdsA7v/C6EmNMN2Mh0hUMOBUm/gCyn4Cv3va6GmNMN2Ih0lWc+QvoPQJevQkqOtlw98aYTstCpKsIjYBLHoOaUmfIeGOMaQcWIl1J72Ew8Uew4nnY9qnX1RhjugELka7m1FuhR1944zYI+I88vzHGHAULka4mPNq5L/ue1bDkMa+rMcZ0cRYiXdHQC2HgWfDuryF3qdfVGGO6MAuRrkgELv47xKbAc5dB8TavKzLGdFEWIl1VbDJc8SIE6uDZS6GmzOuKjDFdkIVIV5Y8BC59BvLXWf+IMSYoLES6umNOd8bWWvIEBOq9rsYY08VYiHQHJ97gjK217nWvKzHGdDEWIt3BoHMgcQAs/ofXlRhjuhgLke4gJATGz4Ltn0HeMq+rMcZ0IRYi3cWYKyA8FhY94nUlxpguxEKku4jsAWNnwsoXoXCT19UYY7oIC5Hu5JRbnNF+P/ij15UYY7oIC5HuJDbF6RtZ+SLsWed1NcaYLsBCpLs5+Wanb+SD//G6EmNMF2Ah0t1E94QJN8Ca/8D2xV5XY4zp5CxEuqOJP4SEfvDKLKgt97oaY0wnZiHSHUXGO7fSLdkO8+/wuhpjTCdmIdJd9ZsAp94Gy2bD6v94XY0xppMKWoiIyJMiskdEVjVq6ykiC0Rkg/tvotsuIvKAiGwUkRUiMrbRMjPd+TeIyMxG7eNEZKW7zAMiIsHali7r9J9C2lh441aoKvK6GmNMJxTMPZF/AlMOaLsTeE9VBwHvuc8BzgMGuT+zgEfACR3gHuBEYDxwz97gceeZ1Wi5A1/LHIkvDKY9CDUl8PbdXldjjOmEghYiqvoRcODX22nA0+7jp4GLG7X/Sx2LgAQRSQXOBRaoapGqFgMLgCnutHhV/UxVFfhXo3WZlug9HE6+BZY/C5ve97oaY0wn0959Ir1VdSeA+2+K254O5DSaL9dtO1x7bhPtTRKRWSKSLSLZ+fn5R70RXc5pt0PSQHjtFrsDojGmRTpKx3pT/RnaivYmqeqjqpqlqlnJycmtLLELC4uEix6E0lx46TpoCHhdkTGmk2jvENntHorC/XeP254L9G00XwaQd4T2jCbaTWv1Pwmm/i9seBve/ZXX1RhjOon2DpF5wN4zrGYCrzZqv9I9S2sCUOoe7nobOEdEEt0O9XOAt91p5SIywT0r68pG6zKtdcJ1zs+nD8CyZ72uxhjTCYQGa8Ui8hwwCeglIrk4Z1n9EZgjItcC24FvubO/CUwFNgJVwNUAqlokIr8Flrjz/UZV93bW34BzBlgUMN/9MUdryh+h4CunfyR5CKSP87oiY0wHJs7JTUeYSeRm4CmgHHgcGAPcqarvBLe8tpeVlaXZ2dlel9GxVRbCY5Mg4IfvfeiM/muM6dZEZKmqZh3Y3tzDWdeoahnO4aRknD0FuylFVxWTBDOehepimDMTGhq8rsgY00E1N0T2ng01FXhKVZfT9BlSpqvoczyc/xfY/qlz/xFjjGlCc0NkqYi8gxMib4tIHGBfT7u6UZdDn5Hw/u/AX+t1NcaYDqi5IXItzhAlJ6hqFRCG2/lturCQEDj711C6HbKf9LoaY0wH1NwQOQlYr6olIvId4OdAafDKMh3GsWfCgNPho3vtanZjzEGaGyKPAFUiMgr4KbANZ7wq0x2c9SuoKoQP/+R1JcaYDqa5IeJ3BzqcBvxNVf8GxAWvLNOhpI+FcVfBZw9BzudeV2OM6UCaGyLlInIX8F3gDRHx4fSLmO7inN9Bj77wnxugvtrraowxHURzQ+QyoBbnepFdOCPm3hu0qkzHExEH0/4PCjc6Z2sZYwzNDBE3OGYDPUTkAqBGVa1PpLs5ZhJkXQOLHoady72uxhjTATQrRETkUuBznLGuLgUWi8j0YBZmOqjJ90BUT3jjNruS3RjT7MNZd+NcIzJTVa/EuVXtL4JXlumwohKca0dyP4cVz3tdjTHGY80NkRBV3dPoeWELljVdzahvQ8YJsOCXUF3idTXGGA81NwjeEpG3ReQqEbkKeANn+HbTHYWEwNR7nWtH5v3QDmsZ0401t2P9duBRYCQwCnhUVe8IZmGmg0sbA2f/FtbOg4/+1+tqjDEeafZNqVT1JeClINZiOpuTboLdq+GD/4GUoTBsmtcVGWPa2WFDRETKgabuWiWAqmp8UKoynYMIXHAfFG6AV74PSQOh93CvqzLGtKPDHs5S1ThVjW/iJ84CxAAQFgmXPgMR8fD8t6Gq6MjLGGO6DDvDyhy9+FS47Bko3QFzr4FAvdcVGWPaiYWIaRt9xzt3Qty8EB45GTYs8LoiY0w7sBAxbWfcTJjxHDTUw+zp8OoPvK7IGBNkFiKmbR03FW5cDBNugi+fgdX/8boiY0wQWYiYthcaDmf/BlJHwxu3QmWh1xUZY4LEQsQEhy8ULn4Eakph/u1eV2OMCRILERM8vYfBpDtg1Uvw3795XY0xJgiafcW6Ma1y8o9h9xpnsEbxwUTrbDemK7EQMcHlC4VLHgMNwDt3Q2gEjL/e66qMMW3EQsQEny8UvvkE+Gth/h3O0Cj9J3pdlTGmDVifiGkfvjC45FFIzIQXr4by3V5XZIxpAxYipv1E9nCGR6kphRevsnG2jOkCLERM++o9HKY9CDmL4P/GwpLHoSHgdVXGmFbyJERE5McislpEVonIcyISKSIDRGSxiGwQkRdEJNydN8J9vtGdntloPXe57etF5FwvtsW0wvHT4fufQO8RzsWIr3wftKk7DhhjOrp2DxERSQd+BGSp6gjAB8wA/gTcp6qDgGLgWneRa4FiVR0I3OfOh4gMc5cbDkwBHhYRX3tuizkKvYfDzNfgtNth5RxY8YLXFRljWsGrw1mhQJSIhALRwE7gTGCuO/1p4GL38TT3Oe70ySIibvvzqlqrqluAjcD4dqrftAURmHQX9Jvo7JEUbfa6ImNMC7V7iKjqDuDPwHac8CgFlgIlqup3Z8sF0t3H6UCOu6zfnT+pcXsTy+xHRGaJSLaIZOfn57ftBpmjE+JzztoSH8yZCYWbvK7IGNMCXhzOSsTZixgApAExwHlNzLr3ILkcYtqh2g9uVH1UVbNUNSs5ObnlRZvgSugL3/i7EyAPnQhv3w01ZV5XZYxpBi8OZ50FbFHVfFWtB14GJgIJ7uEtgAwgz32cC/QFcKf3AIoatzexTJurqQ9QXFkXrNWb46bCj76AUTPgs4fgsTOhYKPXVRljjsCLENkOTBCRaLdvYzKwBlgITHfnmQm86j6e5z7Hnf6+qqrbPsM9e2sAMAj4PFhFn3v/R/xy3upgrd4AxPVxTv+96nWoLoLHz4SN73ldlTHmMLzoE1mM00H+BbDSreFR4A7gJyKyEafP4wl3kSeAJLf9J8Cd7npWA3NwAugt4CZVDdoFB6k9ItlZUh2s1ZvGMk+B6xdCj77w3OWwZ53XFRljDkG0m52fn5WVpdnZ2S1e7icvLGPR5kI+vWtyEKoyTarYAw9PgIR+cO0CZ+gUY4wnRGSpqmYd2G5XrDdTakIku8trCTR0r9D1VGwKnP9XyPsSPv6r19UYY5pgIdJMaQlRBBqUPeU1XpfSvQy/GI7/Fnz0v7DtU6+rMcYcwEKkmdJ6RAGQZ/0i7W/qvZDQH565BNa96XU1xphGLESaKTUhEoC8EtsTaXdRiXDN25AyFF64ApY+feRljDHtwkKkmdISnD2RnaW2J+KJ2GTn1N9jz4TXboZVL3tdkTEGC5Fmi48MIzYi1PZEvBQeA5f9G/pNgFe+B1s+9roiY7o9C5EWSEuItD4Rr4VFwYxnIXEAPH+FdbYb4zELkRZI7RHFzlLbE/FcdE/4zksQ0wv+eQF8cj80NHhdlTHdkoVIC9ieSAeS0BdmfQBDL4R374GnzoMVc6DePh9j2pOFSAuk9YiisLKOmnq7nWuHEBkP3/onXHAfVOyCl6+Hvw6FTQu9rsyYbsNCpAVS3TO0dtkhrY5DBLKugR9+CVfOg7hUePZSWDPP68qM6RYsRFogrcfea0XskEmHExICx5wOV78JaWPgxZnw6YPQYHuNxgSThUgL7L1WJM/2RDquqET47isweAq8czc8cTbsWuV1VcZ0WRYiLdDH3ROxIeE7uPAY5zTgSx6D4m3w6CRYOdfrqozpkixEWiAyzEdSTLjtiXQGIjDyUvjBEuh7Irx0HSz+h9dVGdPlWIi0UFpClPWJdCZ7ryk57nyY/1N4/Sd2/3Zj2pCFSAul9oi08bM6m7BI+NbTMOEmyH4SHhoPa1/zuipjugQLkRZKS4hip42f1fn4QmHKH+C695wr3V/4Dix54sjLGWMOy0KkhdISIimv9VNcWed1KaY1MsbBde/D4PPgjZ/AF894XZExnZqFSAtlZfZEBH45bzXd7f70XUZoOFz6NBw7Geb90O5PYsxRsBBpobH9ErntnCG8tjyPf3y02etyTGuFRsCM2TBwMrz2I3jvt2BfCoxpMQuRVrhx0rGcPzKVP721jgVrdntdjmmtsCi4/HkYeyV8/GeYew3UlHpdlTGdioVIK4gI904fyYi0Htzw76W8/EWu1yWZ1vKFwYUPwFm/gjWvwsMTYfMHHhdlTOdhIdJK0eGhPHv9iYwf0JOfzFnOQws34g/YPS06JRE45cdw7QJn7+Rf0+DpC+HL2VBb7nV1xnRo0t06h7OysjQ7O7vN1lfrD3Dbiyt4bXkemUnR/PDMQUwbnUaoz/K5U6qrgkUPOQFSvMUZi+uMu2Hc1c5pwsZ0UyKyVFWzDmq3EDl6qsq7a/dw34KvWLOzjNQekcw4oR8zxveld3xkm76WaSeqkPM5vP9b2PoxpAyDC/8Gfcd7XZkxnrAQcQUjRPZSVd5bu4d/LdrGR1/lEyIw8dheXDQ6jfOPTyUmwr7JdjqqztXtb/8MSnPhpJvg1Fuds7t84U6fijHdgIWIK5gh0ti2wkpeWprLq8vz2FZYRWxEKNNGp3FpVl+OT+9BSIgEvQbThmrKnNvwZj/5dVt4rDNa8DGne1eXMe3EQsTVXiGyl6ryxfZinl2cw+sr8qj1N5AcF8Gkwcl8Y2w6Jx2ThIgFSqexfRHkZoMG4Mt/Q1UhfO8j6JHhdWXGBJWFiKu9Q6Sxkqo63l+3h4Xr8/lg/R7Ka/xkJkVz8Zh0zh7Wm2Gp8RYonUnBBnj0DEgeDFfPdw5xGdNFWYi4vAyRxmrqA6EpjYUAABi3SURBVMxftZPnP8/h861FqDq3371gVBrTRqdZoHQWa19zBnPMPBXO+Bn0n+h1RcYERYcKERFJAB4HRgAKXAOsB14AMoGtwKWqWizOX9K/AVOBKuAqVf3CXc9M4Ofuan+nqkccBKmjhEhj+eW1LFy/h7dX7eLDr/LxNyh9e0ZxysBkzjwuhbOGpligdGTZT8L7v3MObfWbCJc8Cgl9va7KmDbV0ULkaeBjVX1cRMKBaOBnQJGq/lFE7gQSVfUOEZkK/BAnRE4E/qaqJ4pITyAbyMIJoqXAOFUtPtxrd8QQaay4so43V+1k4bp8Fm0upKLWz4kDevKHS47n2ORYr8szh1JX5fSRvP9b5/a8354DqSO9rsqYNtNhQkRE4oHlwDHa6MVFZD0wSVV3ikgq8IGqDhGRf7iPn2s8394fVf2e277ffIfS0UOksfpAAy8tzeUPb66lpr6Bcf0TiYkIJT0hkstP7MdxfeK9LtEcaPdqmH0p1JTAhBug93BIGQ5Jx0KIz+vqjGm1Q4WIFxcuHAPkA0+JyCicPYibgd6quhPADZIUd/50IKfR8rlu26HaDyIis4BZAP369Wu7LQmyMF8IM8b348yhKdy34Cs27qkgt7iKTzbm8/Rn2zh1UC8uHp3OKYN62UWNHUXv4XDdu/DStfDxX0DdoXDCoqHP8c7wKkPO87ZGY9qQFyESCowFfqiqi0Xkb8Cdh5m/qc4APUz7wY2qjwKPgrMn0rJyvZcSF8n/XPL1oZGSqjpmL97Ovz7byq0vLgfguD5xXDI2nYtHp5NigeKt+FS4+k2or4aCr5y9k50rYNN78NzlMPmXTphYP5fpArw4nNUHWKSqme7zU3FCZCB2OKtFGhqUtbvK+GRDAW+t3sWX20sIEbhwVBo3nTGQwb3jvC7RNFZfDa/eBKtegoFnQeYpkHwc9BkJ8WkWKqZD6zB9Im4xHwPXqep6EfkVEONOKmzUsd5TVX8qIucDP+DrjvUHVHW827G+FGevBuALnI71osO9dlcKkQNtyq/ghSU5/HvRNqrqAoxIjyfcF0JUuI9rTh7A5KG9vS7RqMIn98Hnj0L5zq/bY1LgmEkw8QeQOsqr6ow5pI4WIqNxTvENBzYDV+MMSz8H6AdsB76lqkXuKb4PAlNwTvG9WlWz3fVcg3NWF8DvVfWpI712Vw6RvYor6/jnp1v5MqeEhgZle1EV24uquHBUGvdcOIxesXZRXIdQUwp71sHO5bBjKax7A+rKnb2Us3/j9K8Y00F0qBDxUncIkQPV+Rt45INNPLhwAzERofxs6lC+NS7Drj3paKpLIPsJ+PRBqC2Dk2+B025z7nFijMcsRFzdMUT22rC7nJ+9spIlW4sZ0y+BQSmxhIeGkJEYzZi+CYzMSCAq3E5D9VxlIbxzNyx/DhCISYbE/s7dFzNP8bg4011ZiLi6c4iA0xk/JzuHxz/ZQkWNn1p/gOKqegBCQ4ThafGM7Z/IaYOSmTgwiYhQCxXPbP2vcy+TsjzY8iEUb3P2TE69FXwREGI3PjPtx0LE1d1DpCmFFbV8ub2EL7YXk72tmOU5JdT6G4iLCOWkY5Po0yOSpJgIJhzTkxMye9ow9l6orYD5d8Cyf3/dFp3k3B9+6AXe1WW6DQsRl4XIkdX6A3y6sZC3Vu1iybYiCivqKK129lbSE6I487gUEqLDiI8M47TByQzpY6cSt5sNC2DXCgj4Yf2bsHMZnHqbM/ijXRFvgshCxGUh0jqVtX4WrNnNy1/u4MttxVTU+dn7X2dURg8uGJnGqL4JDE+Ltzs4tpf6GnjzNvjyGecGWb0GQdoYOPEGZ3h6Y9qQhYjLQqRtqCr5FbW8vnwnc7JzWLerHIAQgVF9Ezh1UDIXjUplYIrtpQTd2tdg6yeQvx5yFjsXNQ6/2OmIT8z0uDjTVViIuCxEgmNPWQ0rd5SyLKeEjzcUsCK3BBHhmpMzueWswfhChM35lcRHhZKRGO11uV1XZQEsehgWPwq+UJj+FBx7htdVmS7AQsRlIdI+Cipq+cs763nu8xxiI0KpqvPT4P5Xy0yKZlz/nkSEOWcXnTqwF1NG9LHrVtpS0WZ47ttQsB7OuBtO/B5EuHuFqs7AkNaHYlrAQsRlIdK+lm4r5vnPt5OaEMWglFjyy2v578YCVu4opUGhzh+grMZPVv9Efn7BMEb3TfC65K6jthxe+T6se93pMxk2DaqKnENedZWQPMQZWTjrWsgY53W1poOzEHFZiHQsAfe6lb+88xUFFbVMGpLMrNOOIaeoiueX5LBpTwVD+sQxLDWeUwclc8qgXkSG2TfoZlN1hlTJfhJWv+IM9Nh3AkQlwJ61sCPbGX5lyPkw6U67kZY5JAsRl4VIx1RR6+fpT7fyxCdbKKqsA2BgSiwnZCayYXcFa3aWUVUXIDrcx5ThfbjxjGOt076lVA8eKbi2HBb9HT59wBlqJfNUGH+9M35XeEzT6zHtr6nPrp1ZiLgsRDq2qjo/b63aRf+kaMb2S9zXT1Lnb2DR5kLmr9rFq8t2UF0f4LwRfRjbL5H0hCiGpcXTP6npP3qqav0tR1JdDF/8Cz5/DEpzICQMMk5wDnP1GgwJ/QBx+lLiUiFpoNNxb5qvNUFQXQxzr4WyHTDjWecOmY3XV7QZdq10zsgL1EJcGmRkQXTPr+cr2AAr58K2/8KV81o90oGFiMtCpPMrqqzjiU8288xn2yir8e9rH5oazxlDkgEor/GzraiK9bvKqKlv4E/fHMmUEX28KrnzCPhh60ew+UPY8pFzQ61A7cHz+SIgZSj0n+j8DDgNIns0Wk+9s47VL0NdBYyYDoPPhdAWjCC96iVnPf1PhoS+R79th7JhAXz4J+d06LN+BT0ymrdcbTl88YxzinX/iTBuJkT3gu2fQm42NLj/N4u2uIcNy+CSR2HQ2c1bf+EmePZSZ7ibiFgnNKY/4QTGqpedoXCqCpteNqEfhIQ6719pDiDOuGvTn4LY5Oa9/gEsRFwWIl2HqlJaXU9ucTWLtxQxf+VOsrcVExoixEWGktojiuP6xLExv4KVO0q5Y8pxDE2N59nF29haUMV3T+rPpVl9UZRPNxVS52/gnGG9ba+lsYaA80eoJMf9Fi3O892rIe9LyF0C/hrnD1b/ic5eS/5656r6mlKIiHdGIa7YDRE9nOBJzITYFOdssegkGHYxxCTt/7rZT8LrP/76eeIAGDXD+akqgs0LoXAz+MIgNNL55h3TC9LGQtro5m1b0RZ483bYuAAS+js1SghM/CGMvGz/b/2NleXBokdg6dNQW+psc8EGZ5ovDAJ17owCqLt3MM55vfx1MO0h6D0CPnvQuePl4HOc96B8J2x4x7k1QGWB8zw8FmbMdvb+nr3MOdsOnPdt0LnQd7xzgWlkPPjCndfIWey8Djjb02ckjLjE6Q87ChYiLguRrq0+0EBoiOwXBDX1AW59cTlvrHBuAtUzJpy0hEhW7SijT3wklbV+ymudb42nDurFvdNH4W9o4I0VO6kPNHD5+H4k2T1YmuavdTruv3obvnoLSnOdoOg9HAae7fSthITC5g9g7Tzn23XxVqjM/3oPJzQKRn8bxlwBfUY5f0hfuMJZ/syfw/bPnHutbPlw/9eOS3W+7dfXOPdhAUBgwo0w+RdOuJRsc76th0U7fTwxKc4f2+wnYMEvndpOvwPGz4KKXfD23U6dACnDoecAZ1lfuNNWU+JspzY4Z7ud9EMnIEq2w5eznb2uY8+AfhMh/IDroWrKnO3a8pHzPCzGuQFZzmLQgNMWHgvp4yC2txO0J1zn1ABOKH/xDPQeBpmntfvhRAsRl4VI99TQoDz7+Xbio8I4d3hvwn0hfPhVPk/9dyu94yOYMqIPO4qr+cOb62hQpdbfsG/ZqDAf38rKIC0hikCDkpYQyemDU+gZE+7hFnUBgXrnG/yih2HFC843+LAYJxh6D4erXt+/c794m/MHPra3cxfI2JSvp/nroHKPc9fIJY9Dj75OwFXuOfh1w+O+vvnXhQ9Aj/T9pxdvc0Lrq/nOHkF9lVMr4vQnDJkKE25o3WgA/lp47zfOntO4q51/Kwuc4IxPg34nteyQXzuyEHFZiJjD2VJQycMLN5LZK4YLR6ZRFwjwyAebeXXZDvwNX/+uhAiM6Ze4b2TjURkJJMaEo6qszivjnTW76ZsYxUWj04gI9VHnbyB7WxHJsREMTIm1Q2YHqixw9jS2L3K+cZ/z+1Yfu2fTQvj4LxCfDn1PcP6tr3b2Esp3O4eJMk5wDo3Z59BsFiIuCxHTGrX+AA0Nzt+cr3aX8+7aPXy4fg+r8soIuOHSKzacyDAfucXV+5brFRvBiQN68vGG/H0nASTFhHP64GRmTsxklF1caToJCxGXhYhpS5W1fpbllLB2ZxkbdldQVFXHGUNSmDKiD6vzSnn84y2szitj0pBkzh3eh+KqOhZtKmTBmt2U1/oZ2y+B3vGRVNT6iQrzMbZ/Iln9ExnVN4Ew38GnYuYWV9HQAH17RjW5N2OnM5tgsRBxWYiYjqCi1s8LS3J4MTuHQIMSGxlKcWUdWwurAIiNCOXkgUmMSOtBVLiPmvoAb6/ezcodpQAkRocxtl8i35nQn0lDktm4p4J75q0me1sx00alcfXJAxiYEkt9oIEwXwjhoXYXRHN0LERcFiKmIyuoqCV7axEfflXAh+v3kFdas2/ayIweXDAylZiIUFbklPLhV/nsKqshMyma3OJqYiJCOWNIMm+v3k11fWDfcqEhwnGpcYzKSGBU3wTG9E0g1BfC4s2FLM8tRQQi3dsgV9f7AeGbY9PJyux5YHmHVesPkFNUTXxUKAlR4RZcXYyFiMtCxHQm9YEGquoCqCoJ0fufDVbnb+CNlXk8tziHY1NiuO2cISTFRlBaVc+85Tsoq/ET5hNKqupZnlvCipzSfacy79UjKowwn1BT75yNFhXuo7ouQEWtn/GZPTnjuBQiQp09mXBfCL4QYf3uchZtLqSwoo5vjk1nxvh+fLA+nwfe28CuMif0RGB8Zk++MSad0wYn0zMmvNVjnm3YXc6m/ApOH5xCVPjB6/AHGght4tDf0Sivqee9tXvoHR9JVmZik4cWW6KmPkB+eS0ZiU0fhuwMLERcFiKmu2poUDYXVLIsp4Q6fwPjB/Tk2OSYg/6oVdU5h9oe+2jzfntCe4X7QhjdL4GoMB8fbcjfd4fLsf0SmDG+H7X+BnaVVjN/1S4251d+vVxoCLERocRE+PCJUOdvoEGhV1w4feIjiY0IJUQEX4gQHxVGTLiPjzYUsCynBID4yFAuGZtBmE9Yt6uc7UVVFFXUUV7rJy4ilLSEKFITIukdF0lyXAQNqtTUN1AXCFDnnrI9MCWWYak9KKmuY/HmIjbsKScxOpyUuAhiI0OJCPWRW1zFa8t37tubi48M5aRjkxiUEkdmrxjCfEKtv4FAg6LqnKmXGOOsIy0hiuTYCERgzc4y3l2zh/9uKtj3ng/oFcPU4/sQFxlGbnEVRZV1+EJCCAsRMhKjGNwnjsykGBKiw4gJDyWnuIr1u8rJr6glEFAaFBKiw+jlvkZBRS0lVfX4QoRwXwixkaH0jAknNERYk1fGmp1lRIX5ODYllmOTY5g0JKXVYW4h4rIQMaZ51P0jXOt3/gjXBRqoDyipPSL3/SHaWlDJq8vyGJEez5nHpewXSKrKyh2lrNxRSml1PaVV9VTU+qms9RNQJ4xCBPIratlVWkN1fYAGVer9SnlNPZV1AQalxHLZCX0Z0ieOF7Nzmb9qJyEiDOody4BesSTFhJMQHUZJVT07SqrZWVrN7rJaCipqCQ0RIkJ9+/ai/A1KQcXXQ7jEhPsY3CeOsup68strqaoL4G9QosJ8XDQqjelZGRRW1PLu2j0s3VbM9qKqfWfiHU6EG5aFlXWIwIi0Hpw4oCepCVG8v243n20qpEGdvcBeseGoQq2/gZ2l1TRj9S2SmRRNnb+BvNIaRGDtb6ZYiBwtCxFjOoemDlNV1fmJCPXhCzn8IaFDnaVWVFnHmrwyYiNDGZEWf9D6/QFnj6Wpw2N1/gZ2lFSjqoT5Qgj1CYLQoEphRR27y2rYWVrt7CFV1nPigJ6cOTSFXgeMdlBaXU+IQFxk2H7tNfUBNuVXkFNUTVl1PeW1ftITIhncO460hKh9IzEUV9WRX+6EYa/YCBKiw2hQpc7fQHmNn6LKOmr9AQb1jiPefY3KWj/bi6oYmhp/2PftcCxEXBYixhjTcocKETt9whhjTKtZiBhjjGk1CxFjjDGtZiFijDGm1SxEjDHGtJqFiDHGmFazEDHGGNNqFiLGGGNardtdbCgi+cC2Vi7eCyhow3K8YNvQMdg2dAy2Dc3XX1UPut1ktwuRoyEi2U1dsdmZ2DZ0DLYNHYNtw9Gzw1nGGGNazULEGGNMq1mItMyjXhfQBmwbOgbbho7BtuEoWZ+IMcaYVrM9EWOMMa1mIWKMMabVLESaQUSmiMh6EdkoInd6XU9ziEhfEVkoImtFZLWI3Oy29xSRBSKywf030etaj0REfCLypYi87j4fICKL3W14QUTCva7xSEQkQUTmisg69zM5qbN9FiLyY/f/0ioReU5EIjv6ZyEiT4rIHhFZ1aityfddHA+4v+crRGSsd5V/7RDbcK/7f2mFiLwiIgmNpt3lbsN6ETk32PVZiByBiPiAh4DzgGHA5SIyzNuqmsUP3KqqQ4EJwE1u3XcC76nqIOA993lHdzOwttHzPwH3udtQDFzrSVUt8zfgLVU9DhiFsz2d5rMQkXTgR0CWqo4AfMAMOv5n8U9gygFth3rfzwMGuT+zgEfaqcYj+ScHb8MCYISqjgS+Au4CcH/HZwDD3WUedv+GBY2FyJGNBzaq6mZVrQOeB6Z5XNMRqepOVf3CfVyO80crHaf2p93ZngYu9qbC5hGRDOB84HH3uQBnAnPdWTrDNsQDpwFPAKhqnaqW0Mk+CyAUiBKRUCAa2EkH/yxU9SOg6IDmQ73v04B/qWMRkCAiqe1T6aE1tQ2q+o6q+t2ni4AM9/E04HlVrVXVLcBGnL9hQWMhcmTpQE6j57luW6chIpnAGGAx0FtVd4ITNECKd5U1y/3AT4EG93kSUNLoF6gzfB7HAPnAU+5hucdFJIZO9Fmo6g7gz8B2nPAoBZbS+T4LOPT73ll/168B5ruP230bLESOTJpo6zTnRYtILPAScIuqlnldT0uIyAXAHlVd2ri5iVk7+ucRCowFHlHVMUAlHfjQVVPcfoNpwAAgDYjBOfxzoI7+WRxOp/u/JSJ34xy6nr23qYnZgroNFiJHlgv0bfQ8A8jzqJYWEZEwnACZraovu8279+6iu//u8aq+ZjgZuEhEtuIcRjwTZ88kwT2kAp3j88gFclV1sft8Lk6odKbP4ixgi6rmq2o98DIwkc73WcCh3/dO9bsuIjOBC4Ar9OsL/tp9GyxEjmwJMMg9CyUcp9Nqnsc1HZHbd/AEsFZV/9po0jxgpvt4JvBqe9fWXKp6l6pmqGomzvv+vqpeASwEpruzdehtAFDVXUCOiAxxmyYDa+hEnwXOYawJIhLt/t/auw2d6rNwHep9nwdc6Z6lNQEo3XvYq6MRkSnAHcBFqlrVaNI8YIaIRIjIAJyTBD4PajGqaj9H+AGm4pwBsQm42+t6mlnzKTi7sSuAZe7PVJw+hfeADe6/Pb2utZnbMwl43X18jPuLsRF4EYjwur5m1D8ayHY/j/8AiZ3tswB+DawDVgHPABEd/bMAnsPpw6nH+ZZ+7aHed5xDQQ+5v+crcc5E66jbsBGn72Pv7/bfG81/t7sN64Hzgl2fDXtijDGm1exwljHGmFazEDHGGNNqFiLGGGNazULEGGNMq1mIGGOMaTULEWM6CRGZtHckY2M6CgsRY4wxrWYhYkwbE5HviMjnIrJMRP7h3g+lQkT+IiJfiMh7IpLszjtaRBY1ui/E3ntbDBSRd0VkubvMse7qYxvdl2S2e/W4MZ6xEDGmDYnIUOAy4GRVHQ0EgCtwBiz8QlXHAh8C97iL/Au4Q537Qqxs1D4beEhVR+GMUbV3+I0xwC0497Y5Bmd8MWM8E3rkWYwxLTAZGAcscXcSonAG+GsAXnDn+Tfwsoj0ABJU9UO3/WngRRGJA9JV9RUAVa0BcNf3uarmus+XAZnAJ8HfLGOaZiFiTNsS4GlVvWu/RpFfHDDf4cYbOtwhqtpGjwPY77DxmB3OMqZtvQdMF5EU2Hc/7/44v2t7R7v9NvCJqpYCxSJyqtv+XeBDde77kisiF7vriBCR6HbdCmOayb7FGNOGVHWNiPwceEdEQnBGXr0J50ZUw0VkKc5dAS9zF5kJ/N0Nic3A1W77d4F/iMhv3HV8qx03w5hms1F8jWkHIlKhqrFe12FMW7PDWcYYY1rN9kSMMca0mu2JGGOMaTULEWOMMa1mIWKMMabVLESMMca0moWIMcaYVvt//72CcMFdWIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build plot of training loss and validation loss\n",
    "\n",
    "# print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Price: [[25.32196]]\n",
      "Possible Rating and Subregion [[88 'Côtes de Bordeaux']]\n",
      "---------------------------------------------------------------------------\n",
      "Actual Price: 185.0\n",
      "Actual Rating: 94\n",
      "Actual Subregion: Central Coast\n"
     ]
    }
   ],
   "source": [
    "# Compare one sample y with a predicted y\n",
    "test_data = X_test[0]\n",
    "pred = (model.predict(test_data.reshape(1,125)))\n",
    "print(f\"Predicted Price: {pred}\")\n",
    "print(f\"Possible Rating and Subregion {ohe.inverse_transform(X_test[0].reshape(1,-1))}\")\n",
    "print('---------------------------------------------------------------------------')\n",
    "print(f\"Actual Price: {y[0]}\")\n",
    "print(f\"Actual Rating: {X.rating[1]}\")\n",
    "print(f\"Actual Subregion: {X.subregion[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model_us_french_Cab_Bords_OHE_subregion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old model for testing\n",
    "from tensorflow.keras.models import load_model\n",
    "# old_model = load_model('model_us_french_Cab_Bords_OHE_subregion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to compare predicted price to predicted price of old model\n",
    "\n",
    "# print(y[0])\n",
    "# test_data = X_test[0]\n",
    "# pred = (old_model.predict(test_data.reshape(1,125)))\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show one X value from OHE version of list of possible rate / region combos\n",
    "test = np.expand_dims(X_ohe[0], axis=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18056, 1)\n",
      "(18056, 1)\n",
      "(18056, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>subregion</th>\n",
       "      <th>actual_price</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "      <td>185</td>\n",
       "      <td>131.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>35</td>\n",
       "      <td>77.5505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>56</td>\n",
       "      <td>77.5505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "      <td>102</td>\n",
       "      <td>131.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>Napa</td>\n",
       "      <td>175</td>\n",
       "      <td>131.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18051</td>\n",
       "      <td>81</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>10</td>\n",
       "      <td>15.9121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18052</td>\n",
       "      <td>82</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>28</td>\n",
       "      <td>30.2901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18053</td>\n",
       "      <td>82</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>9</td>\n",
       "      <td>15.9121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18054</td>\n",
       "      <td>82</td>\n",
       "      <td>Sierra Foothills</td>\n",
       "      <td>42</td>\n",
       "      <td>22.5978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18055</td>\n",
       "      <td>85</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>17</td>\n",
       "      <td>31.8451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18056 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating         subregion actual_price predicted_price\n",
       "0         95              Napa          185         131.459\n",
       "1         94     Central Coast           35         77.5505\n",
       "2         94     Central Coast           56         77.5505\n",
       "3         95              Napa          102         131.459\n",
       "4         95              Napa          175         131.459\n",
       "...      ...               ...          ...             ...\n",
       "18051     81          Bordeaux           10         15.9121\n",
       "18052     82     Central Coast           28         30.2901\n",
       "18053     82          Bordeaux            9         15.9121\n",
       "18054     82  Sierra Foothills           42         22.5978\n",
       "18055     85     Central Coast           17         31.8451\n",
       "\n",
       "[18056 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on original dataset\n",
    "\n",
    "predict_dataset = model.predict(X_ohe)\n",
    "predict_dataset\n",
    "print(predict_dataset.shape)\n",
    "# print(df['price'].to_list())\n",
    "orig_rating = np.asarray(X['rating']).reshape(-1,1)\n",
    "print(orig_rating.shape)\n",
    "orig_subreg = np.asarray(X['subregion']).reshape(-1,1)\n",
    "new_y = np.asarray(y[0:]).reshape(-1,1)\n",
    "print(new_y.shape)\n",
    "\n",
    "combo_df = pd.DataFrame(np.concatenate((orig_rating,orig_subreg,new_y, predict_dataset),axis=1))\n",
    "combo_df = combo_df.rename(columns={0: \"rating\", 1: \"subregion\", 2: \"actual_price\", 3: \"predicted_price\"})\n",
    "combo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "combo_df.to_csv(\"Cab_Bords_Actual_Pred_subregion.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80, 'Anjou'],\n",
       "       [80, 'Anjou Villages Brissac'],\n",
       "       [80, 'Atlantique'],\n",
       "       ...,\n",
       "       [100, \"Virginia's Eastern Shore\"],\n",
       "       [100, 'Washington Other'],\n",
       "       [100, 'Yadkin Valley']], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse_transform list of possible rate / region combos\n",
    "prediction_list = ohe.inverse_transform(predict_X_ohe)\n",
    "prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  26.448233],\n",
       "       [  41.78214 ],\n",
       "       [  32.17127 ],\n",
       "       ...,\n",
       "       [2131.4014  ],\n",
       "       [1152.4169  ],\n",
       "       [1983.8982  ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use model to make price predictions from list of possible rate / region combos\n",
    "predicted_prices = model.predict(predict_X_ohe)\n",
    "predicted_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>subregion</th>\n",
       "      <th>predicted_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>Anjou</td>\n",
       "      <td>26.4482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>Anjou Villages Brissac</td>\n",
       "      <td>41.7821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>Atlantique</td>\n",
       "      <td>32.1713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>Augusta</td>\n",
       "      <td>59.3677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>Bergerac</td>\n",
       "      <td>26.6982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2179</td>\n",
       "      <td>100</td>\n",
       "      <td>Vin de Pays des Coteaux de Peyriac</td>\n",
       "      <td>1961.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>100</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>1879.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2181</td>\n",
       "      <td>100</td>\n",
       "      <td>Virginia's Eastern Shore</td>\n",
       "      <td>2131.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2182</td>\n",
       "      <td>100</td>\n",
       "      <td>Washington Other</td>\n",
       "      <td>1152.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2183</td>\n",
       "      <td>100</td>\n",
       "      <td>Yadkin Valley</td>\n",
       "      <td>1983.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                           subregion predicted_\n",
       "0        80                               Anjou    26.4482\n",
       "1        80              Anjou Villages Brissac    41.7821\n",
       "2        80                          Atlantique    32.1713\n",
       "3        80                             Augusta    59.3677\n",
       "4        80                            Bergerac    26.6982\n",
       "...     ...                                 ...        ...\n",
       "2179    100  Vin de Pays des Coteaux de Peyriac    1961.55\n",
       "2180    100                            Virginia    1879.11\n",
       "2181    100            Virginia's Eastern Shore     2131.4\n",
       "2182    100                    Washington Other    1152.42\n",
       "2183    100                       Yadkin Valley     1983.9\n",
       "\n",
       "[2184 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat predicted prices with associated predicted rating and subregion from original prediction list\n",
    "predicted_df = pd.DataFrame(np.concatenate((prediction_list, predicted_prices), axis=1))\n",
    "predicted_df = predicted_df.rename(columns={0: \"rating\", 1: \"subregion\", 2: \"predicted_\"})\n",
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "predicted_df.to_csv(\"Cab_Bords_Predictions_subregion.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
